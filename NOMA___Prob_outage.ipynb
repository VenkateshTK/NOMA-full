{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NOMA _ Prob_outage",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VenkateshTK/NOMA/blob/master/NOMA___Prob_outage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdktJBQELFl_",
        "colab_type": "text"
      },
      "source": [
        "# Non Orthogonal Multiple Access , channel capacity maximising."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ctVLoPfIu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt \n",
        "import math as m\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYhMg7VwfJ3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a05d4e89-7bbf-42f3-b632-57cc4991ca48"
      },
      "source": [
        "\n",
        "dataset_dir='/content/dataset.csv'\n",
        "writer = SummaryWriter('runs/Noma_trainloss')\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,csv_path):\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.coloum_names = [\"h1\",\"h2\",\"h3\",\"h4\",\"p1\",\"p2\",\"p3\",\"p4\",\"pt,\"\"cost\"]\n",
        "        df=pd.read_csv(csv_path)    \n",
        "        self.h = np.asarray(df.iloc[:,0:4])\n",
        "        self.power = np.asarray(df.iloc[:,4:8])\n",
        "        self.power_total = np.asarray(df.iloc[:,8])\n",
        "        self.data_len=len(df.index)\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.data_len\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        data1=self.h[index]\n",
        "        data2=self.power_total[index]\n",
        "        input_data = np.concatenate([data1,data2],axis = None)\n",
        "        #print(input_data)\n",
        "        output_data = self.power[index]\n",
        "        return torch.from_numpy(input_data),torch.from_numpy(output_data)\n",
        "\n",
        "dataset=CustomDataset(dataset_dir)\n",
        "print(dataset.__getitem__(0))\n",
        "\n",
        "#print(dataframe.head())new_a = a[np.newaxis, :]\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset,[train_size, test_size])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([ 0.2891, 11.8935, 13.4525, 45.9215, 14.1200], dtype=torch.float64), tensor([2.7072, 1.2744, 1.3488, 8.7896], dtype=torch.float64))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHvZozkTLP-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b9291726-7b00-438a-d537-4ec4e319b407"
      },
      "source": [
        "\n",
        "input_size = 5\n",
        "output_size= 4\n",
        "hidden_size = 30\n",
        "num_classes = 4\n",
        "num_epochs = 100\n",
        "batch_size = 1\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=10,shuffle=True)\n",
        "\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden_1 = torch.nn.Linear(n_feature, n_hidden)\n",
        "        self.hidden_2 = torch.nn.Linear(n_hidden, 50)    # hidden layer\n",
        "        self.hidden_3 = torch.nn.Linear(50, n_hidden)    # hidden layer\n",
        "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden_1(x))\n",
        "        x = F.relu(self.hidden_2(x))\n",
        "        x = F.relu(self.hidden_3(x))\n",
        "        x = F.relu(self.predict(x))      # activation function for hidden layer\n",
        "        return x\n",
        "\n",
        "\n",
        "model = Net(input_size, hidden_size, num_classes)\n",
        "\n",
        "print(model)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (hidden_1): Linear(in_features=5, out_features=30, bias=True)\n",
            "  (hidden_2): Linear(in_features=30, out_features=50, bias=True)\n",
            "  (hidden_3): Linear(in_features=50, out_features=30, bias=True)\n",
            "  (predict): Linear(in_features=30, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijhAWT0vY-8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)  "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JDB8PiiLbEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc00be7b-f68f-4734-cc71-5efd311704e3"
      },
      "source": [
        "\n",
        "total_step = len(train_loader)\n",
        "running_loss=0.0\n",
        "loss1=[]\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data,labels) in enumerate(train_loader):        \n",
        "        # Forward pass\n",
        "        labels=labels[:,np.newaxis]\n",
        "        outputs = model(data.float())\n",
        "        loss = criterion(outputs, labels.float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss+=loss.item()\n",
        "        if (i+1) % batch_size == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    writer.add_scalar('training loss',running_loss/total_step,epoch)\n",
        "    loss1.append(running_loss/total_step)\n",
        "    running_loss=0\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([1, 1, 4])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch [90/100], Step [425/493], Loss: 0.0045\n",
            "Epoch [90/100], Step [426/493], Loss: 0.0095\n",
            "Epoch [90/100], Step [427/493], Loss: 0.0084\n",
            "Epoch [90/100], Step [428/493], Loss: 0.0393\n",
            "Epoch [90/100], Step [429/493], Loss: 0.0191\n",
            "Epoch [90/100], Step [430/493], Loss: 0.0081\n",
            "Epoch [90/100], Step [431/493], Loss: 0.0597\n",
            "Epoch [90/100], Step [432/493], Loss: 0.0097\n",
            "Epoch [90/100], Step [433/493], Loss: 0.0077\n",
            "Epoch [90/100], Step [434/493], Loss: 0.0035\n",
            "Epoch [90/100], Step [435/493], Loss: 0.1971\n",
            "Epoch [90/100], Step [436/493], Loss: 0.0145\n",
            "Epoch [90/100], Step [437/493], Loss: 0.0096\n",
            "Epoch [90/100], Step [438/493], Loss: 0.0081\n",
            "Epoch [90/100], Step [439/493], Loss: 0.0017\n",
            "Epoch [90/100], Step [440/493], Loss: 0.0770\n",
            "Epoch [90/100], Step [441/493], Loss: 0.0280\n",
            "Epoch [90/100], Step [442/493], Loss: 0.0023\n",
            "Epoch [90/100], Step [443/493], Loss: 0.0060\n",
            "Epoch [90/100], Step [444/493], Loss: 0.1160\n",
            "Epoch [90/100], Step [445/493], Loss: 0.0055\n",
            "Epoch [90/100], Step [446/493], Loss: 0.0578\n",
            "Epoch [90/100], Step [447/493], Loss: 0.0058\n",
            "Epoch [90/100], Step [448/493], Loss: 0.0330\n",
            "Epoch [90/100], Step [449/493], Loss: 0.0154\n",
            "Epoch [90/100], Step [450/493], Loss: 0.0062\n",
            "Epoch [90/100], Step [451/493], Loss: 0.0043\n",
            "Epoch [90/100], Step [452/493], Loss: 0.0144\n",
            "Epoch [90/100], Step [453/493], Loss: 0.0073\n",
            "Epoch [90/100], Step [454/493], Loss: 0.1099\n",
            "Epoch [90/100], Step [455/493], Loss: 0.0039\n",
            "Epoch [90/100], Step [456/493], Loss: 0.0077\n",
            "Epoch [90/100], Step [457/493], Loss: 0.0833\n",
            "Epoch [90/100], Step [458/493], Loss: 0.0095\n",
            "Epoch [90/100], Step [459/493], Loss: 0.2135\n",
            "Epoch [90/100], Step [460/493], Loss: 0.0059\n",
            "Epoch [90/100], Step [461/493], Loss: 0.0032\n",
            "Epoch [90/100], Step [462/493], Loss: 0.0062\n",
            "Epoch [90/100], Step [463/493], Loss: 0.0131\n",
            "Epoch [90/100], Step [464/493], Loss: 0.0018\n",
            "Epoch [90/100], Step [465/493], Loss: 0.0872\n",
            "Epoch [90/100], Step [466/493], Loss: 0.0046\n",
            "Epoch [90/100], Step [467/493], Loss: 0.0087\n",
            "Epoch [90/100], Step [468/493], Loss: 0.0052\n",
            "Epoch [90/100], Step [469/493], Loss: 0.0056\n",
            "Epoch [90/100], Step [470/493], Loss: 0.0029\n",
            "Epoch [90/100], Step [471/493], Loss: 0.0382\n",
            "Epoch [90/100], Step [472/493], Loss: 0.0227\n",
            "Epoch [90/100], Step [473/493], Loss: 0.0092\n",
            "Epoch [90/100], Step [474/493], Loss: 0.0054\n",
            "Epoch [90/100], Step [475/493], Loss: 0.0952\n",
            "Epoch [90/100], Step [476/493], Loss: 0.0081\n",
            "Epoch [90/100], Step [477/493], Loss: 0.0067\n",
            "Epoch [90/100], Step [478/493], Loss: 0.0072\n",
            "Epoch [90/100], Step [479/493], Loss: 0.0060\n",
            "Epoch [90/100], Step [480/493], Loss: 0.0020\n",
            "Epoch [90/100], Step [481/493], Loss: 0.0268\n",
            "Epoch [90/100], Step [482/493], Loss: 0.0424\n",
            "Epoch [90/100], Step [483/493], Loss: 0.0100\n",
            "Epoch [90/100], Step [484/493], Loss: 0.0043\n",
            "Epoch [90/100], Step [485/493], Loss: 0.0264\n",
            "Epoch [90/100], Step [486/493], Loss: 0.0021\n",
            "Epoch [90/100], Step [487/493], Loss: 0.0259\n",
            "Epoch [90/100], Step [488/493], Loss: 0.0048\n",
            "Epoch [90/100], Step [489/493], Loss: 0.1575\n",
            "Epoch [90/100], Step [490/493], Loss: 0.0870\n",
            "Epoch [90/100], Step [491/493], Loss: 0.0051\n",
            "Epoch [90/100], Step [492/493], Loss: 0.0022\n",
            "Epoch [90/100], Step [493/493], Loss: 0.0103\n",
            "Epoch [91/100], Step [1/493], Loss: 0.0070\n",
            "Epoch [91/100], Step [2/493], Loss: 0.0042\n",
            "Epoch [91/100], Step [3/493], Loss: 0.0305\n",
            "Epoch [91/100], Step [4/493], Loss: 0.0179\n",
            "Epoch [91/100], Step [5/493], Loss: 0.0044\n",
            "Epoch [91/100], Step [6/493], Loss: 0.0117\n",
            "Epoch [91/100], Step [7/493], Loss: 0.0035\n",
            "Epoch [91/100], Step [8/493], Loss: 0.1683\n",
            "Epoch [91/100], Step [9/493], Loss: 0.0268\n",
            "Epoch [91/100], Step [10/493], Loss: 0.0412\n",
            "Epoch [91/100], Step [11/493], Loss: 0.0300\n",
            "Epoch [91/100], Step [12/493], Loss: 0.0058\n",
            "Epoch [91/100], Step [13/493], Loss: 0.0169\n",
            "Epoch [91/100], Step [14/493], Loss: 0.0057\n",
            "Epoch [91/100], Step [15/493], Loss: 0.0015\n",
            "Epoch [91/100], Step [16/493], Loss: 0.0068\n",
            "Epoch [91/100], Step [17/493], Loss: 0.0382\n",
            "Epoch [91/100], Step [18/493], Loss: 0.0015\n",
            "Epoch [91/100], Step [19/493], Loss: 0.0119\n",
            "Epoch [91/100], Step [20/493], Loss: 0.0072\n",
            "Epoch [91/100], Step [21/493], Loss: 0.0032\n",
            "Epoch [91/100], Step [22/493], Loss: 0.0063\n",
            "Epoch [91/100], Step [23/493], Loss: 0.0020\n",
            "Epoch [91/100], Step [24/493], Loss: 0.0059\n",
            "Epoch [91/100], Step [25/493], Loss: 0.0043\n",
            "Epoch [91/100], Step [26/493], Loss: 0.1114\n",
            "Epoch [91/100], Step [27/493], Loss: 0.0132\n",
            "Epoch [91/100], Step [28/493], Loss: 0.0044\n",
            "Epoch [91/100], Step [29/493], Loss: 0.0040\n",
            "Epoch [91/100], Step [30/493], Loss: 0.0074\n",
            "Epoch [91/100], Step [31/493], Loss: 0.0040\n",
            "Epoch [91/100], Step [32/493], Loss: 0.0062\n",
            "Epoch [91/100], Step [33/493], Loss: 0.0001\n",
            "Epoch [91/100], Step [34/493], Loss: 0.0141\n",
            "Epoch [91/100], Step [35/493], Loss: 0.0029\n",
            "Epoch [91/100], Step [36/493], Loss: 0.1971\n",
            "Epoch [91/100], Step [37/493], Loss: 0.0070\n",
            "Epoch [91/100], Step [38/493], Loss: 0.0072\n",
            "Epoch [91/100], Step [39/493], Loss: 0.0035\n",
            "Epoch [91/100], Step [40/493], Loss: 0.0252\n",
            "Epoch [91/100], Step [41/493], Loss: 0.0295\n",
            "Epoch [91/100], Step [42/493], Loss: 0.0168\n",
            "Epoch [91/100], Step [43/493], Loss: 0.0195\n",
            "Epoch [91/100], Step [44/493], Loss: 0.0014\n",
            "Epoch [91/100], Step [45/493], Loss: 0.0022\n",
            "Epoch [91/100], Step [46/493], Loss: 0.0055\n",
            "Epoch [91/100], Step [47/493], Loss: 0.0065\n",
            "Epoch [91/100], Step [48/493], Loss: 0.0093\n",
            "Epoch [91/100], Step [49/493], Loss: 0.0255\n",
            "Epoch [91/100], Step [50/493], Loss: 0.0316\n",
            "Epoch [91/100], Step [51/493], Loss: 0.2259\n",
            "Epoch [91/100], Step [52/493], Loss: 0.0085\n",
            "Epoch [91/100], Step [53/493], Loss: 0.0029\n",
            "Epoch [91/100], Step [54/493], Loss: 0.0872\n",
            "Epoch [91/100], Step [55/493], Loss: 0.0075\n",
            "Epoch [91/100], Step [56/493], Loss: 0.0031\n",
            "Epoch [91/100], Step [57/493], Loss: 0.0124\n",
            "Epoch [91/100], Step [58/493], Loss: 0.0080\n",
            "Epoch [91/100], Step [59/493], Loss: 0.0391\n",
            "Epoch [91/100], Step [60/493], Loss: 0.0071\n",
            "Epoch [91/100], Step [61/493], Loss: 0.0018\n",
            "Epoch [91/100], Step [62/493], Loss: 0.0092\n",
            "Epoch [91/100], Step [63/493], Loss: 0.0058\n",
            "Epoch [91/100], Step [64/493], Loss: 0.0033\n",
            "Epoch [91/100], Step [65/493], Loss: 0.0005\n",
            "Epoch [91/100], Step [66/493], Loss: 0.0083\n",
            "Epoch [91/100], Step [67/493], Loss: 0.0095\n",
            "Epoch [91/100], Step [68/493], Loss: 0.0128\n",
            "Epoch [91/100], Step [69/493], Loss: 0.0893\n",
            "Epoch [91/100], Step [70/493], Loss: 0.0154\n",
            "Epoch [91/100], Step [71/493], Loss: 0.0138\n",
            "Epoch [91/100], Step [72/493], Loss: 0.0080\n",
            "Epoch [91/100], Step [73/493], Loss: 0.0029\n",
            "Epoch [91/100], Step [74/493], Loss: 0.0055\n",
            "Epoch [91/100], Step [75/493], Loss: 0.0109\n",
            "Epoch [91/100], Step [76/493], Loss: 0.0434\n",
            "Epoch [91/100], Step [77/493], Loss: 0.0072\n",
            "Epoch [91/100], Step [78/493], Loss: 0.0140\n",
            "Epoch [91/100], Step [79/493], Loss: 0.0067\n",
            "Epoch [91/100], Step [80/493], Loss: 0.0174\n",
            "Epoch [91/100], Step [81/493], Loss: 0.0138\n",
            "Epoch [91/100], Step [82/493], Loss: 0.0077\n",
            "Epoch [91/100], Step [83/493], Loss: 0.0092\n",
            "Epoch [91/100], Step [84/493], Loss: 0.0074\n",
            "Epoch [91/100], Step [85/493], Loss: 0.0141\n",
            "Epoch [91/100], Step [86/493], Loss: 0.0096\n",
            "Epoch [91/100], Step [87/493], Loss: 0.0065\n",
            "Epoch [91/100], Step [88/493], Loss: 0.0069\n",
            "Epoch [91/100], Step [89/493], Loss: 0.0330\n",
            "Epoch [91/100], Step [90/493], Loss: 0.0099\n",
            "Epoch [91/100], Step [91/493], Loss: 0.0016\n",
            "Epoch [91/100], Step [92/493], Loss: 0.0136\n",
            "Epoch [91/100], Step [93/493], Loss: 0.0130\n",
            "Epoch [91/100], Step [94/493], Loss: 0.0077\n",
            "Epoch [91/100], Step [95/493], Loss: 0.0081\n",
            "Epoch [91/100], Step [96/493], Loss: 0.1790\n",
            "Epoch [91/100], Step [97/493], Loss: 0.0057\n",
            "Epoch [91/100], Step [98/493], Loss: 0.0043\n",
            "Epoch [91/100], Step [99/493], Loss: 0.0030\n",
            "Epoch [91/100], Step [100/493], Loss: 0.0140\n",
            "Epoch [91/100], Step [101/493], Loss: 0.0036\n",
            "Epoch [91/100], Step [102/493], Loss: 0.0046\n",
            "Epoch [91/100], Step [103/493], Loss: 0.0084\n",
            "Epoch [91/100], Step [104/493], Loss: 0.0075\n",
            "Epoch [91/100], Step [105/493], Loss: 0.0059\n",
            "Epoch [91/100], Step [106/493], Loss: 0.0143\n",
            "Epoch [91/100], Step [107/493], Loss: 0.0110\n",
            "Epoch [91/100], Step [108/493], Loss: 0.0033\n",
            "Epoch [91/100], Step [109/493], Loss: 0.0136\n",
            "Epoch [91/100], Step [110/493], Loss: 0.0034\n",
            "Epoch [91/100], Step [111/493], Loss: 0.0091\n",
            "Epoch [91/100], Step [112/493], Loss: 0.0053\n",
            "Epoch [91/100], Step [113/493], Loss: 0.0036\n",
            "Epoch [91/100], Step [114/493], Loss: 0.0092\n",
            "Epoch [91/100], Step [115/493], Loss: 0.0045\n",
            "Epoch [91/100], Step [116/493], Loss: 0.0156\n",
            "Epoch [91/100], Step [117/493], Loss: 0.0022\n",
            "Epoch [91/100], Step [118/493], Loss: 0.0172\n",
            "Epoch [91/100], Step [119/493], Loss: 0.0058\n",
            "Epoch [91/100], Step [120/493], Loss: 0.0023\n",
            "Epoch [91/100], Step [121/493], Loss: 0.0087\n",
            "Epoch [91/100], Step [122/493], Loss: 0.0068\n",
            "Epoch [91/100], Step [123/493], Loss: 0.2338\n",
            "Epoch [91/100], Step [124/493], Loss: 0.0059\n",
            "Epoch [91/100], Step [125/493], Loss: 0.0078\n",
            "Epoch [91/100], Step [126/493], Loss: 0.0025\n",
            "Epoch [91/100], Step [127/493], Loss: 0.0084\n",
            "Epoch [91/100], Step [128/493], Loss: 0.0058\n",
            "Epoch [91/100], Step [129/493], Loss: 0.0116\n",
            "Epoch [91/100], Step [130/493], Loss: 0.0109\n",
            "Epoch [91/100], Step [131/493], Loss: 0.0277\n",
            "Epoch [91/100], Step [132/493], Loss: 0.2984\n",
            "Epoch [91/100], Step [133/493], Loss: 0.0002\n",
            "Epoch [91/100], Step [134/493], Loss: 0.0149\n",
            "Epoch [91/100], Step [135/493], Loss: 0.0071\n",
            "Epoch [91/100], Step [136/493], Loss: 0.0060\n",
            "Epoch [91/100], Step [137/493], Loss: 0.0017\n",
            "Epoch [91/100], Step [138/493], Loss: 0.0154\n",
            "Epoch [91/100], Step [139/493], Loss: 0.0096\n",
            "Epoch [91/100], Step [140/493], Loss: 0.0230\n",
            "Epoch [91/100], Step [141/493], Loss: 0.0050\n",
            "Epoch [91/100], Step [142/493], Loss: 0.0126\n",
            "Epoch [91/100], Step [143/493], Loss: 0.0048\n",
            "Epoch [91/100], Step [144/493], Loss: 0.0033\n",
            "Epoch [91/100], Step [145/493], Loss: 0.0055\n",
            "Epoch [91/100], Step [146/493], Loss: 0.0057\n",
            "Epoch [91/100], Step [147/493], Loss: 0.0052\n",
            "Epoch [91/100], Step [148/493], Loss: 0.0080\n",
            "Epoch [91/100], Step [149/493], Loss: 0.0131\n",
            "Epoch [91/100], Step [150/493], Loss: 0.0100\n",
            "Epoch [91/100], Step [151/493], Loss: 0.0016\n",
            "Epoch [91/100], Step [152/493], Loss: 0.0092\n",
            "Epoch [91/100], Step [153/493], Loss: 0.0163\n",
            "Epoch [91/100], Step [154/493], Loss: 0.0014\n",
            "Epoch [91/100], Step [155/493], Loss: 0.0035\n",
            "Epoch [91/100], Step [156/493], Loss: 0.0146\n",
            "Epoch [91/100], Step [157/493], Loss: 0.0202\n",
            "Epoch [91/100], Step [158/493], Loss: 0.0055\n",
            "Epoch [91/100], Step [159/493], Loss: 0.0227\n",
            "Epoch [91/100], Step [160/493], Loss: 0.0175\n",
            "Epoch [91/100], Step [161/493], Loss: 0.0445\n",
            "Epoch [91/100], Step [162/493], Loss: 0.1160\n",
            "Epoch [91/100], Step [163/493], Loss: 0.0019\n",
            "Epoch [91/100], Step [164/493], Loss: 0.0068\n",
            "Epoch [91/100], Step [165/493], Loss: 0.0023\n",
            "Epoch [91/100], Step [166/493], Loss: 0.0011\n",
            "Epoch [91/100], Step [167/493], Loss: 0.0112\n",
            "Epoch [91/100], Step [168/493], Loss: 0.0325\n",
            "Epoch [91/100], Step [169/493], Loss: 0.0074\n",
            "Epoch [91/100], Step [170/493], Loss: 0.0003\n",
            "Epoch [91/100], Step [171/493], Loss: 0.0657\n",
            "Epoch [91/100], Step [172/493], Loss: 0.0123\n",
            "Epoch [91/100], Step [173/493], Loss: 0.0039\n",
            "Epoch [91/100], Step [174/493], Loss: 0.0298\n",
            "Epoch [91/100], Step [175/493], Loss: 0.0026\n",
            "Epoch [91/100], Step [176/493], Loss: 0.0305\n",
            "Epoch [91/100], Step [177/493], Loss: 0.0040\n",
            "Epoch [91/100], Step [178/493], Loss: 0.0340\n",
            "Epoch [91/100], Step [179/493], Loss: 0.0067\n",
            "Epoch [91/100], Step [180/493], Loss: 0.0166\n",
            "Epoch [91/100], Step [181/493], Loss: 0.0132\n",
            "Epoch [91/100], Step [182/493], Loss: 0.0099\n",
            "Epoch [91/100], Step [183/493], Loss: 0.0870\n",
            "Epoch [91/100], Step [184/493], Loss: 0.0077\n",
            "Epoch [91/100], Step [185/493], Loss: 0.0339\n",
            "Epoch [91/100], Step [186/493], Loss: 0.0018\n",
            "Epoch [91/100], Step [187/493], Loss: 0.0054\n",
            "Epoch [91/100], Step [188/493], Loss: 0.0097\n",
            "Epoch [91/100], Step [189/493], Loss: 0.0209\n",
            "Epoch [91/100], Step [190/493], Loss: 0.0047\n",
            "Epoch [91/100], Step [191/493], Loss: 0.0032\n",
            "Epoch [91/100], Step [192/493], Loss: 0.0045\n",
            "Epoch [91/100], Step [193/493], Loss: 0.0059\n",
            "Epoch [91/100], Step [194/493], Loss: 0.0131\n",
            "Epoch [91/100], Step [195/493], Loss: 0.0021\n",
            "Epoch [91/100], Step [196/493], Loss: 0.0058\n",
            "Epoch [91/100], Step [197/493], Loss: 0.0838\n",
            "Epoch [91/100], Step [198/493], Loss: 0.0059\n",
            "Epoch [91/100], Step [199/493], Loss: 0.0076\n",
            "Epoch [91/100], Step [200/493], Loss: 0.0128\n",
            "Epoch [91/100], Step [201/493], Loss: 0.0108\n",
            "Epoch [91/100], Step [202/493], Loss: 0.0070\n",
            "Epoch [91/100], Step [203/493], Loss: 0.0089\n",
            "Epoch [91/100], Step [204/493], Loss: 0.0344\n",
            "Epoch [91/100], Step [205/493], Loss: 0.1286\n",
            "Epoch [91/100], Step [206/493], Loss: 0.0178\n",
            "Epoch [91/100], Step [207/493], Loss: 0.0263\n",
            "Epoch [91/100], Step [208/493], Loss: 0.0086\n",
            "Epoch [91/100], Step [209/493], Loss: 0.0040\n",
            "Epoch [91/100], Step [210/493], Loss: 0.0087\n",
            "Epoch [91/100], Step [211/493], Loss: 0.0102\n",
            "Epoch [91/100], Step [212/493], Loss: 0.0229\n",
            "Epoch [91/100], Step [213/493], Loss: 0.0114\n",
            "Epoch [91/100], Step [214/493], Loss: 0.0200\n",
            "Epoch [91/100], Step [215/493], Loss: 0.0369\n",
            "Epoch [91/100], Step [216/493], Loss: 0.1857\n",
            "Epoch [91/100], Step [217/493], Loss: 0.0070\n",
            "Epoch [91/100], Step [218/493], Loss: 0.0161\n",
            "Epoch [91/100], Step [219/493], Loss: 0.0024\n",
            "Epoch [91/100], Step [220/493], Loss: 0.0039\n",
            "Epoch [91/100], Step [221/493], Loss: 0.0063\n",
            "Epoch [91/100], Step [222/493], Loss: 0.0131\n",
            "Epoch [91/100], Step [223/493], Loss: 0.0097\n",
            "Epoch [91/100], Step [224/493], Loss: 0.0037\n",
            "Epoch [91/100], Step [225/493], Loss: 0.0045\n",
            "Epoch [91/100], Step [226/493], Loss: 0.0049\n",
            "Epoch [91/100], Step [227/493], Loss: 0.0069\n",
            "Epoch [91/100], Step [228/493], Loss: 0.0259\n",
            "Epoch [91/100], Step [229/493], Loss: 0.0060\n",
            "Epoch [91/100], Step [230/493], Loss: 0.2135\n",
            "Epoch [91/100], Step [231/493], Loss: 0.0109\n",
            "Epoch [91/100], Step [232/493], Loss: 0.0268\n",
            "Epoch [91/100], Step [233/493], Loss: 0.0121\n",
            "Epoch [91/100], Step [234/493], Loss: 0.0084\n",
            "Epoch [91/100], Step [235/493], Loss: 0.0103\n",
            "Epoch [91/100], Step [236/493], Loss: 0.0103\n",
            "Epoch [91/100], Step [237/493], Loss: 0.0144\n",
            "Epoch [91/100], Step [238/493], Loss: 0.0253\n",
            "Epoch [91/100], Step [239/493], Loss: 0.0041\n",
            "Epoch [91/100], Step [240/493], Loss: 0.0045\n",
            "Epoch [91/100], Step [241/493], Loss: 0.0472\n",
            "Epoch [91/100], Step [242/493], Loss: 0.0080\n",
            "Epoch [91/100], Step [243/493], Loss: 0.0047\n",
            "Epoch [91/100], Step [244/493], Loss: 0.1317\n",
            "Epoch [91/100], Step [245/493], Loss: 0.0003\n",
            "Epoch [91/100], Step [246/493], Loss: 0.0019\n",
            "Epoch [91/100], Step [247/493], Loss: 0.0393\n",
            "Epoch [91/100], Step [248/493], Loss: 0.0133\n",
            "Epoch [91/100], Step [249/493], Loss: 0.0100\n",
            "Epoch [91/100], Step [250/493], Loss: 0.0179\n",
            "Epoch [91/100], Step [251/493], Loss: 0.1666\n",
            "Epoch [91/100], Step [252/493], Loss: 0.0042\n",
            "Epoch [91/100], Step [253/493], Loss: 0.0099\n",
            "Epoch [91/100], Step [254/493], Loss: 0.0054\n",
            "Epoch [91/100], Step [255/493], Loss: 0.0075\n",
            "Epoch [91/100], Step [256/493], Loss: 0.0117\n",
            "Epoch [91/100], Step [257/493], Loss: 0.0048\n",
            "Epoch [91/100], Step [258/493], Loss: 0.0072\n",
            "Epoch [91/100], Step [259/493], Loss: 0.0117\n",
            "Epoch [91/100], Step [260/493], Loss: 0.0228\n",
            "Epoch [91/100], Step [261/493], Loss: 0.0181\n",
            "Epoch [91/100], Step [262/493], Loss: 0.0014\n",
            "Epoch [91/100], Step [263/493], Loss: 0.1575\n",
            "Epoch [91/100], Step [264/493], Loss: 0.0027\n",
            "Epoch [91/100], Step [265/493], Loss: 0.0133\n",
            "Epoch [91/100], Step [266/493], Loss: 0.0029\n",
            "Epoch [91/100], Step [267/493], Loss: 0.0077\n",
            "Epoch [91/100], Step [268/493], Loss: 0.0049\n",
            "Epoch [91/100], Step [269/493], Loss: 0.0051\n",
            "Epoch [91/100], Step [270/493], Loss: 0.0128\n",
            "Epoch [91/100], Step [271/493], Loss: 0.0081\n",
            "Epoch [91/100], Step [272/493], Loss: 0.0054\n",
            "Epoch [91/100], Step [273/493], Loss: 0.0011\n",
            "Epoch [91/100], Step [274/493], Loss: 0.0141\n",
            "Epoch [91/100], Step [275/493], Loss: 0.0250\n",
            "Epoch [91/100], Step [276/493], Loss: 0.0086\n",
            "Epoch [91/100], Step [277/493], Loss: 0.0158\n",
            "Epoch [91/100], Step [278/493], Loss: 0.0033\n",
            "Epoch [91/100], Step [279/493], Loss: 0.0054\n",
            "Epoch [91/100], Step [280/493], Loss: 0.2014\n",
            "Epoch [91/100], Step [281/493], Loss: 0.1910\n",
            "Epoch [91/100], Step [282/493], Loss: 0.0061\n",
            "Epoch [91/100], Step [283/493], Loss: 0.0770\n",
            "Epoch [91/100], Step [284/493], Loss: 0.0078\n",
            "Epoch [91/100], Step [285/493], Loss: 0.0073\n",
            "Epoch [91/100], Step [286/493], Loss: 0.0001\n",
            "Epoch [91/100], Step [287/493], Loss: 0.0097\n",
            "Epoch [91/100], Step [288/493], Loss: 0.0124\n",
            "Epoch [91/100], Step [289/493], Loss: 0.0100\n",
            "Epoch [91/100], Step [290/493], Loss: 0.0045\n",
            "Epoch [91/100], Step [291/493], Loss: 0.0597\n",
            "Epoch [91/100], Step [292/493], Loss: 0.0087\n",
            "Epoch [91/100], Step [293/493], Loss: 0.0092\n",
            "Epoch [91/100], Step [294/493], Loss: 0.0031\n",
            "Epoch [91/100], Step [295/493], Loss: 0.0264\n",
            "Epoch [91/100], Step [296/493], Loss: 0.0094\n",
            "Epoch [91/100], Step [297/493], Loss: 0.0073\n",
            "Epoch [91/100], Step [298/493], Loss: 0.0167\n",
            "Epoch [91/100], Step [299/493], Loss: 0.0071\n",
            "Epoch [91/100], Step [300/493], Loss: 0.0022\n",
            "Epoch [91/100], Step [301/493], Loss: 0.0010\n",
            "Epoch [91/100], Step [302/493], Loss: 0.0085\n",
            "Epoch [91/100], Step [303/493], Loss: 0.0098\n",
            "Epoch [91/100], Step [304/493], Loss: 0.0145\n",
            "Epoch [91/100], Step [305/493], Loss: 0.0103\n",
            "Epoch [91/100], Step [306/493], Loss: 0.0277\n",
            "Epoch [91/100], Step [307/493], Loss: 0.0210\n",
            "Epoch [91/100], Step [308/493], Loss: 0.0081\n",
            "Epoch [91/100], Step [309/493], Loss: 0.0170\n",
            "Epoch [91/100], Step [310/493], Loss: 0.0077\n",
            "Epoch [91/100], Step [311/493], Loss: 0.0065\n",
            "Epoch [91/100], Step [312/493], Loss: 0.0092\n",
            "Epoch [91/100], Step [313/493], Loss: 0.0360\n",
            "Epoch [91/100], Step [314/493], Loss: 0.0060\n",
            "Epoch [91/100], Step [315/493], Loss: 0.0012\n",
            "Epoch [91/100], Step [316/493], Loss: 0.0065\n",
            "Epoch [91/100], Step [317/493], Loss: 0.0011\n",
            "Epoch [91/100], Step [318/493], Loss: 0.0263\n",
            "Epoch [91/100], Step [319/493], Loss: 0.0058\n",
            "Epoch [91/100], Step [320/493], Loss: 0.1024\n",
            "Epoch [91/100], Step [321/493], Loss: 0.2699\n",
            "Epoch [91/100], Step [322/493], Loss: 0.0029\n",
            "Epoch [91/100], Step [323/493], Loss: 0.0225\n",
            "Epoch [91/100], Step [324/493], Loss: 0.0308\n",
            "Epoch [91/100], Step [325/493], Loss: 0.0116\n",
            "Epoch [91/100], Step [326/493], Loss: 0.0079\n",
            "Epoch [91/100], Step [327/493], Loss: 0.0066\n",
            "Epoch [91/100], Step [328/493], Loss: 0.0117\n",
            "Epoch [91/100], Step [329/493], Loss: 0.0123\n",
            "Epoch [91/100], Step [330/493], Loss: 0.0079\n",
            "Epoch [91/100], Step [331/493], Loss: 0.0131\n",
            "Epoch [91/100], Step [332/493], Loss: 0.0057\n",
            "Epoch [91/100], Step [333/493], Loss: 0.0096\n",
            "Epoch [91/100], Step [334/493], Loss: 0.5350\n",
            "Epoch [91/100], Step [335/493], Loss: 0.0122\n",
            "Epoch [91/100], Step [336/493], Loss: 0.0078\n",
            "Epoch [91/100], Step [337/493], Loss: 0.0074\n",
            "Epoch [91/100], Step [338/493], Loss: 0.0330\n",
            "Epoch [91/100], Step [339/493], Loss: 0.0064\n",
            "Epoch [91/100], Step [340/493], Loss: 0.0039\n",
            "Epoch [91/100], Step [341/493], Loss: 0.0033\n",
            "Epoch [91/100], Step [342/493], Loss: 0.0097\n",
            "Epoch [91/100], Step [343/493], Loss: 0.0007\n",
            "Epoch [91/100], Step [344/493], Loss: 0.0018\n",
            "Epoch [91/100], Step [345/493], Loss: 0.2245\n",
            "Epoch [91/100], Step [346/493], Loss: 0.0060\n",
            "Epoch [91/100], Step [347/493], Loss: 0.0077\n",
            "Epoch [91/100], Step [348/493], Loss: 0.0010\n",
            "Epoch [91/100], Step [349/493], Loss: 0.0147\n",
            "Epoch [91/100], Step [350/493], Loss: 0.1099\n",
            "Epoch [91/100], Step [351/493], Loss: 0.0083\n",
            "Epoch [91/100], Step [352/493], Loss: 0.0070\n",
            "Epoch [91/100], Step [353/493], Loss: 0.0043\n",
            "Epoch [91/100], Step [354/493], Loss: 0.0001\n",
            "Epoch [91/100], Step [355/493], Loss: 0.0097\n",
            "Epoch [91/100], Step [356/493], Loss: 0.0067\n",
            "Epoch [91/100], Step [357/493], Loss: 0.0043\n",
            "Epoch [91/100], Step [358/493], Loss: 0.0057\n",
            "Epoch [91/100], Step [359/493], Loss: 0.0056\n",
            "Epoch [91/100], Step [360/493], Loss: 0.0284\n",
            "Epoch [91/100], Step [361/493], Loss: 0.0068\n",
            "Epoch [91/100], Step [362/493], Loss: 0.0081\n",
            "Epoch [91/100], Step [363/493], Loss: 0.0061\n",
            "Epoch [91/100], Step [364/493], Loss: 0.0039\n",
            "Epoch [91/100], Step [365/493], Loss: 0.0027\n",
            "Epoch [91/100], Step [366/493], Loss: 0.0117\n",
            "Epoch [91/100], Step [367/493], Loss: 0.0042\n",
            "Epoch [91/100], Step [368/493], Loss: 0.0175\n",
            "Epoch [91/100], Step [369/493], Loss: 0.0022\n",
            "Epoch [91/100], Step [370/493], Loss: 0.0096\n",
            "Epoch [91/100], Step [371/493], Loss: 0.1087\n",
            "Epoch [91/100], Step [372/493], Loss: 0.0066\n",
            "Epoch [91/100], Step [373/493], Loss: 0.0095\n",
            "Epoch [91/100], Step [374/493], Loss: 0.0578\n",
            "Epoch [91/100], Step [375/493], Loss: 0.0230\n",
            "Epoch [91/100], Step [376/493], Loss: 0.0456\n",
            "Epoch [91/100], Step [377/493], Loss: 0.0070\n",
            "Epoch [91/100], Step [378/493], Loss: 0.0059\n",
            "Epoch [91/100], Step [379/493], Loss: 0.0093\n",
            "Epoch [91/100], Step [380/493], Loss: 0.0101\n",
            "Epoch [91/100], Step [381/493], Loss: 0.0059\n",
            "Epoch [91/100], Step [382/493], Loss: 0.0033\n",
            "Epoch [91/100], Step [383/493], Loss: 0.0109\n",
            "Epoch [91/100], Step [384/493], Loss: 0.0952\n",
            "Epoch [91/100], Step [385/493], Loss: 0.0073\n",
            "Epoch [91/100], Step [386/493], Loss: 0.0088\n",
            "Epoch [91/100], Step [387/493], Loss: 0.0145\n",
            "Epoch [91/100], Step [388/493], Loss: 0.0290\n",
            "Epoch [91/100], Step [389/493], Loss: 0.0042\n",
            "Epoch [91/100], Step [390/493], Loss: 0.0121\n",
            "Epoch [91/100], Step [391/493], Loss: 0.0005\n",
            "Epoch [91/100], Step [392/493], Loss: 0.0194\n",
            "Epoch [91/100], Step [393/493], Loss: 0.0014\n",
            "Epoch [91/100], Step [394/493], Loss: 0.0030\n",
            "Epoch [91/100], Step [395/493], Loss: 0.0046\n",
            "Epoch [91/100], Step [396/493], Loss: 0.0095\n",
            "Epoch [91/100], Step [397/493], Loss: 0.0121\n",
            "Epoch [91/100], Step [398/493], Loss: 0.0097\n",
            "Epoch [91/100], Step [399/493], Loss: 0.0139\n",
            "Epoch [91/100], Step [400/493], Loss: 0.0180\n",
            "Epoch [91/100], Step [401/493], Loss: 0.0081\n",
            "Epoch [91/100], Step [402/493], Loss: 0.0050\n",
            "Epoch [91/100], Step [403/493], Loss: 0.0253\n",
            "Epoch [91/100], Step [404/493], Loss: 0.0072\n",
            "Epoch [91/100], Step [405/493], Loss: 0.0037\n",
            "Epoch [91/100], Step [406/493], Loss: 0.1181\n",
            "Epoch [91/100], Step [407/493], Loss: 0.0110\n",
            "Epoch [91/100], Step [408/493], Loss: 0.0023\n",
            "Epoch [91/100], Step [409/493], Loss: 0.0280\n",
            "Epoch [91/100], Step [410/493], Loss: 0.0403\n",
            "Epoch [91/100], Step [411/493], Loss: 0.0112\n",
            "Epoch [91/100], Step [412/493], Loss: 0.0123\n",
            "Epoch [91/100], Step [413/493], Loss: 0.0129\n",
            "Epoch [91/100], Step [414/493], Loss: 0.0280\n",
            "Epoch [91/100], Step [415/493], Loss: 0.0051\n",
            "Epoch [91/100], Step [416/493], Loss: 0.0129\n",
            "Epoch [91/100], Step [417/493], Loss: 0.0073\n",
            "Epoch [91/100], Step [418/493], Loss: 0.0077\n",
            "Epoch [91/100], Step [419/493], Loss: 0.0153\n",
            "Epoch [91/100], Step [420/493], Loss: 0.0094\n",
            "Epoch [91/100], Step [421/493], Loss: 0.0079\n",
            "Epoch [91/100], Step [422/493], Loss: 0.0037\n",
            "Epoch [91/100], Step [423/493], Loss: 0.0102\n",
            "Epoch [91/100], Step [424/493], Loss: 0.0027\n",
            "Epoch [91/100], Step [425/493], Loss: 0.0289\n",
            "Epoch [91/100], Step [426/493], Loss: 0.0018\n",
            "Epoch [91/100], Step [427/493], Loss: 0.0100\n",
            "Epoch [91/100], Step [428/493], Loss: 0.0066\n",
            "Epoch [91/100], Step [429/493], Loss: 0.0686\n",
            "Epoch [91/100], Step [430/493], Loss: 0.0154\n",
            "Epoch [91/100], Step [431/493], Loss: 0.0065\n",
            "Epoch [91/100], Step [432/493], Loss: 0.0054\n",
            "Epoch [91/100], Step [433/493], Loss: 0.0833\n",
            "Epoch [91/100], Step [434/493], Loss: 0.0032\n",
            "Epoch [91/100], Step [435/493], Loss: 0.0330\n",
            "Epoch [91/100], Step [436/493], Loss: 0.0424\n",
            "Epoch [91/100], Step [437/493], Loss: 0.0060\n",
            "Epoch [91/100], Step [438/493], Loss: 0.0074\n",
            "Epoch [91/100], Step [439/493], Loss: 0.0206\n",
            "Epoch [91/100], Step [440/493], Loss: 0.0244\n",
            "Epoch [91/100], Step [441/493], Loss: 0.0147\n",
            "Epoch [91/100], Step [442/493], Loss: 0.0029\n",
            "Epoch [91/100], Step [443/493], Loss: 0.0083\n",
            "Epoch [91/100], Step [444/493], Loss: 0.0121\n",
            "Epoch [91/100], Step [445/493], Loss: 0.0074\n",
            "Epoch [91/100], Step [446/493], Loss: 0.0413\n",
            "Epoch [91/100], Step [447/493], Loss: 0.0046\n",
            "Epoch [91/100], Step [448/493], Loss: 0.0104\n",
            "Epoch [91/100], Step [449/493], Loss: 0.0253\n",
            "Epoch [91/100], Step [450/493], Loss: 0.0143\n",
            "Epoch [91/100], Step [451/493], Loss: 0.0029\n",
            "Epoch [91/100], Step [452/493], Loss: 0.0208\n",
            "Epoch [91/100], Step [453/493], Loss: 0.0240\n",
            "Epoch [91/100], Step [454/493], Loss: 0.0105\n",
            "Epoch [91/100], Step [455/493], Loss: 0.0097\n",
            "Epoch [91/100], Step [456/493], Loss: 0.0051\n",
            "Epoch [91/100], Step [457/493], Loss: 0.0050\n",
            "Epoch [91/100], Step [458/493], Loss: 0.0258\n",
            "Epoch [91/100], Step [459/493], Loss: 0.0062\n",
            "Epoch [91/100], Step [460/493], Loss: 0.0131\n",
            "Epoch [91/100], Step [461/493], Loss: 0.0041\n",
            "Epoch [91/100], Step [462/493], Loss: 0.0032\n",
            "Epoch [91/100], Step [463/493], Loss: 0.0140\n",
            "Epoch [91/100], Step [464/493], Loss: 0.0074\n",
            "Epoch [91/100], Step [465/493], Loss: 0.0105\n",
            "Epoch [91/100], Step [466/493], Loss: 0.0081\n",
            "Epoch [91/100], Step [467/493], Loss: 0.2286\n",
            "Epoch [91/100], Step [468/493], Loss: 0.0382\n",
            "Epoch [91/100], Step [469/493], Loss: 0.0045\n",
            "Epoch [91/100], Step [470/493], Loss: 0.0035\n",
            "Epoch [91/100], Step [471/493], Loss: 0.0067\n",
            "Epoch [91/100], Step [472/493], Loss: 0.0047\n",
            "Epoch [91/100], Step [473/493], Loss: 0.0075\n",
            "Epoch [91/100], Step [474/493], Loss: 0.0144\n",
            "Epoch [91/100], Step [475/493], Loss: 0.3149\n",
            "Epoch [91/100], Step [476/493], Loss: 0.0078\n",
            "Epoch [91/100], Step [477/493], Loss: 0.0098\n",
            "Epoch [91/100], Step [478/493], Loss: 0.4792\n",
            "Epoch [91/100], Step [479/493], Loss: 0.0124\n",
            "Epoch [91/100], Step [480/493], Loss: 0.0031\n",
            "Epoch [91/100], Step [481/493], Loss: 0.0685\n",
            "Epoch [91/100], Step [482/493], Loss: 0.0094\n",
            "Epoch [91/100], Step [483/493], Loss: 0.0340\n",
            "Epoch [91/100], Step [484/493], Loss: 0.0141\n",
            "Epoch [91/100], Step [485/493], Loss: 0.0023\n",
            "Epoch [91/100], Step [486/493], Loss: 0.0084\n",
            "Epoch [91/100], Step [487/493], Loss: 0.0191\n",
            "Epoch [91/100], Step [488/493], Loss: 0.0031\n",
            "Epoch [91/100], Step [489/493], Loss: 0.0118\n",
            "Epoch [91/100], Step [490/493], Loss: 0.0062\n",
            "Epoch [91/100], Step [491/493], Loss: 0.0086\n",
            "Epoch [91/100], Step [492/493], Loss: 0.0264\n",
            "Epoch [91/100], Step [493/493], Loss: 0.0264\n",
            "Epoch [92/100], Step [1/493], Loss: 0.0073\n",
            "Epoch [92/100], Step [2/493], Loss: 0.0325\n",
            "Epoch [92/100], Step [3/493], Loss: 0.4792\n",
            "Epoch [92/100], Step [4/493], Loss: 0.0027\n",
            "Epoch [92/100], Step [5/493], Loss: 0.0086\n",
            "Epoch [92/100], Step [6/493], Loss: 0.0167\n",
            "Epoch [92/100], Step [7/493], Loss: 0.0057\n",
            "Epoch [92/100], Step [8/493], Loss: 0.0128\n",
            "Epoch [92/100], Step [9/493], Loss: 0.0010\n",
            "Epoch [92/100], Step [10/493], Loss: 0.0136\n",
            "Epoch [92/100], Step [11/493], Loss: 0.0277\n",
            "Epoch [92/100], Step [12/493], Loss: 0.0093\n",
            "Epoch [92/100], Step [13/493], Loss: 0.0036\n",
            "Epoch [92/100], Step [14/493], Loss: 0.0084\n",
            "Epoch [92/100], Step [15/493], Loss: 0.0685\n",
            "Epoch [92/100], Step [16/493], Loss: 0.0067\n",
            "Epoch [92/100], Step [17/493], Loss: 0.0253\n",
            "Epoch [92/100], Step [18/493], Loss: 0.0044\n",
            "Epoch [92/100], Step [19/493], Loss: 0.0043\n",
            "Epoch [92/100], Step [20/493], Loss: 0.0277\n",
            "Epoch [92/100], Step [21/493], Loss: 0.0123\n",
            "Epoch [92/100], Step [22/493], Loss: 0.0043\n",
            "Epoch [92/100], Step [23/493], Loss: 0.0018\n",
            "Epoch [92/100], Step [24/493], Loss: 0.0253\n",
            "Epoch [92/100], Step [25/493], Loss: 0.0403\n",
            "Epoch [92/100], Step [26/493], Loss: 0.0330\n",
            "Epoch [92/100], Step [27/493], Loss: 0.0072\n",
            "Epoch [92/100], Step [28/493], Loss: 0.0012\n",
            "Epoch [92/100], Step [29/493], Loss: 0.0070\n",
            "Epoch [92/100], Step [30/493], Loss: 0.0080\n",
            "Epoch [92/100], Step [31/493], Loss: 0.0259\n",
            "Epoch [92/100], Step [32/493], Loss: 0.0031\n",
            "Epoch [92/100], Step [33/493], Loss: 0.0087\n",
            "Epoch [92/100], Step [34/493], Loss: 0.1666\n",
            "Epoch [92/100], Step [35/493], Loss: 0.0068\n",
            "Epoch [92/100], Step [36/493], Loss: 0.0143\n",
            "Epoch [92/100], Step [37/493], Loss: 0.0052\n",
            "Epoch [92/100], Step [38/493], Loss: 0.0042\n",
            "Epoch [92/100], Step [39/493], Loss: 0.2259\n",
            "Epoch [92/100], Step [40/493], Loss: 0.0095\n",
            "Epoch [92/100], Step [41/493], Loss: 0.0099\n",
            "Epoch [92/100], Step [42/493], Loss: 0.0075\n",
            "Epoch [92/100], Step [43/493], Loss: 0.0077\n",
            "Epoch [92/100], Step [44/493], Loss: 0.0128\n",
            "Epoch [92/100], Step [45/493], Loss: 0.0870\n",
            "Epoch [92/100], Step [46/493], Loss: 0.0305\n",
            "Epoch [92/100], Step [47/493], Loss: 0.0051\n",
            "Epoch [92/100], Step [48/493], Loss: 0.0412\n",
            "Epoch [92/100], Step [49/493], Loss: 0.0091\n",
            "Epoch [92/100], Step [50/493], Loss: 0.0133\n",
            "Epoch [92/100], Step [51/493], Loss: 0.0031\n",
            "Epoch [92/100], Step [52/493], Loss: 0.0014\n",
            "Epoch [92/100], Step [53/493], Loss: 0.0124\n",
            "Epoch [92/100], Step [54/493], Loss: 0.0161\n",
            "Epoch [92/100], Step [55/493], Loss: 0.0147\n",
            "Epoch [92/100], Step [56/493], Loss: 0.0045\n",
            "Epoch [92/100], Step [57/493], Loss: 0.0181\n",
            "Epoch [92/100], Step [58/493], Loss: 0.0055\n",
            "Epoch [92/100], Step [59/493], Loss: 0.0174\n",
            "Epoch [92/100], Step [60/493], Loss: 0.2245\n",
            "Epoch [92/100], Step [61/493], Loss: 0.0043\n",
            "Epoch [92/100], Step [62/493], Loss: 0.0072\n",
            "Epoch [92/100], Step [63/493], Loss: 0.0144\n",
            "Epoch [92/100], Step [64/493], Loss: 0.0084\n",
            "Epoch [92/100], Step [65/493], Loss: 0.0085\n",
            "Epoch [92/100], Step [66/493], Loss: 0.0066\n",
            "Epoch [92/100], Step [67/493], Loss: 0.0068\n",
            "Epoch [92/100], Step [68/493], Loss: 0.0340\n",
            "Epoch [92/100], Step [69/493], Loss: 0.0085\n",
            "Epoch [92/100], Step [70/493], Loss: 0.0096\n",
            "Epoch [92/100], Step [71/493], Loss: 0.0154\n",
            "Epoch [92/100], Step [72/493], Loss: 0.0025\n",
            "Epoch [92/100], Step [73/493], Loss: 0.0097\n",
            "Epoch [92/100], Step [74/493], Loss: 0.0077\n",
            "Epoch [92/100], Step [75/493], Loss: 0.0076\n",
            "Epoch [92/100], Step [76/493], Loss: 0.0031\n",
            "Epoch [92/100], Step [77/493], Loss: 0.0065\n",
            "Epoch [92/100], Step [78/493], Loss: 0.1575\n",
            "Epoch [92/100], Step [79/493], Loss: 0.0240\n",
            "Epoch [92/100], Step [80/493], Loss: 0.0360\n",
            "Epoch [92/100], Step [81/493], Loss: 0.0081\n",
            "Epoch [92/100], Step [82/493], Loss: 0.0072\n",
            "Epoch [92/100], Step [83/493], Loss: 0.0066\n",
            "Epoch [92/100], Step [84/493], Loss: 0.0031\n",
            "Epoch [92/100], Step [85/493], Loss: 0.0019\n",
            "Epoch [92/100], Step [86/493], Loss: 0.0077\n",
            "Epoch [92/100], Step [87/493], Loss: 0.0071\n",
            "Epoch [92/100], Step [88/493], Loss: 0.0124\n",
            "Epoch [92/100], Step [89/493], Loss: 0.0047\n",
            "Epoch [92/100], Step [90/493], Loss: 0.0122\n",
            "Epoch [92/100], Step [91/493], Loss: 0.0016\n",
            "Epoch [92/100], Step [92/493], Loss: 0.0011\n",
            "Epoch [92/100], Step [93/493], Loss: 0.0146\n",
            "Epoch [92/100], Step [94/493], Loss: 0.0145\n",
            "Epoch [92/100], Step [95/493], Loss: 0.0472\n",
            "Epoch [92/100], Step [96/493], Loss: 0.0154\n",
            "Epoch [92/100], Step [97/493], Loss: 0.0002\n",
            "Epoch [92/100], Step [98/493], Loss: 0.0045\n",
            "Epoch [92/100], Step [99/493], Loss: 0.0255\n",
            "Epoch [92/100], Step [100/493], Loss: 0.0170\n",
            "Epoch [92/100], Step [101/493], Loss: 0.0015\n",
            "Epoch [92/100], Step [102/493], Loss: 0.0059\n",
            "Epoch [92/100], Step [103/493], Loss: 0.0099\n",
            "Epoch [92/100], Step [104/493], Loss: 0.0097\n",
            "Epoch [92/100], Step [105/493], Loss: 0.0059\n",
            "Epoch [92/100], Step [106/493], Loss: 0.0096\n",
            "Epoch [92/100], Step [107/493], Loss: 0.0041\n",
            "Epoch [92/100], Step [108/493], Loss: 0.0032\n",
            "Epoch [92/100], Step [109/493], Loss: 0.0140\n",
            "Epoch [92/100], Step [110/493], Loss: 0.0117\n",
            "Epoch [92/100], Step [111/493], Loss: 0.0007\n",
            "Epoch [92/100], Step [112/493], Loss: 0.0139\n",
            "Epoch [92/100], Step [113/493], Loss: 0.0130\n",
            "Epoch [92/100], Step [114/493], Loss: 0.1683\n",
            "Epoch [92/100], Step [115/493], Loss: 0.0037\n",
            "Epoch [92/100], Step [116/493], Loss: 0.0005\n",
            "Epoch [92/100], Step [117/493], Loss: 0.0048\n",
            "Epoch [92/100], Step [118/493], Loss: 0.0089\n",
            "Epoch [92/100], Step [119/493], Loss: 0.0029\n",
            "Epoch [92/100], Step [120/493], Loss: 0.0080\n",
            "Epoch [92/100], Step [121/493], Loss: 0.0046\n",
            "Epoch [92/100], Step [122/493], Loss: 0.0102\n",
            "Epoch [92/100], Step [123/493], Loss: 0.0169\n",
            "Epoch [92/100], Step [124/493], Loss: 0.0330\n",
            "Epoch [92/100], Step [125/493], Loss: 0.0069\n",
            "Epoch [92/100], Step [126/493], Loss: 0.1910\n",
            "Epoch [92/100], Step [127/493], Loss: 0.0126\n",
            "Epoch [92/100], Step [128/493], Loss: 0.0330\n",
            "Epoch [92/100], Step [129/493], Loss: 0.0109\n",
            "Epoch [92/100], Step [130/493], Loss: 0.0175\n",
            "Epoch [92/100], Step [131/493], Loss: 0.1181\n",
            "Epoch [92/100], Step [132/493], Loss: 0.0112\n",
            "Epoch [92/100], Step [133/493], Loss: 0.0123\n",
            "Epoch [92/100], Step [134/493], Loss: 0.0035\n",
            "Epoch [92/100], Step [135/493], Loss: 0.0050\n",
            "Epoch [92/100], Step [136/493], Loss: 0.0019\n",
            "Epoch [92/100], Step [137/493], Loss: 0.0141\n",
            "Epoch [92/100], Step [138/493], Loss: 0.0172\n",
            "Epoch [92/100], Step [139/493], Loss: 0.0033\n",
            "Epoch [92/100], Step [140/493], Loss: 0.0140\n",
            "Epoch [92/100], Step [141/493], Loss: 0.0059\n",
            "Epoch [92/100], Step [142/493], Loss: 0.0092\n",
            "Epoch [92/100], Step [143/493], Loss: 0.0033\n",
            "Epoch [92/100], Step [144/493], Loss: 0.0072\n",
            "Epoch [92/100], Step [145/493], Loss: 0.0124\n",
            "Epoch [92/100], Step [146/493], Loss: 0.1024\n",
            "Epoch [92/100], Step [147/493], Loss: 0.0078\n",
            "Epoch [92/100], Step [148/493], Loss: 0.0042\n",
            "Epoch [92/100], Step [149/493], Loss: 0.0096\n",
            "Epoch [92/100], Step [150/493], Loss: 0.0037\n",
            "Epoch [92/100], Step [151/493], Loss: 0.0117\n",
            "Epoch [92/100], Step [152/493], Loss: 0.0099\n",
            "Epoch [92/100], Step [153/493], Loss: 0.0029\n",
            "Epoch [92/100], Step [154/493], Loss: 0.0058\n",
            "Epoch [92/100], Step [155/493], Loss: 0.0023\n",
            "Epoch [92/100], Step [156/493], Loss: 0.0103\n",
            "Epoch [92/100], Step [157/493], Loss: 0.0040\n",
            "Epoch [92/100], Step [158/493], Loss: 0.0097\n",
            "Epoch [92/100], Step [159/493], Loss: 0.0059\n",
            "Epoch [92/100], Step [160/493], Loss: 0.0039\n",
            "Epoch [92/100], Step [161/493], Loss: 0.0032\n",
            "Epoch [92/100], Step [162/493], Loss: 0.0138\n",
            "Epoch [92/100], Step [163/493], Loss: 0.0058\n",
            "Epoch [92/100], Step [164/493], Loss: 0.0153\n",
            "Epoch [92/100], Step [165/493], Loss: 0.0093\n",
            "Epoch [92/100], Step [166/493], Loss: 0.2135\n",
            "Epoch [92/100], Step [167/493], Loss: 0.0133\n",
            "Epoch [92/100], Step [168/493], Loss: 0.1160\n",
            "Epoch [92/100], Step [169/493], Loss: 0.2014\n",
            "Epoch [92/100], Step [170/493], Loss: 0.0050\n",
            "Epoch [92/100], Step [171/493], Loss: 0.0101\n",
            "Epoch [92/100], Step [172/493], Loss: 0.0029\n",
            "Epoch [92/100], Step [173/493], Loss: 0.0208\n",
            "Epoch [92/100], Step [174/493], Loss: 0.0097\n",
            "Epoch [92/100], Step [175/493], Loss: 0.0048\n",
            "Epoch [92/100], Step [176/493], Loss: 0.0578\n",
            "Epoch [92/100], Step [177/493], Loss: 0.0141\n",
            "Epoch [92/100], Step [178/493], Loss: 0.0065\n",
            "Epoch [92/100], Step [179/493], Loss: 0.0143\n",
            "Epoch [92/100], Step [180/493], Loss: 0.1317\n",
            "Epoch [92/100], Step [181/493], Loss: 0.0015\n",
            "Epoch [92/100], Step [182/493], Loss: 0.0014\n",
            "Epoch [92/100], Step [183/493], Loss: 0.0080\n",
            "Epoch [92/100], Step [184/493], Loss: 0.0045\n",
            "Epoch [92/100], Step [185/493], Loss: 0.0290\n",
            "Epoch [92/100], Step [186/493], Loss: 0.0060\n",
            "Epoch [92/100], Step [187/493], Loss: 0.0046\n",
            "Epoch [92/100], Step [188/493], Loss: 0.0145\n",
            "Epoch [92/100], Step [189/493], Loss: 0.0121\n",
            "Epoch [92/100], Step [190/493], Loss: 0.0058\n",
            "Epoch [92/100], Step [191/493], Loss: 0.0051\n",
            "Epoch [92/100], Step [192/493], Loss: 0.0070\n",
            "Epoch [92/100], Step [193/493], Loss: 0.0062\n",
            "Epoch [92/100], Step [194/493], Loss: 0.0027\n",
            "Epoch [92/100], Step [195/493], Loss: 0.0061\n",
            "Epoch [92/100], Step [196/493], Loss: 0.0264\n",
            "Epoch [92/100], Step [197/493], Loss: 0.0073\n",
            "Epoch [92/100], Step [198/493], Loss: 0.0086\n",
            "Epoch [92/100], Step [199/493], Loss: 0.0063\n",
            "Epoch [92/100], Step [200/493], Loss: 0.0225\n",
            "Epoch [92/100], Step [201/493], Loss: 0.0067\n",
            "Epoch [92/100], Step [202/493], Loss: 0.0258\n",
            "Epoch [92/100], Step [203/493], Loss: 0.0230\n",
            "Epoch [92/100], Step [204/493], Loss: 0.0045\n",
            "Epoch [92/100], Step [205/493], Loss: 0.0036\n",
            "Epoch [92/100], Step [206/493], Loss: 0.0022\n",
            "Epoch [92/100], Step [207/493], Loss: 0.0144\n",
            "Epoch [92/100], Step [208/493], Loss: 0.0154\n",
            "Epoch [92/100], Step [209/493], Loss: 0.0035\n",
            "Epoch [92/100], Step [210/493], Loss: 0.0227\n",
            "Epoch [92/100], Step [211/493], Loss: 0.0952\n",
            "Epoch [92/100], Step [212/493], Loss: 0.0022\n",
            "Epoch [92/100], Step [213/493], Loss: 0.0156\n",
            "Epoch [92/100], Step [214/493], Loss: 0.2984\n",
            "Epoch [92/100], Step [215/493], Loss: 0.0253\n",
            "Epoch [92/100], Step [216/493], Loss: 0.0011\n",
            "Epoch [92/100], Step [217/493], Loss: 0.0109\n",
            "Epoch [92/100], Step [218/493], Loss: 0.0060\n",
            "Epoch [92/100], Step [219/493], Loss: 0.0132\n",
            "Epoch [92/100], Step [220/493], Loss: 0.0078\n",
            "Epoch [92/100], Step [221/493], Loss: 0.0094\n",
            "Epoch [92/100], Step [222/493], Loss: 0.0202\n",
            "Epoch [92/100], Step [223/493], Loss: 0.0083\n",
            "Epoch [92/100], Step [224/493], Loss: 0.0069\n",
            "Epoch [92/100], Step [225/493], Loss: 0.0054\n",
            "Epoch [92/100], Step [226/493], Loss: 0.0109\n",
            "Epoch [92/100], Step [227/493], Loss: 0.0045\n",
            "Epoch [92/100], Step [228/493], Loss: 0.0045\n",
            "Epoch [92/100], Step [229/493], Loss: 0.0103\n",
            "Epoch [92/100], Step [230/493], Loss: 0.0074\n",
            "Epoch [92/100], Step [231/493], Loss: 0.0066\n",
            "Epoch [92/100], Step [232/493], Loss: 0.0055\n",
            "Epoch [92/100], Step [233/493], Loss: 0.0068\n",
            "Epoch [92/100], Step [234/493], Loss: 0.0163\n",
            "Epoch [92/100], Step [235/493], Loss: 0.0065\n",
            "Epoch [92/100], Step [236/493], Loss: 0.0194\n",
            "Epoch [92/100], Step [237/493], Loss: 0.0018\n",
            "Epoch [92/100], Step [238/493], Loss: 0.0054\n",
            "Epoch [92/100], Step [239/493], Loss: 0.0077\n",
            "Epoch [92/100], Step [240/493], Loss: 0.0179\n",
            "Epoch [92/100], Step [241/493], Loss: 0.0121\n",
            "Epoch [92/100], Step [242/493], Loss: 0.0046\n",
            "Epoch [92/100], Step [243/493], Loss: 0.0168\n",
            "Epoch [92/100], Step [244/493], Loss: 0.0230\n",
            "Epoch [92/100], Step [245/493], Loss: 0.0339\n",
            "Epoch [92/100], Step [246/493], Loss: 0.0001\n",
            "Epoch [92/100], Step [247/493], Loss: 0.0103\n",
            "Epoch [92/100], Step [248/493], Loss: 0.0872\n",
            "Epoch [92/100], Step [249/493], Loss: 0.0131\n",
            "Epoch [92/100], Step [250/493], Loss: 0.0023\n",
            "Epoch [92/100], Step [251/493], Loss: 0.0023\n",
            "Epoch [92/100], Step [252/493], Loss: 0.0005\n",
            "Epoch [92/100], Step [253/493], Loss: 0.0195\n",
            "Epoch [92/100], Step [254/493], Loss: 0.0084\n",
            "Epoch [92/100], Step [255/493], Loss: 0.0034\n",
            "Epoch [92/100], Step [256/493], Loss: 0.0026\n",
            "Epoch [92/100], Step [257/493], Loss: 0.0077\n",
            "Epoch [92/100], Step [258/493], Loss: 0.0062\n",
            "Epoch [92/100], Step [259/493], Loss: 0.0068\n",
            "Epoch [92/100], Step [260/493], Loss: 0.2338\n",
            "Epoch [92/100], Step [261/493], Loss: 0.2286\n",
            "Epoch [92/100], Step [262/493], Loss: 0.0030\n",
            "Epoch [92/100], Step [263/493], Loss: 0.0064\n",
            "Epoch [92/100], Step [264/493], Loss: 0.0228\n",
            "Epoch [92/100], Step [265/493], Loss: 0.0158\n",
            "Epoch [92/100], Step [266/493], Loss: 0.0039\n",
            "Epoch [92/100], Step [267/493], Loss: 0.0059\n",
            "Epoch [92/100], Step [268/493], Loss: 0.0016\n",
            "Epoch [92/100], Step [269/493], Loss: 0.0075\n",
            "Epoch [92/100], Step [270/493], Loss: 0.0229\n",
            "Epoch [92/100], Step [271/493], Loss: 0.0074\n",
            "Epoch [92/100], Step [272/493], Loss: 0.0344\n",
            "Epoch [92/100], Step [273/493], Loss: 0.0047\n",
            "Epoch [92/100], Step [274/493], Loss: 0.0035\n",
            "Epoch [92/100], Step [275/493], Loss: 0.0079\n",
            "Epoch [92/100], Step [276/493], Loss: 0.0043\n",
            "Epoch [92/100], Step [277/493], Loss: 0.0149\n",
            "Epoch [92/100], Step [278/493], Loss: 0.0058\n",
            "Epoch [92/100], Step [279/493], Loss: 0.0129\n",
            "Epoch [92/100], Step [280/493], Loss: 0.0131\n",
            "Epoch [92/100], Step [281/493], Loss: 0.0079\n",
            "Epoch [92/100], Step [282/493], Loss: 0.0121\n",
            "Epoch [92/100], Step [283/493], Loss: 0.0022\n",
            "Epoch [92/100], Step [284/493], Loss: 0.0062\n",
            "Epoch [92/100], Step [285/493], Loss: 0.0021\n",
            "Epoch [92/100], Step [286/493], Loss: 0.0070\n",
            "Epoch [92/100], Step [287/493], Loss: 0.0050\n",
            "Epoch [92/100], Step [288/493], Loss: 0.0178\n",
            "Epoch [92/100], Step [289/493], Loss: 0.0833\n",
            "Epoch [92/100], Step [290/493], Loss: 0.0095\n",
            "Epoch [92/100], Step [291/493], Loss: 0.0117\n",
            "Epoch [92/100], Step [292/493], Loss: 0.0098\n",
            "Epoch [92/100], Step [293/493], Loss: 0.0128\n",
            "Epoch [92/100], Step [294/493], Loss: 0.0071\n",
            "Epoch [92/100], Step [295/493], Loss: 0.0081\n",
            "Epoch [92/100], Step [296/493], Loss: 0.0340\n",
            "Epoch [92/100], Step [297/493], Loss: 0.0033\n",
            "Epoch [92/100], Step [298/493], Loss: 0.0770\n",
            "Epoch [92/100], Step [299/493], Loss: 0.1857\n",
            "Epoch [92/100], Step [300/493], Loss: 0.0073\n",
            "Epoch [92/100], Step [301/493], Loss: 0.0018\n",
            "Epoch [92/100], Step [302/493], Loss: 0.1286\n",
            "Epoch [92/100], Step [303/493], Loss: 0.0393\n",
            "Epoch [92/100], Step [304/493], Loss: 0.0316\n",
            "Epoch [92/100], Step [305/493], Loss: 0.0073\n",
            "Epoch [92/100], Step [306/493], Loss: 0.0100\n",
            "Epoch [92/100], Step [307/493], Loss: 0.0074\n",
            "Epoch [92/100], Step [308/493], Loss: 0.0092\n",
            "Epoch [92/100], Step [309/493], Loss: 0.0078\n",
            "Epoch [92/100], Step [310/493], Loss: 0.0123\n",
            "Epoch [92/100], Step [311/493], Loss: 0.0147\n",
            "Epoch [92/100], Step [312/493], Loss: 0.0264\n",
            "Epoch [92/100], Step [313/493], Loss: 0.0042\n",
            "Epoch [92/100], Step [314/493], Loss: 0.0136\n",
            "Epoch [92/100], Step [315/493], Loss: 0.0081\n",
            "Epoch [92/100], Step [316/493], Loss: 0.0097\n",
            "Epoch [92/100], Step [317/493], Loss: 0.0037\n",
            "Epoch [92/100], Step [318/493], Loss: 0.0252\n",
            "Epoch [92/100], Step [319/493], Loss: 0.0300\n",
            "Epoch [92/100], Step [320/493], Loss: 0.0094\n",
            "Epoch [92/100], Step [321/493], Loss: 0.0095\n",
            "Epoch [92/100], Step [322/493], Loss: 0.0010\n",
            "Epoch [92/100], Step [323/493], Loss: 0.0029\n",
            "Epoch [92/100], Step [324/493], Loss: 0.0081\n",
            "Epoch [92/100], Step [325/493], Loss: 0.0065\n",
            "Epoch [92/100], Step [326/493], Loss: 0.0597\n",
            "Epoch [92/100], Step [327/493], Loss: 0.0070\n",
            "Epoch [92/100], Step [328/493], Loss: 0.0059\n",
            "Epoch [92/100], Step [329/493], Loss: 0.1114\n",
            "Epoch [92/100], Step [330/493], Loss: 0.0067\n",
            "Epoch [92/100], Step [331/493], Loss: 0.0445\n",
            "Epoch [92/100], Step [332/493], Loss: 0.0098\n",
            "Epoch [92/100], Step [333/493], Loss: 0.0179\n",
            "Epoch [92/100], Step [334/493], Loss: 0.0054\n",
            "Epoch [92/100], Step [335/493], Loss: 0.0191\n",
            "Epoch [92/100], Step [336/493], Loss: 0.0039\n",
            "Epoch [92/100], Step [337/493], Loss: 0.0083\n",
            "Epoch [92/100], Step [338/493], Loss: 0.0132\n",
            "Epoch [92/100], Step [339/493], Loss: 0.0055\n",
            "Epoch [92/100], Step [340/493], Loss: 0.0049\n",
            "Epoch [92/100], Step [341/493], Loss: 0.0023\n",
            "Epoch [92/100], Step [342/493], Loss: 0.0138\n",
            "Epoch [92/100], Step [343/493], Loss: 0.0112\n",
            "Epoch [92/100], Step [344/493], Loss: 0.0141\n",
            "Epoch [92/100], Step [345/493], Loss: 0.0434\n",
            "Epoch [92/100], Step [346/493], Loss: 0.0110\n",
            "Epoch [92/100], Step [347/493], Loss: 0.0117\n",
            "Epoch [92/100], Step [348/493], Loss: 0.0382\n",
            "Epoch [92/100], Step [349/493], Loss: 0.0083\n",
            "Epoch [92/100], Step [350/493], Loss: 0.0092\n",
            "Epoch [92/100], Step [351/493], Loss: 0.0087\n",
            "Epoch [92/100], Step [352/493], Loss: 0.0030\n",
            "Epoch [92/100], Step [353/493], Loss: 0.0391\n",
            "Epoch [92/100], Step [354/493], Loss: 0.0166\n",
            "Epoch [92/100], Step [355/493], Loss: 0.0049\n",
            "Epoch [92/100], Step [356/493], Loss: 0.0029\n",
            "Epoch [92/100], Step [357/493], Loss: 0.0102\n",
            "Epoch [92/100], Step [358/493], Loss: 0.0070\n",
            "Epoch [92/100], Step [359/493], Loss: 0.0042\n",
            "Epoch [92/100], Step [360/493], Loss: 0.0116\n",
            "Epoch [92/100], Step [361/493], Loss: 0.0209\n",
            "Epoch [92/100], Step [362/493], Loss: 0.0040\n",
            "Epoch [92/100], Step [363/493], Loss: 0.0074\n",
            "Epoch [92/100], Step [364/493], Loss: 0.0078\n",
            "Epoch [92/100], Step [365/493], Loss: 0.0060\n",
            "Epoch [92/100], Step [366/493], Loss: 0.0118\n",
            "Epoch [92/100], Step [367/493], Loss: 0.0060\n",
            "Epoch [92/100], Step [368/493], Loss: 0.0065\n",
            "Epoch [92/100], Step [369/493], Loss: 0.1087\n",
            "Epoch [92/100], Step [370/493], Loss: 0.0308\n",
            "Epoch [92/100], Step [371/493], Loss: 0.0077\n",
            "Epoch [92/100], Step [372/493], Loss: 0.0117\n",
            "Epoch [92/100], Step [373/493], Loss: 0.0081\n",
            "Epoch [92/100], Step [374/493], Loss: 0.0114\n",
            "Epoch [92/100], Step [375/493], Loss: 0.0074\n",
            "Epoch [92/100], Step [376/493], Loss: 0.0047\n",
            "Epoch [92/100], Step [377/493], Loss: 0.0104\n",
            "Epoch [92/100], Step [378/493], Loss: 0.0210\n",
            "Epoch [92/100], Step [379/493], Loss: 0.0027\n",
            "Epoch [92/100], Step [380/493], Loss: 0.0087\n",
            "Epoch [92/100], Step [381/493], Loss: 0.0129\n",
            "Epoch [92/100], Step [382/493], Loss: 0.0084\n",
            "Epoch [92/100], Step [383/493], Loss: 0.0020\n",
            "Epoch [92/100], Step [384/493], Loss: 0.0032\n",
            "Epoch [92/100], Step [385/493], Loss: 0.0057\n",
            "Epoch [92/100], Step [386/493], Loss: 0.0086\n",
            "Epoch [92/100], Step [387/493], Loss: 0.0424\n",
            "Epoch [92/100], Step [388/493], Loss: 0.0071\n",
            "Epoch [92/100], Step [389/493], Loss: 0.0140\n",
            "Epoch [92/100], Step [390/493], Loss: 0.0075\n",
            "Epoch [92/100], Step [391/493], Loss: 0.0056\n",
            "Epoch [92/100], Step [392/493], Loss: 0.0116\n",
            "Epoch [92/100], Step [393/493], Loss: 0.0058\n",
            "Epoch [92/100], Step [394/493], Loss: 0.0121\n",
            "Epoch [92/100], Step [395/493], Loss: 0.0080\n",
            "Epoch [92/100], Step [396/493], Loss: 0.0032\n",
            "Epoch [92/100], Step [397/493], Loss: 0.0092\n",
            "Epoch [92/100], Step [398/493], Loss: 0.0039\n",
            "Epoch [92/100], Step [399/493], Loss: 0.0057\n",
            "Epoch [92/100], Step [400/493], Loss: 0.0088\n",
            "Epoch [92/100], Step [401/493], Loss: 0.0057\n",
            "Epoch [92/100], Step [402/493], Loss: 0.0382\n",
            "Epoch [92/100], Step [403/493], Loss: 0.0657\n",
            "Epoch [92/100], Step [404/493], Loss: 0.0305\n",
            "Epoch [92/100], Step [405/493], Loss: 0.0250\n",
            "Epoch [92/100], Step [406/493], Loss: 0.0268\n",
            "Epoch [92/100], Step [407/493], Loss: 0.0686\n",
            "Epoch [92/100], Step [408/493], Loss: 0.0017\n",
            "Epoch [92/100], Step [409/493], Loss: 0.0055\n",
            "Epoch [92/100], Step [410/493], Loss: 0.0014\n",
            "Epoch [92/100], Step [411/493], Loss: 0.0051\n",
            "Epoch [92/100], Step [412/493], Loss: 0.0059\n",
            "Epoch [92/100], Step [413/493], Loss: 0.0100\n",
            "Epoch [92/100], Step [414/493], Loss: 0.0011\n",
            "Epoch [92/100], Step [415/493], Loss: 0.0244\n",
            "Epoch [92/100], Step [416/493], Loss: 0.0074\n",
            "Epoch [92/100], Step [417/493], Loss: 0.3149\n",
            "Epoch [92/100], Step [418/493], Loss: 0.0054\n",
            "Epoch [92/100], Step [419/493], Loss: 0.0063\n",
            "Epoch [92/100], Step [420/493], Loss: 0.0298\n",
            "Epoch [92/100], Step [421/493], Loss: 0.0264\n",
            "Epoch [92/100], Step [422/493], Loss: 0.0200\n",
            "Epoch [92/100], Step [423/493], Loss: 0.0074\n",
            "Epoch [92/100], Step [424/493], Loss: 0.0001\n",
            "Epoch [92/100], Step [425/493], Loss: 0.0014\n",
            "Epoch [92/100], Step [426/493], Loss: 0.1971\n",
            "Epoch [92/100], Step [427/493], Loss: 0.0263\n",
            "Epoch [92/100], Step [428/493], Loss: 0.0094\n",
            "Epoch [92/100], Step [429/493], Loss: 0.0284\n",
            "Epoch [92/100], Step [430/493], Loss: 0.0131\n",
            "Epoch [92/100], Step [431/493], Loss: 0.0108\n",
            "Epoch [92/100], Step [432/493], Loss: 0.0054\n",
            "Epoch [92/100], Step [433/493], Loss: 0.0058\n",
            "Epoch [92/100], Step [434/493], Loss: 0.0456\n",
            "Epoch [92/100], Step [435/493], Loss: 0.0035\n",
            "Epoch [92/100], Step [436/493], Loss: 0.0263\n",
            "Epoch [92/100], Step [437/493], Loss: 0.5350\n",
            "Epoch [92/100], Step [438/493], Loss: 0.0061\n",
            "Epoch [92/100], Step [439/493], Loss: 0.0075\n",
            "Epoch [92/100], Step [440/493], Loss: 0.0131\n",
            "Epoch [92/100], Step [441/493], Loss: 0.0268\n",
            "Epoch [92/100], Step [442/493], Loss: 0.0096\n",
            "Epoch [92/100], Step [443/493], Loss: 0.0206\n",
            "Epoch [92/100], Step [444/493], Loss: 0.0097\n",
            "Epoch [92/100], Step [445/493], Loss: 0.0060\n",
            "Epoch [92/100], Step [446/493], Loss: 0.0097\n",
            "Epoch [92/100], Step [447/493], Loss: 0.0295\n",
            "Epoch [92/100], Step [448/493], Loss: 0.0105\n",
            "Epoch [92/100], Step [449/493], Loss: 0.0413\n",
            "Epoch [92/100], Step [450/493], Loss: 0.0079\n",
            "Epoch [92/100], Step [451/493], Loss: 0.0067\n",
            "Epoch [92/100], Step [452/493], Loss: 0.0024\n",
            "Epoch [92/100], Step [453/493], Loss: 0.0280\n",
            "Epoch [92/100], Step [454/493], Loss: 0.0289\n",
            "Epoch [92/100], Step [455/493], Loss: 0.0003\n",
            "Epoch [92/100], Step [456/493], Loss: 0.0040\n",
            "Epoch [92/100], Step [457/493], Loss: 0.0029\n",
            "Epoch [92/100], Step [458/493], Loss: 0.0092\n",
            "Epoch [92/100], Step [459/493], Loss: 0.0141\n",
            "Epoch [92/100], Step [460/493], Loss: 0.0003\n",
            "Epoch [92/100], Step [461/493], Loss: 0.0033\n",
            "Epoch [92/100], Step [462/493], Loss: 0.0109\n",
            "Epoch [92/100], Step [463/493], Loss: 0.0175\n",
            "Epoch [92/100], Step [464/493], Loss: 0.0057\n",
            "Epoch [92/100], Step [465/493], Loss: 0.1099\n",
            "Epoch [92/100], Step [466/493], Loss: 0.0070\n",
            "Epoch [92/100], Step [467/493], Loss: 0.0119\n",
            "Epoch [92/100], Step [468/493], Loss: 0.2699\n",
            "Epoch [92/100], Step [469/493], Loss: 0.0280\n",
            "Epoch [92/100], Step [470/493], Loss: 0.1790\n",
            "Epoch [92/100], Step [471/493], Loss: 0.0893\n",
            "Epoch [92/100], Step [472/493], Loss: 0.0072\n",
            "Epoch [92/100], Step [473/493], Loss: 0.0180\n",
            "Epoch [92/100], Step [474/493], Loss: 0.0077\n",
            "Epoch [92/100], Step [475/493], Loss: 0.0029\n",
            "Epoch [92/100], Step [476/493], Loss: 0.0838\n",
            "Epoch [92/100], Step [477/493], Loss: 0.0033\n",
            "Epoch [92/100], Step [478/493], Loss: 0.0040\n",
            "Epoch [92/100], Step [479/493], Loss: 0.0092\n",
            "Epoch [92/100], Step [480/493], Loss: 0.0131\n",
            "Epoch [92/100], Step [481/493], Loss: 0.0053\n",
            "Epoch [92/100], Step [482/493], Loss: 0.0001\n",
            "Epoch [92/100], Step [483/493], Loss: 0.0041\n",
            "Epoch [92/100], Step [484/493], Loss: 0.0033\n",
            "Epoch [92/100], Step [485/493], Loss: 0.0110\n",
            "Epoch [92/100], Step [486/493], Loss: 0.0044\n",
            "Epoch [92/100], Step [487/493], Loss: 0.0018\n",
            "Epoch [92/100], Step [488/493], Loss: 0.0369\n",
            "Epoch [92/100], Step [489/493], Loss: 0.0100\n",
            "Epoch [92/100], Step [490/493], Loss: 0.0022\n",
            "Epoch [92/100], Step [491/493], Loss: 0.0100\n",
            "Epoch [92/100], Step [492/493], Loss: 0.0081\n",
            "Epoch [92/100], Step [493/493], Loss: 0.0105\n",
            "Epoch [93/100], Step [1/493], Loss: 0.0229\n",
            "Epoch [93/100], Step [2/493], Loss: 0.0116\n",
            "Epoch [93/100], Step [3/493], Loss: 0.0018\n",
            "Epoch [93/100], Step [4/493], Loss: 0.0141\n",
            "Epoch [93/100], Step [5/493], Loss: 0.0057\n",
            "Epoch [93/100], Step [6/493], Loss: 0.1971\n",
            "Epoch [93/100], Step [7/493], Loss: 0.0084\n",
            "Epoch [93/100], Step [8/493], Loss: 0.0445\n",
            "Epoch [93/100], Step [9/493], Loss: 0.0045\n",
            "Epoch [93/100], Step [10/493], Loss: 0.0035\n",
            "Epoch [93/100], Step [11/493], Loss: 0.0031\n",
            "Epoch [93/100], Step [12/493], Loss: 0.0095\n",
            "Epoch [93/100], Step [13/493], Loss: 0.0472\n",
            "Epoch [93/100], Step [14/493], Loss: 0.0077\n",
            "Epoch [93/100], Step [15/493], Loss: 0.0103\n",
            "Epoch [93/100], Step [16/493], Loss: 0.0369\n",
            "Epoch [93/100], Step [17/493], Loss: 0.0011\n",
            "Epoch [93/100], Step [18/493], Loss: 0.0025\n",
            "Epoch [93/100], Step [19/493], Loss: 0.0001\n",
            "Epoch [93/100], Step [20/493], Loss: 0.0083\n",
            "Epoch [93/100], Step [21/493], Loss: 0.0014\n",
            "Epoch [93/100], Step [22/493], Loss: 0.0035\n",
            "Epoch [93/100], Step [23/493], Loss: 0.0019\n",
            "Epoch [93/100], Step [24/493], Loss: 0.0163\n",
            "Epoch [93/100], Step [25/493], Loss: 0.0200\n",
            "Epoch [93/100], Step [26/493], Loss: 0.0230\n",
            "Epoch [93/100], Step [27/493], Loss: 0.0131\n",
            "Epoch [93/100], Step [28/493], Loss: 0.0076\n",
            "Epoch [93/100], Step [29/493], Loss: 0.0109\n",
            "Epoch [93/100], Step [30/493], Loss: 0.0092\n",
            "Epoch [93/100], Step [31/493], Loss: 0.2245\n",
            "Epoch [93/100], Step [32/493], Loss: 0.0277\n",
            "Epoch [93/100], Step [33/493], Loss: 0.0166\n",
            "Epoch [93/100], Step [34/493], Loss: 0.0101\n",
            "Epoch [93/100], Step [35/493], Loss: 0.0029\n",
            "Epoch [93/100], Step [36/493], Loss: 0.1286\n",
            "Epoch [93/100], Step [37/493], Loss: 0.0086\n",
            "Epoch [93/100], Step [38/493], Loss: 0.1181\n",
            "Epoch [93/100], Step [39/493], Loss: 0.0094\n",
            "Epoch [93/100], Step [40/493], Loss: 0.0252\n",
            "Epoch [93/100], Step [41/493], Loss: 0.0179\n",
            "Epoch [93/100], Step [42/493], Loss: 0.0086\n",
            "Epoch [93/100], Step [43/493], Loss: 0.0121\n",
            "Epoch [93/100], Step [44/493], Loss: 0.1666\n",
            "Epoch [93/100], Step [45/493], Loss: 0.0081\n",
            "Epoch [93/100], Step [46/493], Loss: 0.0124\n",
            "Epoch [93/100], Step [47/493], Loss: 0.0074\n",
            "Epoch [93/100], Step [48/493], Loss: 0.0161\n",
            "Epoch [93/100], Step [49/493], Loss: 0.0077\n",
            "Epoch [93/100], Step [50/493], Loss: 0.0033\n",
            "Epoch [93/100], Step [51/493], Loss: 0.0003\n",
            "Epoch [93/100], Step [52/493], Loss: 0.0047\n",
            "Epoch [93/100], Step [53/493], Loss: 0.0041\n",
            "Epoch [93/100], Step [54/493], Loss: 0.0023\n",
            "Epoch [93/100], Step [55/493], Loss: 0.0031\n",
            "Epoch [93/100], Step [56/493], Loss: 0.0087\n",
            "Epoch [93/100], Step [57/493], Loss: 0.0084\n",
            "Epoch [93/100], Step [58/493], Loss: 0.0412\n",
            "Epoch [93/100], Step [59/493], Loss: 0.0057\n",
            "Epoch [93/100], Step [60/493], Loss: 0.0597\n",
            "Epoch [93/100], Step [61/493], Loss: 0.0085\n",
            "Epoch [93/100], Step [62/493], Loss: 0.0110\n",
            "Epoch [93/100], Step [63/493], Loss: 0.0093\n",
            "Epoch [93/100], Step [64/493], Loss: 0.0005\n",
            "Epoch [93/100], Step [65/493], Loss: 0.0149\n",
            "Epoch [93/100], Step [66/493], Loss: 0.0003\n",
            "Epoch [93/100], Step [67/493], Loss: 0.0092\n",
            "Epoch [93/100], Step [68/493], Loss: 0.0382\n",
            "Epoch [93/100], Step [69/493], Loss: 0.0121\n",
            "Epoch [93/100], Step [70/493], Loss: 0.0054\n",
            "Epoch [93/100], Step [71/493], Loss: 0.0069\n",
            "Epoch [93/100], Step [72/493], Loss: 0.0153\n",
            "Epoch [93/100], Step [73/493], Loss: 0.4792\n",
            "Epoch [93/100], Step [74/493], Loss: 0.0007\n",
            "Epoch [93/100], Step [75/493], Loss: 0.0102\n",
            "Epoch [93/100], Step [76/493], Loss: 0.1790\n",
            "Epoch [93/100], Step [77/493], Loss: 0.0030\n",
            "Epoch [93/100], Step [78/493], Loss: 0.2259\n",
            "Epoch [93/100], Step [79/493], Loss: 0.0079\n",
            "Epoch [93/100], Step [80/493], Loss: 0.0126\n",
            "Epoch [93/100], Step [81/493], Loss: 0.0316\n",
            "Epoch [93/100], Step [82/493], Loss: 0.0026\n",
            "Epoch [93/100], Step [83/493], Loss: 0.0070\n",
            "Epoch [93/100], Step [84/493], Loss: 0.0360\n",
            "Epoch [93/100], Step [85/493], Loss: 0.0017\n",
            "Epoch [93/100], Step [86/493], Loss: 0.0141\n",
            "Epoch [93/100], Step [87/493], Loss: 0.0053\n",
            "Epoch [93/100], Step [88/493], Loss: 0.0097\n",
            "Epoch [93/100], Step [89/493], Loss: 0.0014\n",
            "Epoch [93/100], Step [90/493], Loss: 0.0058\n",
            "Epoch [93/100], Step [91/493], Loss: 0.0132\n",
            "Epoch [93/100], Step [92/493], Loss: 0.0049\n",
            "Epoch [93/100], Step [93/493], Loss: 0.0105\n",
            "Epoch [93/100], Step [94/493], Loss: 0.0016\n",
            "Epoch [93/100], Step [95/493], Loss: 0.0206\n",
            "Epoch [93/100], Step [96/493], Loss: 0.1087\n",
            "Epoch [93/100], Step [97/493], Loss: 0.0253\n",
            "Epoch [93/100], Step [98/493], Loss: 0.0893\n",
            "Epoch [93/100], Step [99/493], Loss: 0.0112\n",
            "Epoch [93/100], Step [100/493], Loss: 0.1099\n",
            "Epoch [93/100], Step [101/493], Loss: 0.0089\n",
            "Epoch [93/100], Step [102/493], Loss: 0.0191\n",
            "Epoch [93/100], Step [103/493], Loss: 0.0071\n",
            "Epoch [93/100], Step [104/493], Loss: 0.0046\n",
            "Epoch [93/100], Step [105/493], Loss: 0.0010\n",
            "Epoch [93/100], Step [106/493], Loss: 0.0117\n",
            "Epoch [93/100], Step [107/493], Loss: 0.0075\n",
            "Epoch [93/100], Step [108/493], Loss: 0.0060\n",
            "Epoch [93/100], Step [109/493], Loss: 0.0289\n",
            "Epoch [93/100], Step [110/493], Loss: 0.0456\n",
            "Epoch [93/100], Step [111/493], Loss: 0.0124\n",
            "Epoch [93/100], Step [112/493], Loss: 0.0228\n",
            "Epoch [93/100], Step [113/493], Loss: 0.0263\n",
            "Epoch [93/100], Step [114/493], Loss: 0.0290\n",
            "Epoch [93/100], Step [115/493], Loss: 0.0060\n",
            "Epoch [93/100], Step [116/493], Loss: 0.0132\n",
            "Epoch [93/100], Step [117/493], Loss: 0.0070\n",
            "Epoch [93/100], Step [118/493], Loss: 0.0046\n",
            "Epoch [93/100], Step [119/493], Loss: 0.0045\n",
            "Epoch [93/100], Step [120/493], Loss: 0.0128\n",
            "Epoch [93/100], Step [121/493], Loss: 0.0099\n",
            "Epoch [93/100], Step [122/493], Loss: 0.0144\n",
            "Epoch [93/100], Step [123/493], Loss: 0.0434\n",
            "Epoch [93/100], Step [124/493], Loss: 0.0167\n",
            "Epoch [93/100], Step [125/493], Loss: 0.0067\n",
            "Epoch [93/100], Step [126/493], Loss: 0.0094\n",
            "Epoch [93/100], Step [127/493], Loss: 0.0042\n",
            "Epoch [93/100], Step [128/493], Loss: 0.0059\n",
            "Epoch [93/100], Step [129/493], Loss: 0.0040\n",
            "Epoch [93/100], Step [130/493], Loss: 0.0180\n",
            "Epoch [93/100], Step [131/493], Loss: 0.0096\n",
            "Epoch [93/100], Step [132/493], Loss: 0.0066\n",
            "Epoch [93/100], Step [133/493], Loss: 0.5350\n",
            "Epoch [93/100], Step [134/493], Loss: 0.0035\n",
            "Epoch [93/100], Step [135/493], Loss: 0.0096\n",
            "Epoch [93/100], Step [136/493], Loss: 0.0067\n",
            "Epoch [93/100], Step [137/493], Loss: 0.0048\n",
            "Epoch [93/100], Step [138/493], Loss: 0.2135\n",
            "Epoch [93/100], Step [139/493], Loss: 0.0147\n",
            "Epoch [93/100], Step [140/493], Loss: 0.0202\n",
            "Epoch [93/100], Step [141/493], Loss: 0.0253\n",
            "Epoch [93/100], Step [142/493], Loss: 0.0092\n",
            "Epoch [93/100], Step [143/493], Loss: 0.0250\n",
            "Epoch [93/100], Step [144/493], Loss: 0.0092\n",
            "Epoch [93/100], Step [145/493], Loss: 0.0300\n",
            "Epoch [93/100], Step [146/493], Loss: 0.0045\n",
            "Epoch [93/100], Step [147/493], Loss: 0.0018\n",
            "Epoch [93/100], Step [148/493], Loss: 0.0264\n",
            "Epoch [93/100], Step [149/493], Loss: 0.0029\n",
            "Epoch [93/100], Step [150/493], Loss: 0.0077\n",
            "Epoch [93/100], Step [151/493], Loss: 0.0124\n",
            "Epoch [93/100], Step [152/493], Loss: 0.0040\n",
            "Epoch [93/100], Step [153/493], Loss: 0.0195\n",
            "Epoch [93/100], Step [154/493], Loss: 0.0081\n",
            "Epoch [93/100], Step [155/493], Loss: 0.0227\n",
            "Epoch [93/100], Step [156/493], Loss: 0.0280\n",
            "Epoch [93/100], Step [157/493], Loss: 0.0099\n",
            "Epoch [93/100], Step [158/493], Loss: 0.0050\n",
            "Epoch [93/100], Step [159/493], Loss: 0.0037\n",
            "Epoch [93/100], Step [160/493], Loss: 0.0174\n",
            "Epoch [93/100], Step [161/493], Loss: 0.0578\n",
            "Epoch [93/100], Step [162/493], Loss: 0.0170\n",
            "Epoch [93/100], Step [163/493], Loss: 0.0070\n",
            "Epoch [93/100], Step [164/493], Loss: 0.0117\n",
            "Epoch [93/100], Step [165/493], Loss: 0.2286\n",
            "Epoch [93/100], Step [166/493], Loss: 0.0039\n",
            "Epoch [93/100], Step [167/493], Loss: 0.0012\n",
            "Epoch [93/100], Step [168/493], Loss: 0.0145\n",
            "Epoch [93/100], Step [169/493], Loss: 0.0051\n",
            "Epoch [93/100], Step [170/493], Loss: 0.0340\n",
            "Epoch [93/100], Step [171/493], Loss: 0.0128\n",
            "Epoch [93/100], Step [172/493], Loss: 0.0040\n",
            "Epoch [93/100], Step [173/493], Loss: 0.0104\n",
            "Epoch [93/100], Step [174/493], Loss: 0.0121\n",
            "Epoch [93/100], Step [175/493], Loss: 0.0098\n",
            "Epoch [93/100], Step [176/493], Loss: 0.0014\n",
            "Epoch [93/100], Step [177/493], Loss: 0.0091\n",
            "Epoch [93/100], Step [178/493], Loss: 0.0264\n",
            "Epoch [93/100], Step [179/493], Loss: 0.0240\n",
            "Epoch [93/100], Step [180/493], Loss: 0.0121\n",
            "Epoch [93/100], Step [181/493], Loss: 0.0059\n",
            "Epoch [93/100], Step [182/493], Loss: 0.0230\n",
            "Epoch [93/100], Step [183/493], Loss: 0.0016\n",
            "Epoch [93/100], Step [184/493], Loss: 0.0033\n",
            "Epoch [93/100], Step [185/493], Loss: 0.0019\n",
            "Epoch [93/100], Step [186/493], Loss: 0.0040\n",
            "Epoch [93/100], Step [187/493], Loss: 0.0018\n",
            "Epoch [93/100], Step [188/493], Loss: 0.0109\n",
            "Epoch [93/100], Step [189/493], Loss: 0.0093\n",
            "Epoch [93/100], Step [190/493], Loss: 0.0268\n",
            "Epoch [93/100], Step [191/493], Loss: 0.0088\n",
            "Epoch [93/100], Step [192/493], Loss: 0.0029\n",
            "Epoch [93/100], Step [193/493], Loss: 0.0277\n",
            "Epoch [93/100], Step [194/493], Loss: 0.0032\n",
            "Epoch [93/100], Step [195/493], Loss: 0.0109\n",
            "Epoch [93/100], Step [196/493], Loss: 0.0340\n",
            "Epoch [93/100], Step [197/493], Loss: 0.0079\n",
            "Epoch [93/100], Step [198/493], Loss: 0.0059\n",
            "Epoch [93/100], Step [199/493], Loss: 0.0043\n",
            "Epoch [93/100], Step [200/493], Loss: 0.0043\n",
            "Epoch [93/100], Step [201/493], Loss: 0.0061\n",
            "Epoch [93/100], Step [202/493], Loss: 0.0045\n",
            "Epoch [93/100], Step [203/493], Loss: 0.0258\n",
            "Epoch [93/100], Step [204/493], Loss: 0.0042\n",
            "Epoch [93/100], Step [205/493], Loss: 0.0123\n",
            "Epoch [93/100], Step [206/493], Loss: 0.0037\n",
            "Epoch [93/100], Step [207/493], Loss: 0.0027\n",
            "Epoch [93/100], Step [208/493], Loss: 0.0072\n",
            "Epoch [93/100], Step [209/493], Loss: 0.0073\n",
            "Epoch [93/100], Step [210/493], Loss: 0.0074\n",
            "Epoch [93/100], Step [211/493], Loss: 0.0083\n",
            "Epoch [93/100], Step [212/493], Loss: 0.0063\n",
            "Epoch [93/100], Step [213/493], Loss: 0.0030\n",
            "Epoch [93/100], Step [214/493], Loss: 0.0109\n",
            "Epoch [93/100], Step [215/493], Loss: 0.0039\n",
            "Epoch [93/100], Step [216/493], Loss: 0.0022\n",
            "Epoch [93/100], Step [217/493], Loss: 0.0305\n",
            "Epoch [93/100], Step [218/493], Loss: 0.0051\n",
            "Epoch [93/100], Step [219/493], Loss: 0.0057\n",
            "Epoch [93/100], Step [220/493], Loss: 0.0298\n",
            "Epoch [93/100], Step [221/493], Loss: 0.0140\n",
            "Epoch [93/100], Step [222/493], Loss: 0.0099\n",
            "Epoch [93/100], Step [223/493], Loss: 0.0054\n",
            "Epoch [93/100], Step [224/493], Loss: 0.0047\n",
            "Epoch [93/100], Step [225/493], Loss: 0.0058\n",
            "Epoch [93/100], Step [226/493], Loss: 0.0062\n",
            "Epoch [93/100], Step [227/493], Loss: 0.0022\n",
            "Epoch [93/100], Step [228/493], Loss: 0.0001\n",
            "Epoch [93/100], Step [229/493], Loss: 0.0054\n",
            "Epoch [93/100], Step [230/493], Loss: 0.0295\n",
            "Epoch [93/100], Step [231/493], Loss: 0.0129\n",
            "Epoch [93/100], Step [232/493], Loss: 0.0225\n",
            "Epoch [93/100], Step [233/493], Loss: 0.0339\n",
            "Epoch [93/100], Step [234/493], Loss: 0.0096\n",
            "Epoch [93/100], Step [235/493], Loss: 0.0179\n",
            "Epoch [93/100], Step [236/493], Loss: 0.0078\n",
            "Epoch [93/100], Step [237/493], Loss: 0.0154\n",
            "Epoch [93/100], Step [238/493], Loss: 0.0080\n",
            "Epoch [93/100], Step [239/493], Loss: 0.0209\n",
            "Epoch [93/100], Step [240/493], Loss: 0.0263\n",
            "Epoch [93/100], Step [241/493], Loss: 0.2699\n",
            "Epoch [93/100], Step [242/493], Loss: 0.0077\n",
            "Epoch [93/100], Step [243/493], Loss: 0.0031\n",
            "Epoch [93/100], Step [244/493], Loss: 0.0077\n",
            "Epoch [93/100], Step [245/493], Loss: 0.0011\n",
            "Epoch [93/100], Step [246/493], Loss: 0.0023\n",
            "Epoch [93/100], Step [247/493], Loss: 0.0092\n",
            "Epoch [93/100], Step [248/493], Loss: 0.0029\n",
            "Epoch [93/100], Step [249/493], Loss: 0.0031\n",
            "Epoch [93/100], Step [250/493], Loss: 0.0032\n",
            "Epoch [93/100], Step [251/493], Loss: 0.0065\n",
            "Epoch [93/100], Step [252/493], Loss: 0.0244\n",
            "Epoch [93/100], Step [253/493], Loss: 0.0097\n",
            "Epoch [93/100], Step [254/493], Loss: 0.0133\n",
            "Epoch [93/100], Step [255/493], Loss: 0.0072\n",
            "Epoch [93/100], Step [256/493], Loss: 0.0952\n",
            "Epoch [93/100], Step [257/493], Loss: 0.1575\n",
            "Epoch [93/100], Step [258/493], Loss: 0.0095\n",
            "Epoch [93/100], Step [259/493], Loss: 0.0146\n",
            "Epoch [93/100], Step [260/493], Loss: 0.0081\n",
            "Epoch [93/100], Step [261/493], Loss: 0.0097\n",
            "Epoch [93/100], Step [262/493], Loss: 0.0033\n",
            "Epoch [93/100], Step [263/493], Loss: 0.0084\n",
            "Epoch [93/100], Step [264/493], Loss: 0.0084\n",
            "Epoch [93/100], Step [265/493], Loss: 0.0045\n",
            "Epoch [93/100], Step [266/493], Loss: 0.0015\n",
            "Epoch [93/100], Step [267/493], Loss: 0.0143\n",
            "Epoch [93/100], Step [268/493], Loss: 0.0065\n",
            "Epoch [93/100], Step [269/493], Loss: 0.0059\n",
            "Epoch [93/100], Step [270/493], Loss: 0.0059\n",
            "Epoch [93/100], Step [271/493], Loss: 0.0403\n",
            "Epoch [93/100], Step [272/493], Loss: 0.0058\n",
            "Epoch [93/100], Step [273/493], Loss: 0.0143\n",
            "Epoch [93/100], Step [274/493], Loss: 0.0064\n",
            "Epoch [93/100], Step [275/493], Loss: 0.0284\n",
            "Epoch [93/100], Step [276/493], Loss: 0.0080\n",
            "Epoch [93/100], Step [277/493], Loss: 0.0136\n",
            "Epoch [93/100], Step [278/493], Loss: 0.0098\n",
            "Epoch [93/100], Step [279/493], Loss: 0.1857\n",
            "Epoch [93/100], Step [280/493], Loss: 0.0068\n",
            "Epoch [93/100], Step [281/493], Loss: 0.0065\n",
            "Epoch [93/100], Step [282/493], Loss: 0.0052\n",
            "Epoch [93/100], Step [283/493], Loss: 0.0095\n",
            "Epoch [93/100], Step [284/493], Loss: 0.0060\n",
            "Epoch [93/100], Step [285/493], Loss: 0.0011\n",
            "Epoch [93/100], Step [286/493], Loss: 0.1114\n",
            "Epoch [93/100], Step [287/493], Loss: 0.0086\n",
            "Epoch [93/100], Step [288/493], Loss: 0.0685\n",
            "Epoch [93/100], Step [289/493], Loss: 0.0156\n",
            "Epoch [93/100], Step [290/493], Loss: 0.0037\n",
            "Epoch [93/100], Step [291/493], Loss: 0.0208\n",
            "Epoch [93/100], Step [292/493], Loss: 0.0686\n",
            "Epoch [93/100], Step [293/493], Loss: 0.0087\n",
            "Epoch [93/100], Step [294/493], Loss: 0.2338\n",
            "Epoch [93/100], Step [295/493], Loss: 0.0022\n",
            "Epoch [93/100], Step [296/493], Loss: 0.0175\n",
            "Epoch [93/100], Step [297/493], Loss: 0.0033\n",
            "Epoch [93/100], Step [298/493], Loss: 0.0110\n",
            "Epoch [93/100], Step [299/493], Loss: 0.0033\n",
            "Epoch [93/100], Step [300/493], Loss: 0.0103\n",
            "Epoch [93/100], Step [301/493], Loss: 0.0145\n",
            "Epoch [93/100], Step [302/493], Loss: 0.0083\n",
            "Epoch [93/100], Step [303/493], Loss: 0.1160\n",
            "Epoch [93/100], Step [304/493], Loss: 0.0073\n",
            "Epoch [93/100], Step [305/493], Loss: 0.0029\n",
            "Epoch [93/100], Step [306/493], Loss: 0.0065\n",
            "Epoch [93/100], Step [307/493], Loss: 0.0138\n",
            "Epoch [93/100], Step [308/493], Loss: 0.0035\n",
            "Epoch [93/100], Step [309/493], Loss: 0.0039\n",
            "Epoch [93/100], Step [310/493], Loss: 0.0044\n",
            "Epoch [93/100], Step [311/493], Loss: 0.0058\n",
            "Epoch [93/100], Step [312/493], Loss: 0.0870\n",
            "Epoch [93/100], Step [313/493], Loss: 0.0002\n",
            "Epoch [93/100], Step [314/493], Loss: 0.0074\n",
            "Epoch [93/100], Step [315/493], Loss: 0.0042\n",
            "Epoch [93/100], Step [316/493], Loss: 0.0056\n",
            "Epoch [93/100], Step [317/493], Loss: 0.0075\n",
            "Epoch [93/100], Step [318/493], Loss: 0.0100\n",
            "Epoch [93/100], Step [319/493], Loss: 0.0023\n",
            "Epoch [93/100], Step [320/493], Loss: 0.0097\n",
            "Epoch [93/100], Step [321/493], Loss: 0.0073\n",
            "Epoch [93/100], Step [322/493], Loss: 0.0039\n",
            "Epoch [93/100], Step [323/493], Loss: 0.0045\n",
            "Epoch [93/100], Step [324/493], Loss: 0.0044\n",
            "Epoch [93/100], Step [325/493], Loss: 0.0068\n",
            "Epoch [93/100], Step [326/493], Loss: 0.0131\n",
            "Epoch [93/100], Step [327/493], Loss: 0.0833\n",
            "Epoch [93/100], Step [328/493], Loss: 0.0074\n",
            "Epoch [93/100], Step [329/493], Loss: 0.0072\n",
            "Epoch [93/100], Step [330/493], Loss: 0.0131\n",
            "Epoch [93/100], Step [331/493], Loss: 0.0133\n",
            "Epoch [93/100], Step [332/493], Loss: 0.0087\n",
            "Epoch [93/100], Step [333/493], Loss: 0.0036\n",
            "Epoch [93/100], Step [334/493], Loss: 0.0024\n",
            "Epoch [93/100], Step [335/493], Loss: 0.0027\n",
            "Epoch [93/100], Step [336/493], Loss: 0.0382\n",
            "Epoch [93/100], Step [337/493], Loss: 0.0022\n",
            "Epoch [93/100], Step [338/493], Loss: 0.0067\n",
            "Epoch [93/100], Step [339/493], Loss: 0.0033\n",
            "Epoch [93/100], Step [340/493], Loss: 0.0178\n",
            "Epoch [93/100], Step [341/493], Loss: 0.0141\n",
            "Epoch [93/100], Step [342/493], Loss: 0.0280\n",
            "Epoch [93/100], Step [343/493], Loss: 0.0194\n",
            "Epoch [93/100], Step [344/493], Loss: 0.0054\n",
            "Epoch [93/100], Step [345/493], Loss: 0.0116\n",
            "Epoch [93/100], Step [346/493], Loss: 0.0344\n",
            "Epoch [93/100], Step [347/493], Loss: 0.0032\n",
            "Epoch [93/100], Step [348/493], Loss: 0.0105\n",
            "Epoch [93/100], Step [349/493], Loss: 0.0020\n",
            "Epoch [93/100], Step [350/493], Loss: 0.0059\n",
            "Epoch [93/100], Step [351/493], Loss: 0.0036\n",
            "Epoch [93/100], Step [352/493], Loss: 0.0029\n",
            "Epoch [93/100], Step [353/493], Loss: 0.0139\n",
            "Epoch [93/100], Step [354/493], Loss: 0.0010\n",
            "Epoch [93/100], Step [355/493], Loss: 0.0181\n",
            "Epoch [93/100], Step [356/493], Loss: 0.0123\n",
            "Epoch [93/100], Step [357/493], Loss: 0.0059\n",
            "Epoch [93/100], Step [358/493], Loss: 0.0147\n",
            "Epoch [93/100], Step [359/493], Loss: 0.0055\n",
            "Epoch [93/100], Step [360/493], Loss: 0.2984\n",
            "Epoch [93/100], Step [361/493], Loss: 0.0330\n",
            "Epoch [93/100], Step [362/493], Loss: 0.0154\n",
            "Epoch [93/100], Step [363/493], Loss: 0.0108\n",
            "Epoch [93/100], Step [364/493], Loss: 0.0046\n",
            "Epoch [93/100], Step [365/493], Loss: 0.0102\n",
            "Epoch [93/100], Step [366/493], Loss: 0.0136\n",
            "Epoch [93/100], Step [367/493], Loss: 0.0259\n",
            "Epoch [93/100], Step [368/493], Loss: 0.0042\n",
            "Epoch [93/100], Step [369/493], Loss: 0.0023\n",
            "Epoch [93/100], Step [370/493], Loss: 0.0081\n",
            "Epoch [93/100], Step [371/493], Loss: 0.0065\n",
            "Epoch [93/100], Step [372/493], Loss: 0.0074\n",
            "Epoch [93/100], Step [373/493], Loss: 0.0078\n",
            "Epoch [93/100], Step [374/493], Loss: 0.0069\n",
            "Epoch [93/100], Step [375/493], Loss: 0.0077\n",
            "Epoch [93/100], Step [376/493], Loss: 0.0043\n",
            "Epoch [93/100], Step [377/493], Loss: 0.0080\n",
            "Epoch [93/100], Step [378/493], Loss: 0.0075\n",
            "Epoch [93/100], Step [379/493], Loss: 0.0325\n",
            "Epoch [93/100], Step [380/493], Loss: 0.0872\n",
            "Epoch [93/100], Step [381/493], Loss: 0.0117\n",
            "Epoch [93/100], Step [382/493], Loss: 0.0078\n",
            "Epoch [93/100], Step [383/493], Loss: 0.0050\n",
            "Epoch [93/100], Step [384/493], Loss: 0.0080\n",
            "Epoch [93/100], Step [385/493], Loss: 0.0268\n",
            "Epoch [93/100], Step [386/493], Loss: 0.0253\n",
            "Epoch [93/100], Step [387/493], Loss: 0.0063\n",
            "Epoch [93/100], Step [388/493], Loss: 0.0027\n",
            "Epoch [93/100], Step [389/493], Loss: 0.0054\n",
            "Epoch [93/100], Step [390/493], Loss: 0.0097\n",
            "Epoch [93/100], Step [391/493], Loss: 0.0066\n",
            "Epoch [93/100], Step [392/493], Loss: 0.0015\n",
            "Epoch [93/100], Step [393/493], Loss: 0.0072\n",
            "Epoch [93/100], Step [394/493], Loss: 0.0071\n",
            "Epoch [93/100], Step [395/493], Loss: 0.0308\n",
            "Epoch [93/100], Step [396/493], Loss: 0.0169\n",
            "Epoch [93/100], Step [397/493], Loss: 0.0062\n",
            "Epoch [93/100], Step [398/493], Loss: 0.0097\n",
            "Epoch [93/100], Step [399/493], Loss: 0.1910\n",
            "Epoch [93/100], Step [400/493], Loss: 0.0043\n",
            "Epoch [93/100], Step [401/493], Loss: 0.0057\n",
            "Epoch [93/100], Step [402/493], Loss: 0.0014\n",
            "Epoch [93/100], Step [403/493], Loss: 0.0391\n",
            "Epoch [93/100], Step [404/493], Loss: 0.0114\n",
            "Epoch [93/100], Step [405/493], Loss: 0.0168\n",
            "Epoch [93/100], Step [406/493], Loss: 0.0123\n",
            "Epoch [93/100], Step [407/493], Loss: 0.0075\n",
            "Epoch [93/100], Step [408/493], Loss: 0.0085\n",
            "Epoch [93/100], Step [409/493], Loss: 0.0081\n",
            "Epoch [93/100], Step [410/493], Loss: 0.0079\n",
            "Epoch [93/100], Step [411/493], Loss: 0.0018\n",
            "Epoch [93/100], Step [412/493], Loss: 0.0129\n",
            "Epoch [93/100], Step [413/493], Loss: 0.0175\n",
            "Epoch [93/100], Step [414/493], Loss: 0.0047\n",
            "Epoch [93/100], Step [415/493], Loss: 0.0048\n",
            "Epoch [93/100], Step [416/493], Loss: 0.0330\n",
            "Epoch [93/100], Step [417/493], Loss: 0.0050\n",
            "Epoch [93/100], Step [418/493], Loss: 0.0072\n",
            "Epoch [93/100], Step [419/493], Loss: 0.0838\n",
            "Epoch [93/100], Step [420/493], Loss: 0.0770\n",
            "Epoch [93/100], Step [421/493], Loss: 0.0122\n",
            "Epoch [93/100], Step [422/493], Loss: 0.0066\n",
            "Epoch [93/100], Step [423/493], Loss: 0.0305\n",
            "Epoch [93/100], Step [424/493], Loss: 0.0118\n",
            "Epoch [93/100], Step [425/493], Loss: 0.0100\n",
            "Epoch [93/100], Step [426/493], Loss: 0.2014\n",
            "Epoch [93/100], Step [427/493], Loss: 0.0393\n",
            "Epoch [93/100], Step [428/493], Loss: 0.0005\n",
            "Epoch [93/100], Step [429/493], Loss: 0.0055\n",
            "Epoch [93/100], Step [430/493], Loss: 0.0077\n",
            "Epoch [93/100], Step [431/493], Loss: 0.0144\n",
            "Epoch [93/100], Step [432/493], Loss: 0.0264\n",
            "Epoch [93/100], Step [433/493], Loss: 0.0424\n",
            "Epoch [93/100], Step [434/493], Loss: 0.0131\n",
            "Epoch [93/100], Step [435/493], Loss: 0.0092\n",
            "Epoch [93/100], Step [436/493], Loss: 0.0067\n",
            "Epoch [93/100], Step [437/493], Loss: 0.0140\n",
            "Epoch [93/100], Step [438/493], Loss: 0.0154\n",
            "Epoch [93/100], Step [439/493], Loss: 0.0138\n",
            "Epoch [93/100], Step [440/493], Loss: 0.0074\n",
            "Epoch [93/100], Step [441/493], Loss: 0.0049\n",
            "Epoch [93/100], Step [442/493], Loss: 0.0112\n",
            "Epoch [93/100], Step [443/493], Loss: 0.0055\n",
            "Epoch [93/100], Step [444/493], Loss: 0.0001\n",
            "Epoch [93/100], Step [445/493], Loss: 0.0103\n",
            "Epoch [93/100], Step [446/493], Loss: 0.0100\n",
            "Epoch [93/100], Step [447/493], Loss: 0.0413\n",
            "Epoch [93/100], Step [448/493], Loss: 0.0097\n",
            "Epoch [93/100], Step [449/493], Loss: 0.0034\n",
            "Epoch [93/100], Step [450/493], Loss: 0.0117\n",
            "Epoch [93/100], Step [451/493], Loss: 0.0117\n",
            "Epoch [93/100], Step [452/493], Loss: 0.0094\n",
            "Epoch [93/100], Step [453/493], Loss: 0.0158\n",
            "Epoch [93/100], Step [454/493], Loss: 0.0070\n",
            "Epoch [93/100], Step [455/493], Loss: 0.0070\n",
            "Epoch [93/100], Step [456/493], Loss: 0.0210\n",
            "Epoch [93/100], Step [457/493], Loss: 0.1317\n",
            "Epoch [93/100], Step [458/493], Loss: 0.0071\n",
            "Epoch [93/100], Step [459/493], Loss: 0.0140\n",
            "Epoch [93/100], Step [460/493], Loss: 0.0330\n",
            "Epoch [93/100], Step [461/493], Loss: 0.0060\n",
            "Epoch [93/100], Step [462/493], Loss: 0.0055\n",
            "Epoch [93/100], Step [463/493], Loss: 0.0119\n",
            "Epoch [93/100], Step [464/493], Loss: 0.1024\n",
            "Epoch [93/100], Step [465/493], Loss: 0.0032\n",
            "Epoch [93/100], Step [466/493], Loss: 0.0100\n",
            "Epoch [93/100], Step [467/493], Loss: 0.0172\n",
            "Epoch [93/100], Step [468/493], Loss: 0.0029\n",
            "Epoch [93/100], Step [469/493], Loss: 0.0128\n",
            "Epoch [93/100], Step [470/493], Loss: 0.3149\n",
            "Epoch [93/100], Step [471/493], Loss: 0.0051\n",
            "Epoch [93/100], Step [472/493], Loss: 0.0060\n",
            "Epoch [93/100], Step [473/493], Loss: 0.0068\n",
            "Epoch [93/100], Step [474/493], Loss: 0.0131\n",
            "Epoch [93/100], Step [475/493], Loss: 0.0062\n",
            "Epoch [93/100], Step [476/493], Loss: 0.0070\n",
            "Epoch [93/100], Step [477/493], Loss: 0.0096\n",
            "Epoch [93/100], Step [478/493], Loss: 0.0130\n",
            "Epoch [93/100], Step [479/493], Loss: 0.0078\n",
            "Epoch [93/100], Step [480/493], Loss: 0.0058\n",
            "Epoch [93/100], Step [481/493], Loss: 0.0061\n",
            "Epoch [93/100], Step [482/493], Loss: 0.0021\n",
            "Epoch [93/100], Step [483/493], Loss: 0.0141\n",
            "Epoch [93/100], Step [484/493], Loss: 0.0073\n",
            "Epoch [93/100], Step [485/493], Loss: 0.0255\n",
            "Epoch [93/100], Step [486/493], Loss: 0.0081\n",
            "Epoch [93/100], Step [487/493], Loss: 0.1683\n",
            "Epoch [93/100], Step [488/493], Loss: 0.0068\n",
            "Epoch [93/100], Step [489/493], Loss: 0.0657\n",
            "Epoch [93/100], Step [490/493], Loss: 0.0041\n",
            "Epoch [93/100], Step [491/493], Loss: 0.0057\n",
            "Epoch [93/100], Step [492/493], Loss: 0.0058\n",
            "Epoch [93/100], Step [493/493], Loss: 0.0074\n",
            "Epoch [94/100], Step [1/493], Loss: 0.0033\n",
            "Epoch [94/100], Step [2/493], Loss: 0.0391\n",
            "Epoch [94/100], Step [3/493], Loss: 0.0074\n",
            "Epoch [94/100], Step [4/493], Loss: 0.0046\n",
            "Epoch [94/100], Step [5/493], Loss: 0.1666\n",
            "Epoch [94/100], Step [6/493], Loss: 0.0133\n",
            "Epoch [94/100], Step [7/493], Loss: 0.0136\n",
            "Epoch [94/100], Step [8/493], Loss: 0.0295\n",
            "Epoch [94/100], Step [9/493], Loss: 0.0175\n",
            "Epoch [94/100], Step [10/493], Loss: 0.0019\n",
            "Epoch [94/100], Step [11/493], Loss: 0.0191\n",
            "Epoch [94/100], Step [12/493], Loss: 0.0252\n",
            "Epoch [94/100], Step [13/493], Loss: 0.0264\n",
            "Epoch [94/100], Step [14/493], Loss: 0.0070\n",
            "Epoch [94/100], Step [15/493], Loss: 0.0073\n",
            "Epoch [94/100], Step [16/493], Loss: 0.0068\n",
            "Epoch [94/100], Step [17/493], Loss: 0.0031\n",
            "Epoch [94/100], Step [18/493], Loss: 0.0040\n",
            "Epoch [94/100], Step [19/493], Loss: 0.0033\n",
            "Epoch [94/100], Step [20/493], Loss: 0.0032\n",
            "Epoch [94/100], Step [21/493], Loss: 0.0060\n",
            "Epoch [94/100], Step [22/493], Loss: 0.0025\n",
            "Epoch [94/100], Step [23/493], Loss: 0.0230\n",
            "Epoch [94/100], Step [24/493], Loss: 0.0195\n",
            "Epoch [94/100], Step [25/493], Loss: 0.0001\n",
            "Epoch [94/100], Step [26/493], Loss: 0.0124\n",
            "Epoch [94/100], Step [27/493], Loss: 0.0080\n",
            "Epoch [94/100], Step [28/493], Loss: 0.0054\n",
            "Epoch [94/100], Step [29/493], Loss: 0.0085\n",
            "Epoch [94/100], Step [30/493], Loss: 0.1024\n",
            "Epoch [94/100], Step [31/493], Loss: 0.0018\n",
            "Epoch [94/100], Step [32/493], Loss: 0.0011\n",
            "Epoch [94/100], Step [33/493], Loss: 0.0893\n",
            "Epoch [94/100], Step [34/493], Loss: 0.0833\n",
            "Epoch [94/100], Step [35/493], Loss: 0.0030\n",
            "Epoch [94/100], Step [36/493], Loss: 0.0065\n",
            "Epoch [94/100], Step [37/493], Loss: 0.0097\n",
            "Epoch [94/100], Step [38/493], Loss: 0.0030\n",
            "Epoch [94/100], Step [39/493], Loss: 0.0102\n",
            "Epoch [94/100], Step [40/493], Loss: 0.0360\n",
            "Epoch [94/100], Step [41/493], Loss: 0.0031\n",
            "Epoch [94/100], Step [42/493], Loss: 0.0058\n",
            "Epoch [94/100], Step [43/493], Loss: 0.0074\n",
            "Epoch [94/100], Step [44/493], Loss: 0.0149\n",
            "Epoch [94/100], Step [45/493], Loss: 0.0049\n",
            "Epoch [94/100], Step [46/493], Loss: 0.1317\n",
            "Epoch [94/100], Step [47/493], Loss: 0.0084\n",
            "Epoch [94/100], Step [48/493], Loss: 0.0032\n",
            "Epoch [94/100], Step [49/493], Loss: 0.0092\n",
            "Epoch [94/100], Step [50/493], Loss: 0.0031\n",
            "Epoch [94/100], Step [51/493], Loss: 0.0083\n",
            "Epoch [94/100], Step [52/493], Loss: 0.0081\n",
            "Epoch [94/100], Step [53/493], Loss: 0.0001\n",
            "Epoch [94/100], Step [54/493], Loss: 0.0102\n",
            "Epoch [94/100], Step [55/493], Loss: 0.0268\n",
            "Epoch [94/100], Step [56/493], Loss: 0.0051\n",
            "Epoch [94/100], Step [57/493], Loss: 0.0092\n",
            "Epoch [94/100], Step [58/493], Loss: 0.0099\n",
            "Epoch [94/100], Step [59/493], Loss: 0.0117\n",
            "Epoch [94/100], Step [60/493], Loss: 0.0123\n",
            "Epoch [94/100], Step [61/493], Loss: 0.0339\n",
            "Epoch [94/100], Step [62/493], Loss: 0.0023\n",
            "Epoch [94/100], Step [63/493], Loss: 0.0092\n",
            "Epoch [94/100], Step [64/493], Loss: 0.0097\n",
            "Epoch [94/100], Step [65/493], Loss: 0.0057\n",
            "Epoch [94/100], Step [66/493], Loss: 0.0264\n",
            "Epoch [94/100], Step [67/493], Loss: 0.0424\n",
            "Epoch [94/100], Step [68/493], Loss: 0.0044\n",
            "Epoch [94/100], Step [69/493], Loss: 0.0121\n",
            "Epoch [94/100], Step [70/493], Loss: 0.0166\n",
            "Epoch [94/100], Step [71/493], Loss: 0.0138\n",
            "Epoch [94/100], Step [72/493], Loss: 0.0097\n",
            "Epoch [94/100], Step [73/493], Loss: 0.0002\n",
            "Epoch [94/100], Step [74/493], Loss: 0.0253\n",
            "Epoch [94/100], Step [75/493], Loss: 0.0017\n",
            "Epoch [94/100], Step [76/493], Loss: 0.0109\n",
            "Epoch [94/100], Step [77/493], Loss: 0.0022\n",
            "Epoch [94/100], Step [78/493], Loss: 0.0060\n",
            "Epoch [94/100], Step [79/493], Loss: 0.0085\n",
            "Epoch [94/100], Step [80/493], Loss: 0.0685\n",
            "Epoch [94/100], Step [81/493], Loss: 0.0065\n",
            "Epoch [94/100], Step [82/493], Loss: 0.0109\n",
            "Epoch [94/100], Step [83/493], Loss: 0.0067\n",
            "Epoch [94/100], Step [84/493], Loss: 0.0128\n",
            "Epoch [94/100], Step [85/493], Loss: 0.0088\n",
            "Epoch [94/100], Step [86/493], Loss: 0.0033\n",
            "Epoch [94/100], Step [87/493], Loss: 0.0046\n",
            "Epoch [94/100], Step [88/493], Loss: 0.0099\n",
            "Epoch [94/100], Step [89/493], Loss: 0.0077\n",
            "Epoch [94/100], Step [90/493], Loss: 0.0016\n",
            "Epoch [94/100], Step [91/493], Loss: 0.0145\n",
            "Epoch [94/100], Step [92/493], Loss: 0.0051\n",
            "Epoch [94/100], Step [93/493], Loss: 0.0103\n",
            "Epoch [94/100], Step [94/493], Loss: 0.0037\n",
            "Epoch [94/100], Step [95/493], Loss: 0.0061\n",
            "Epoch [94/100], Step [96/493], Loss: 0.2984\n",
            "Epoch [94/100], Step [97/493], Loss: 0.0068\n",
            "Epoch [94/100], Step [98/493], Loss: 0.0228\n",
            "Epoch [94/100], Step [99/493], Loss: 0.0040\n",
            "Epoch [94/100], Step [100/493], Loss: 0.0109\n",
            "Epoch [94/100], Step [101/493], Loss: 0.0065\n",
            "Epoch [94/100], Step [102/493], Loss: 0.0118\n",
            "Epoch [94/100], Step [103/493], Loss: 0.0131\n",
            "Epoch [94/100], Step [104/493], Loss: 0.0229\n",
            "Epoch [94/100], Step [105/493], Loss: 0.0413\n",
            "Epoch [94/100], Step [106/493], Loss: 0.0029\n",
            "Epoch [94/100], Step [107/493], Loss: 0.0178\n",
            "Epoch [94/100], Step [108/493], Loss: 0.2699\n",
            "Epoch [94/100], Step [109/493], Loss: 0.1971\n",
            "Epoch [94/100], Step [110/493], Loss: 0.0657\n",
            "Epoch [94/100], Step [111/493], Loss: 0.0041\n",
            "Epoch [94/100], Step [112/493], Loss: 0.0083\n",
            "Epoch [94/100], Step [113/493], Loss: 0.0140\n",
            "Epoch [94/100], Step [114/493], Loss: 0.0060\n",
            "Epoch [94/100], Step [115/493], Loss: 0.0034\n",
            "Epoch [94/100], Step [116/493], Loss: 0.0067\n",
            "Epoch [94/100], Step [117/493], Loss: 0.0179\n",
            "Epoch [94/100], Step [118/493], Loss: 0.0253\n",
            "Epoch [94/100], Step [119/493], Loss: 0.0026\n",
            "Epoch [94/100], Step [120/493], Loss: 0.0117\n",
            "Epoch [94/100], Step [121/493], Loss: 0.0095\n",
            "Epoch [94/100], Step [122/493], Loss: 0.0206\n",
            "Epoch [94/100], Step [123/493], Loss: 0.0005\n",
            "Epoch [94/100], Step [124/493], Loss: 0.0108\n",
            "Epoch [94/100], Step [125/493], Loss: 0.0093\n",
            "Epoch [94/100], Step [126/493], Loss: 0.0096\n",
            "Epoch [94/100], Step [127/493], Loss: 0.0179\n",
            "Epoch [94/100], Step [128/493], Loss: 0.0035\n",
            "Epoch [94/100], Step [129/493], Loss: 0.0014\n",
            "Epoch [94/100], Step [130/493], Loss: 0.0051\n",
            "Epoch [94/100], Step [131/493], Loss: 0.1114\n",
            "Epoch [94/100], Step [132/493], Loss: 0.0209\n",
            "Epoch [94/100], Step [133/493], Loss: 0.0043\n",
            "Epoch [94/100], Step [134/493], Loss: 0.0062\n",
            "Epoch [94/100], Step [135/493], Loss: 0.0003\n",
            "Epoch [94/100], Step [136/493], Loss: 0.0069\n",
            "Epoch [94/100], Step [137/493], Loss: 0.0202\n",
            "Epoch [94/100], Step [138/493], Loss: 0.0040\n",
            "Epoch [94/100], Step [139/493], Loss: 0.0061\n",
            "Epoch [94/100], Step [140/493], Loss: 0.0117\n",
            "Epoch [94/100], Step [141/493], Loss: 0.0035\n",
            "Epoch [94/100], Step [142/493], Loss: 0.3149\n",
            "Epoch [94/100], Step [143/493], Loss: 0.0072\n",
            "Epoch [94/100], Step [144/493], Loss: 0.0048\n",
            "Epoch [94/100], Step [145/493], Loss: 0.0168\n",
            "Epoch [94/100], Step [146/493], Loss: 0.0225\n",
            "Epoch [94/100], Step [147/493], Loss: 0.0316\n",
            "Epoch [94/100], Step [148/493], Loss: 0.0382\n",
            "Epoch [94/100], Step [149/493], Loss: 0.0048\n",
            "Epoch [94/100], Step [150/493], Loss: 0.0300\n",
            "Epoch [94/100], Step [151/493], Loss: 0.0132\n",
            "Epoch [94/100], Step [152/493], Loss: 0.0100\n",
            "Epoch [94/100], Step [153/493], Loss: 0.0087\n",
            "Epoch [94/100], Step [154/493], Loss: 0.0083\n",
            "Epoch [94/100], Step [155/493], Loss: 0.0097\n",
            "Epoch [94/100], Step [156/493], Loss: 0.0131\n",
            "Epoch [94/100], Step [157/493], Loss: 0.0014\n",
            "Epoch [94/100], Step [158/493], Loss: 0.0059\n",
            "Epoch [94/100], Step [159/493], Loss: 0.0055\n",
            "Epoch [94/100], Step [160/493], Loss: 0.0073\n",
            "Epoch [94/100], Step [161/493], Loss: 0.0015\n",
            "Epoch [94/100], Step [162/493], Loss: 0.0075\n",
            "Epoch [94/100], Step [163/493], Loss: 0.0147\n",
            "Epoch [94/100], Step [164/493], Loss: 0.0117\n",
            "Epoch [94/100], Step [165/493], Loss: 0.0092\n",
            "Epoch [94/100], Step [166/493], Loss: 0.0340\n",
            "Epoch [94/100], Step [167/493], Loss: 0.0001\n",
            "Epoch [94/100], Step [168/493], Loss: 0.0003\n",
            "Epoch [94/100], Step [169/493], Loss: 0.0033\n",
            "Epoch [94/100], Step [170/493], Loss: 0.0035\n",
            "Epoch [94/100], Step [171/493], Loss: 0.0104\n",
            "Epoch [94/100], Step [172/493], Loss: 0.0141\n",
            "Epoch [94/100], Step [173/493], Loss: 0.0045\n",
            "Epoch [94/100], Step [174/493], Loss: 0.0227\n",
            "Epoch [94/100], Step [175/493], Loss: 0.0147\n",
            "Epoch [94/100], Step [176/493], Loss: 0.0163\n",
            "Epoch [94/100], Step [177/493], Loss: 0.0057\n",
            "Epoch [94/100], Step [178/493], Loss: 0.0131\n",
            "Epoch [94/100], Step [179/493], Loss: 0.0369\n",
            "Epoch [94/100], Step [180/493], Loss: 0.0022\n",
            "Epoch [94/100], Step [181/493], Loss: 0.0066\n",
            "Epoch [94/100], Step [182/493], Loss: 0.0058\n",
            "Epoch [94/100], Step [183/493], Loss: 0.0144\n",
            "Epoch [94/100], Step [184/493], Loss: 0.0023\n",
            "Epoch [94/100], Step [185/493], Loss: 0.0277\n",
            "Epoch [94/100], Step [186/493], Loss: 0.1160\n",
            "Epoch [94/100], Step [187/493], Loss: 0.0131\n",
            "Epoch [94/100], Step [188/493], Loss: 0.0059\n",
            "Epoch [94/100], Step [189/493], Loss: 0.1910\n",
            "Epoch [94/100], Step [190/493], Loss: 0.2014\n",
            "Epoch [94/100], Step [191/493], Loss: 0.0080\n",
            "Epoch [94/100], Step [192/493], Loss: 0.2245\n",
            "Epoch [94/100], Step [193/493], Loss: 0.0194\n",
            "Epoch [94/100], Step [194/493], Loss: 0.0079\n",
            "Epoch [94/100], Step [195/493], Loss: 0.0156\n",
            "Epoch [94/100], Step [196/493], Loss: 0.0067\n",
            "Epoch [94/100], Step [197/493], Loss: 0.0031\n",
            "Epoch [94/100], Step [198/493], Loss: 0.0043\n",
            "Epoch [94/100], Step [199/493], Loss: 0.0027\n",
            "Epoch [94/100], Step [200/493], Loss: 0.0058\n",
            "Epoch [94/100], Step [201/493], Loss: 0.0952\n",
            "Epoch [94/100], Step [202/493], Loss: 0.0116\n",
            "Epoch [94/100], Step [203/493], Loss: 0.1286\n",
            "Epoch [94/100], Step [204/493], Loss: 0.0073\n",
            "Epoch [94/100], Step [205/493], Loss: 0.0305\n",
            "Epoch [94/100], Step [206/493], Loss: 0.0058\n",
            "Epoch [94/100], Step [207/493], Loss: 0.0089\n",
            "Epoch [94/100], Step [208/493], Loss: 0.0084\n",
            "Epoch [94/100], Step [209/493], Loss: 0.1087\n",
            "Epoch [94/100], Step [210/493], Loss: 0.0143\n",
            "Epoch [94/100], Step [211/493], Loss: 0.0037\n",
            "Epoch [94/100], Step [212/493], Loss: 0.0290\n",
            "Epoch [94/100], Step [213/493], Loss: 0.0138\n",
            "Epoch [94/100], Step [214/493], Loss: 0.0029\n",
            "Epoch [94/100], Step [215/493], Loss: 0.0022\n",
            "Epoch [94/100], Step [216/493], Loss: 0.0154\n",
            "Epoch [94/100], Step [217/493], Loss: 0.0141\n",
            "Epoch [94/100], Step [218/493], Loss: 0.0039\n",
            "Epoch [94/100], Step [219/493], Loss: 0.0097\n",
            "Epoch [94/100], Step [220/493], Loss: 0.0070\n",
            "Epoch [94/100], Step [221/493], Loss: 0.0050\n",
            "Epoch [94/100], Step [222/493], Loss: 0.0052\n",
            "Epoch [94/100], Step [223/493], Loss: 0.0023\n",
            "Epoch [94/100], Step [224/493], Loss: 0.0382\n",
            "Epoch [94/100], Step [225/493], Loss: 0.0094\n",
            "Epoch [94/100], Step [226/493], Loss: 0.0027\n",
            "Epoch [94/100], Step [227/493], Loss: 0.0045\n",
            "Epoch [94/100], Step [228/493], Loss: 0.0062\n",
            "Epoch [94/100], Step [229/493], Loss: 0.0042\n",
            "Epoch [94/100], Step [230/493], Loss: 0.0086\n",
            "Epoch [94/100], Step [231/493], Loss: 0.0036\n",
            "Epoch [94/100], Step [232/493], Loss: 0.0330\n",
            "Epoch [94/100], Step [233/493], Loss: 0.0240\n",
            "Epoch [94/100], Step [234/493], Loss: 0.0130\n",
            "Epoch [94/100], Step [235/493], Loss: 0.0128\n",
            "Epoch [94/100], Step [236/493], Loss: 0.0114\n",
            "Epoch [94/100], Step [237/493], Loss: 0.0055\n",
            "Epoch [94/100], Step [238/493], Loss: 0.0169\n",
            "Epoch [94/100], Step [239/493], Loss: 0.0063\n",
            "Epoch [94/100], Step [240/493], Loss: 0.0096\n",
            "Epoch [94/100], Step [241/493], Loss: 0.0074\n",
            "Epoch [94/100], Step [242/493], Loss: 0.1099\n",
            "Epoch [94/100], Step [243/493], Loss: 0.0101\n",
            "Epoch [94/100], Step [244/493], Loss: 0.0289\n",
            "Epoch [94/100], Step [245/493], Loss: 0.0280\n",
            "Epoch [94/100], Step [246/493], Loss: 0.0284\n",
            "Epoch [94/100], Step [247/493], Loss: 0.0045\n",
            "Epoch [94/100], Step [248/493], Loss: 0.0092\n",
            "Epoch [94/100], Step [249/493], Loss: 0.0074\n",
            "Epoch [94/100], Step [250/493], Loss: 0.0174\n",
            "Epoch [94/100], Step [251/493], Loss: 0.0036\n",
            "Epoch [94/100], Step [252/493], Loss: 0.0277\n",
            "Epoch [94/100], Step [253/493], Loss: 0.0093\n",
            "Epoch [94/100], Step [254/493], Loss: 0.0087\n",
            "Epoch [94/100], Step [255/493], Loss: 0.0057\n",
            "Epoch [94/100], Step [256/493], Loss: 0.0059\n",
            "Epoch [94/100], Step [257/493], Loss: 0.0098\n",
            "Epoch [94/100], Step [258/493], Loss: 0.0050\n",
            "Epoch [94/100], Step [259/493], Loss: 0.0103\n",
            "Epoch [94/100], Step [260/493], Loss: 0.0263\n",
            "Epoch [94/100], Step [261/493], Loss: 0.0393\n",
            "Epoch [94/100], Step [262/493], Loss: 0.0005\n",
            "Epoch [94/100], Step [263/493], Loss: 0.0110\n",
            "Epoch [94/100], Step [264/493], Loss: 0.0039\n",
            "Epoch [94/100], Step [265/493], Loss: 0.0081\n",
            "Epoch [94/100], Step [266/493], Loss: 0.0081\n",
            "Epoch [94/100], Step [267/493], Loss: 0.0180\n",
            "Epoch [94/100], Step [268/493], Loss: 0.0029\n",
            "Epoch [94/100], Step [269/493], Loss: 0.0268\n",
            "Epoch [94/100], Step [270/493], Loss: 0.0035\n",
            "Epoch [94/100], Step [271/493], Loss: 0.0043\n",
            "Epoch [94/100], Step [272/493], Loss: 0.0250\n",
            "Epoch [94/100], Step [273/493], Loss: 0.0014\n",
            "Epoch [94/100], Step [274/493], Loss: 0.0094\n",
            "Epoch [94/100], Step [275/493], Loss: 0.0074\n",
            "Epoch [94/100], Step [276/493], Loss: 0.0070\n",
            "Epoch [94/100], Step [277/493], Loss: 0.0078\n",
            "Epoch [94/100], Step [278/493], Loss: 0.0112\n",
            "Epoch [94/100], Step [279/493], Loss: 0.0059\n",
            "Epoch [94/100], Step [280/493], Loss: 0.0074\n",
            "Epoch [94/100], Step [281/493], Loss: 0.0042\n",
            "Epoch [94/100], Step [282/493], Loss: 0.0054\n",
            "Epoch [94/100], Step [283/493], Loss: 0.0121\n",
            "Epoch [94/100], Step [284/493], Loss: 0.0033\n",
            "Epoch [94/100], Step [285/493], Loss: 0.0058\n",
            "Epoch [94/100], Step [286/493], Loss: 0.0032\n",
            "Epoch [94/100], Step [287/493], Loss: 0.0280\n",
            "Epoch [94/100], Step [288/493], Loss: 0.0045\n",
            "Epoch [94/100], Step [289/493], Loss: 0.0123\n",
            "Epoch [94/100], Step [290/493], Loss: 0.0445\n",
            "Epoch [94/100], Step [291/493], Loss: 0.0077\n",
            "Epoch [94/100], Step [292/493], Loss: 0.0308\n",
            "Epoch [94/100], Step [293/493], Loss: 0.0097\n",
            "Epoch [94/100], Step [294/493], Loss: 0.0068\n",
            "Epoch [94/100], Step [295/493], Loss: 0.0259\n",
            "Epoch [94/100], Step [296/493], Loss: 0.0072\n",
            "Epoch [94/100], Step [297/493], Loss: 0.0022\n",
            "Epoch [94/100], Step [298/493], Loss: 0.0244\n",
            "Epoch [94/100], Step [299/493], Loss: 0.0033\n",
            "Epoch [94/100], Step [300/493], Loss: 0.0870\n",
            "Epoch [94/100], Step [301/493], Loss: 0.0037\n",
            "Epoch [94/100], Step [302/493], Loss: 0.0175\n",
            "Epoch [94/100], Step [303/493], Loss: 0.0126\n",
            "Epoch [94/100], Step [304/493], Loss: 0.0071\n",
            "Epoch [94/100], Step [305/493], Loss: 0.0072\n",
            "Epoch [94/100], Step [306/493], Loss: 0.0084\n",
            "Epoch [94/100], Step [307/493], Loss: 0.0872\n",
            "Epoch [94/100], Step [308/493], Loss: 0.0054\n",
            "Epoch [94/100], Step [309/493], Loss: 0.0077\n",
            "Epoch [94/100], Step [310/493], Loss: 0.0208\n",
            "Epoch [94/100], Step [311/493], Loss: 0.0140\n",
            "Epoch [94/100], Step [312/493], Loss: 0.0121\n",
            "Epoch [94/100], Step [313/493], Loss: 0.0200\n",
            "Epoch [94/100], Step [314/493], Loss: 0.0167\n",
            "Epoch [94/100], Step [315/493], Loss: 0.2259\n",
            "Epoch [94/100], Step [316/493], Loss: 0.0305\n",
            "Epoch [94/100], Step [317/493], Loss: 0.0058\n",
            "Epoch [94/100], Step [318/493], Loss: 0.1683\n",
            "Epoch [94/100], Step [319/493], Loss: 0.0020\n",
            "Epoch [94/100], Step [320/493], Loss: 0.0094\n",
            "Epoch [94/100], Step [321/493], Loss: 0.0139\n",
            "Epoch [94/100], Step [322/493], Loss: 0.0072\n",
            "Epoch [94/100], Step [323/493], Loss: 0.0578\n",
            "Epoch [94/100], Step [324/493], Loss: 0.0027\n",
            "Epoch [94/100], Step [325/493], Loss: 0.0098\n",
            "Epoch [94/100], Step [326/493], Loss: 0.0403\n",
            "Epoch [94/100], Step [327/493], Loss: 0.0081\n",
            "Epoch [94/100], Step [328/493], Loss: 0.0041\n",
            "Epoch [94/100], Step [329/493], Loss: 0.0065\n",
            "Epoch [94/100], Step [330/493], Loss: 0.0330\n",
            "Epoch [94/100], Step [331/493], Loss: 0.0172\n",
            "Epoch [94/100], Step [332/493], Loss: 0.0019\n",
            "Epoch [94/100], Step [333/493], Loss: 0.0029\n",
            "Epoch [94/100], Step [334/493], Loss: 0.0146\n",
            "Epoch [94/100], Step [335/493], Loss: 0.0117\n",
            "Epoch [94/100], Step [336/493], Loss: 0.0029\n",
            "Epoch [94/100], Step [337/493], Loss: 0.0066\n",
            "Epoch [94/100], Step [338/493], Loss: 0.0075\n",
            "Epoch [94/100], Step [339/493], Loss: 0.0045\n",
            "Epoch [94/100], Step [340/493], Loss: 0.0412\n",
            "Epoch [94/100], Step [341/493], Loss: 0.0181\n",
            "Epoch [94/100], Step [342/493], Loss: 0.0154\n",
            "Epoch [94/100], Step [343/493], Loss: 0.0132\n",
            "Epoch [94/100], Step [344/493], Loss: 0.0086\n",
            "Epoch [94/100], Step [345/493], Loss: 0.0029\n",
            "Epoch [94/100], Step [346/493], Loss: 0.0105\n",
            "Epoch [94/100], Step [347/493], Loss: 0.0263\n",
            "Epoch [94/100], Step [348/493], Loss: 0.0770\n",
            "Epoch [94/100], Step [349/493], Loss: 0.0065\n",
            "Epoch [94/100], Step [350/493], Loss: 0.0100\n",
            "Epoch [94/100], Step [351/493], Loss: 0.0078\n",
            "Epoch [94/100], Step [352/493], Loss: 0.0042\n",
            "Epoch [94/100], Step [353/493], Loss: 0.1181\n",
            "Epoch [94/100], Step [354/493], Loss: 0.2338\n",
            "Epoch [94/100], Step [355/493], Loss: 0.0258\n",
            "Epoch [94/100], Step [356/493], Loss: 0.0050\n",
            "Epoch [94/100], Step [357/493], Loss: 0.0092\n",
            "Epoch [94/100], Step [358/493], Loss: 0.0070\n",
            "Epoch [94/100], Step [359/493], Loss: 0.5350\n",
            "Epoch [94/100], Step [360/493], Loss: 0.0096\n",
            "Epoch [94/100], Step [361/493], Loss: 0.0059\n",
            "Epoch [94/100], Step [362/493], Loss: 0.0143\n",
            "Epoch [94/100], Step [363/493], Loss: 0.0100\n",
            "Epoch [94/100], Step [364/493], Loss: 0.0129\n",
            "Epoch [94/100], Step [365/493], Loss: 0.0070\n",
            "Epoch [94/100], Step [366/493], Loss: 0.0095\n",
            "Epoch [94/100], Step [367/493], Loss: 0.0054\n",
            "Epoch [94/100], Step [368/493], Loss: 0.0154\n",
            "Epoch [94/100], Step [369/493], Loss: 0.0077\n",
            "Epoch [94/100], Step [370/493], Loss: 0.0055\n",
            "Epoch [94/100], Step [371/493], Loss: 0.0045\n",
            "Epoch [94/100], Step [372/493], Loss: 0.0100\n",
            "Epoch [94/100], Step [373/493], Loss: 0.0032\n",
            "Epoch [94/100], Step [374/493], Loss: 0.0128\n",
            "Epoch [94/100], Step [375/493], Loss: 0.0040\n",
            "Epoch [94/100], Step [376/493], Loss: 0.0021\n",
            "Epoch [94/100], Step [377/493], Loss: 0.0141\n",
            "Epoch [94/100], Step [378/493], Loss: 0.0170\n",
            "Epoch [94/100], Step [379/493], Loss: 0.0121\n",
            "Epoch [94/100], Step [380/493], Loss: 0.0141\n",
            "Epoch [94/100], Step [381/493], Loss: 0.0344\n",
            "Epoch [94/100], Step [382/493], Loss: 0.0046\n",
            "Epoch [94/100], Step [383/493], Loss: 0.0434\n",
            "Epoch [94/100], Step [384/493], Loss: 0.0070\n",
            "Epoch [94/100], Step [385/493], Loss: 0.0079\n",
            "Epoch [94/100], Step [386/493], Loss: 0.0011\n",
            "Epoch [94/100], Step [387/493], Loss: 0.0298\n",
            "Epoch [94/100], Step [388/493], Loss: 0.0838\n",
            "Epoch [94/100], Step [389/493], Loss: 0.0015\n",
            "Epoch [94/100], Step [390/493], Loss: 0.0077\n",
            "Epoch [94/100], Step [391/493], Loss: 0.0062\n",
            "Epoch [94/100], Step [392/493], Loss: 0.0018\n",
            "Epoch [94/100], Step [393/493], Loss: 0.0047\n",
            "Epoch [94/100], Step [394/493], Loss: 0.2135\n",
            "Epoch [94/100], Step [395/493], Loss: 0.0012\n",
            "Epoch [94/100], Step [396/493], Loss: 0.0081\n",
            "Epoch [94/100], Step [397/493], Loss: 0.0095\n",
            "Epoch [94/100], Step [398/493], Loss: 0.0055\n",
            "Epoch [94/100], Step [399/493], Loss: 0.0255\n",
            "Epoch [94/100], Step [400/493], Loss: 0.0056\n",
            "Epoch [94/100], Step [401/493], Loss: 0.0024\n",
            "Epoch [94/100], Step [402/493], Loss: 0.0047\n",
            "Epoch [94/100], Step [403/493], Loss: 0.0044\n",
            "Epoch [94/100], Step [404/493], Loss: 0.0099\n",
            "Epoch [94/100], Step [405/493], Loss: 0.0136\n",
            "Epoch [94/100], Step [406/493], Loss: 0.0064\n",
            "Epoch [94/100], Step [407/493], Loss: 0.0077\n",
            "Epoch [94/100], Step [408/493], Loss: 0.0131\n",
            "Epoch [94/100], Step [409/493], Loss: 0.0057\n",
            "Epoch [94/100], Step [410/493], Loss: 0.0116\n",
            "Epoch [94/100], Step [411/493], Loss: 0.0144\n",
            "Epoch [94/100], Step [412/493], Loss: 0.0043\n",
            "Epoch [94/100], Step [413/493], Loss: 0.0054\n",
            "Epoch [94/100], Step [414/493], Loss: 0.0039\n",
            "Epoch [94/100], Step [415/493], Loss: 0.1575\n",
            "Epoch [94/100], Step [416/493], Loss: 0.0119\n",
            "Epoch [94/100], Step [417/493], Loss: 0.0110\n",
            "Epoch [94/100], Step [418/493], Loss: 0.0010\n",
            "Epoch [94/100], Step [419/493], Loss: 0.0060\n",
            "Epoch [94/100], Step [420/493], Loss: 0.0340\n",
            "Epoch [94/100], Step [421/493], Loss: 0.0039\n",
            "Epoch [94/100], Step [422/493], Loss: 0.0080\n",
            "Epoch [94/100], Step [423/493], Loss: 0.0230\n",
            "Epoch [94/100], Step [424/493], Loss: 0.0122\n",
            "Epoch [94/100], Step [425/493], Loss: 0.0057\n",
            "Epoch [94/100], Step [426/493], Loss: 0.2286\n",
            "Epoch [94/100], Step [427/493], Loss: 0.0059\n",
            "Epoch [94/100], Step [428/493], Loss: 0.0087\n",
            "Epoch [94/100], Step [429/493], Loss: 0.0078\n",
            "Epoch [94/100], Step [430/493], Loss: 0.0080\n",
            "Epoch [94/100], Step [431/493], Loss: 0.0076\n",
            "Epoch [94/100], Step [432/493], Loss: 0.0103\n",
            "Epoch [94/100], Step [433/493], Loss: 0.0075\n",
            "Epoch [94/100], Step [434/493], Loss: 0.0010\n",
            "Epoch [94/100], Step [435/493], Loss: 0.0060\n",
            "Epoch [94/100], Step [436/493], Loss: 0.0023\n",
            "Epoch [94/100], Step [437/493], Loss: 0.0077\n",
            "Epoch [94/100], Step [438/493], Loss: 0.0073\n",
            "Epoch [94/100], Step [439/493], Loss: 0.0011\n",
            "Epoch [94/100], Step [440/493], Loss: 0.0140\n",
            "Epoch [94/100], Step [441/493], Loss: 0.0153\n",
            "Epoch [94/100], Step [442/493], Loss: 0.0047\n",
            "Epoch [94/100], Step [443/493], Loss: 0.0049\n",
            "Epoch [94/100], Step [444/493], Loss: 0.0456\n",
            "Epoch [94/100], Step [445/493], Loss: 0.0097\n",
            "Epoch [94/100], Step [446/493], Loss: 0.0084\n",
            "Epoch [94/100], Step [447/493], Loss: 0.0007\n",
            "Epoch [94/100], Step [448/493], Loss: 0.0096\n",
            "Epoch [94/100], Step [449/493], Loss: 0.0072\n",
            "Epoch [94/100], Step [450/493], Loss: 0.0129\n",
            "Epoch [94/100], Step [451/493], Loss: 0.0079\n",
            "Epoch [94/100], Step [452/493], Loss: 0.0109\n",
            "Epoch [94/100], Step [453/493], Loss: 0.0075\n",
            "Epoch [94/100], Step [454/493], Loss: 0.0161\n",
            "Epoch [94/100], Step [455/493], Loss: 0.1857\n",
            "Epoch [94/100], Step [456/493], Loss: 0.0042\n",
            "Epoch [94/100], Step [457/493], Loss: 0.0069\n",
            "Epoch [94/100], Step [458/493], Loss: 0.0133\n",
            "Epoch [94/100], Step [459/493], Loss: 0.0210\n",
            "Epoch [94/100], Step [460/493], Loss: 0.0018\n",
            "Epoch [94/100], Step [461/493], Loss: 0.4792\n",
            "Epoch [94/100], Step [462/493], Loss: 0.0686\n",
            "Epoch [94/100], Step [463/493], Loss: 0.0018\n",
            "Epoch [94/100], Step [464/493], Loss: 0.0029\n",
            "Epoch [94/100], Step [465/493], Loss: 0.1790\n",
            "Epoch [94/100], Step [466/493], Loss: 0.0112\n",
            "Epoch [94/100], Step [467/493], Loss: 0.0325\n",
            "Epoch [94/100], Step [468/493], Loss: 0.0053\n",
            "Epoch [94/100], Step [469/493], Loss: 0.0066\n",
            "Epoch [94/100], Step [470/493], Loss: 0.0124\n",
            "Epoch [94/100], Step [471/493], Loss: 0.0330\n",
            "Epoch [94/100], Step [472/493], Loss: 0.0078\n",
            "Epoch [94/100], Step [473/493], Loss: 0.0123\n",
            "Epoch [94/100], Step [474/493], Loss: 0.0105\n",
            "Epoch [94/100], Step [475/493], Loss: 0.0158\n",
            "Epoch [94/100], Step [476/493], Loss: 0.0124\n",
            "Epoch [94/100], Step [477/493], Loss: 0.0071\n",
            "Epoch [94/100], Step [478/493], Loss: 0.0081\n",
            "Epoch [94/100], Step [479/493], Loss: 0.0068\n",
            "Epoch [94/100], Step [480/493], Loss: 0.0063\n",
            "Epoch [94/100], Step [481/493], Loss: 0.0016\n",
            "Epoch [94/100], Step [482/493], Loss: 0.0071\n",
            "Epoch [94/100], Step [483/493], Loss: 0.0074\n",
            "Epoch [94/100], Step [484/493], Loss: 0.0067\n",
            "Epoch [94/100], Step [485/493], Loss: 0.0264\n",
            "Epoch [94/100], Step [486/493], Loss: 0.0145\n",
            "Epoch [94/100], Step [487/493], Loss: 0.0014\n",
            "Epoch [94/100], Step [488/493], Loss: 0.0597\n",
            "Epoch [94/100], Step [489/493], Loss: 0.0472\n",
            "Epoch [94/100], Step [490/493], Loss: 0.0086\n",
            "Epoch [94/100], Step [491/493], Loss: 0.0091\n",
            "Epoch [94/100], Step [492/493], Loss: 0.0059\n",
            "Epoch [94/100], Step [493/493], Loss: 0.0253\n",
            "Epoch [95/100], Step [1/493], Loss: 0.0078\n",
            "Epoch [95/100], Step [2/493], Loss: 0.0086\n",
            "Epoch [95/100], Step [3/493], Loss: 0.0072\n",
            "Epoch [95/100], Step [4/493], Loss: 0.0118\n",
            "Epoch [95/100], Step [5/493], Loss: 0.0268\n",
            "Epoch [95/100], Step [6/493], Loss: 0.0018\n",
            "Epoch [95/100], Step [7/493], Loss: 0.0119\n",
            "Epoch [95/100], Step [8/493], Loss: 0.0080\n",
            "Epoch [95/100], Step [9/493], Loss: 0.0048\n",
            "Epoch [95/100], Step [10/493], Loss: 0.0031\n",
            "Epoch [95/100], Step [11/493], Loss: 0.0129\n",
            "Epoch [95/100], Step [12/493], Loss: 0.0284\n",
            "Epoch [95/100], Step [13/493], Loss: 0.0018\n",
            "Epoch [95/100], Step [14/493], Loss: 0.1087\n",
            "Epoch [95/100], Step [15/493], Loss: 0.1181\n",
            "Epoch [95/100], Step [16/493], Loss: 0.0045\n",
            "Epoch [95/100], Step [17/493], Loss: 0.0067\n",
            "Epoch [95/100], Step [18/493], Loss: 0.0092\n",
            "Epoch [95/100], Step [19/493], Loss: 0.0051\n",
            "Epoch [95/100], Step [20/493], Loss: 0.0077\n",
            "Epoch [95/100], Step [21/493], Loss: 0.0872\n",
            "Epoch [95/100], Step [22/493], Loss: 0.0068\n",
            "Epoch [95/100], Step [23/493], Loss: 0.0026\n",
            "Epoch [95/100], Step [24/493], Loss: 0.0076\n",
            "Epoch [95/100], Step [25/493], Loss: 0.0169\n",
            "Epoch [95/100], Step [26/493], Loss: 0.0037\n",
            "Epoch [95/100], Step [27/493], Loss: 0.0051\n",
            "Epoch [95/100], Step [28/493], Loss: 0.0063\n",
            "Epoch [95/100], Step [29/493], Loss: 0.0005\n",
            "Epoch [95/100], Step [30/493], Loss: 0.0059\n",
            "Epoch [95/100], Step [31/493], Loss: 0.0012\n",
            "Epoch [95/100], Step [32/493], Loss: 0.0062\n",
            "Epoch [95/100], Step [33/493], Loss: 0.0264\n",
            "Epoch [95/100], Step [34/493], Loss: 0.0194\n",
            "Epoch [95/100], Step [35/493], Loss: 0.0059\n",
            "Epoch [95/100], Step [36/493], Loss: 0.0209\n",
            "Epoch [95/100], Step [37/493], Loss: 0.0059\n",
            "Epoch [95/100], Step [38/493], Loss: 0.0952\n",
            "Epoch [95/100], Step [39/493], Loss: 0.0065\n",
            "Epoch [95/100], Step [40/493], Loss: 0.0020\n",
            "Epoch [95/100], Step [41/493], Loss: 0.1857\n",
            "Epoch [95/100], Step [42/493], Loss: 0.0057\n",
            "Epoch [95/100], Step [43/493], Loss: 0.0180\n",
            "Epoch [95/100], Step [44/493], Loss: 0.0036\n",
            "Epoch [95/100], Step [45/493], Loss: 0.0100\n",
            "Epoch [95/100], Step [46/493], Loss: 0.0033\n",
            "Epoch [95/100], Step [47/493], Loss: 0.0043\n",
            "Epoch [95/100], Step [48/493], Loss: 0.0064\n",
            "Epoch [95/100], Step [49/493], Loss: 0.0123\n",
            "Epoch [95/100], Step [50/493], Loss: 0.0263\n",
            "Epoch [95/100], Step [51/493], Loss: 0.0097\n",
            "Epoch [95/100], Step [52/493], Loss: 0.0099\n",
            "Epoch [95/100], Step [53/493], Loss: 0.0067\n",
            "Epoch [95/100], Step [54/493], Loss: 0.0154\n",
            "Epoch [95/100], Step [55/493], Loss: 0.0047\n",
            "Epoch [95/100], Step [56/493], Loss: 0.0175\n",
            "Epoch [95/100], Step [57/493], Loss: 0.0083\n",
            "Epoch [95/100], Step [58/493], Loss: 0.0098\n",
            "Epoch [95/100], Step [59/493], Loss: 0.0055\n",
            "Epoch [95/100], Step [60/493], Loss: 0.0022\n",
            "Epoch [95/100], Step [61/493], Loss: 0.0060\n",
            "Epoch [95/100], Step [62/493], Loss: 0.0050\n",
            "Epoch [95/100], Step [63/493], Loss: 0.0253\n",
            "Epoch [95/100], Step [64/493], Loss: 0.0161\n",
            "Epoch [95/100], Step [65/493], Loss: 0.0001\n",
            "Epoch [95/100], Step [66/493], Loss: 0.0087\n",
            "Epoch [95/100], Step [67/493], Loss: 0.0075\n",
            "Epoch [95/100], Step [68/493], Loss: 0.0002\n",
            "Epoch [95/100], Step [69/493], Loss: 0.0136\n",
            "Epoch [95/100], Step [70/493], Loss: 0.0095\n",
            "Epoch [95/100], Step [71/493], Loss: 0.0033\n",
            "Epoch [95/100], Step [72/493], Loss: 0.0019\n",
            "Epoch [95/100], Step [73/493], Loss: 0.0073\n",
            "Epoch [95/100], Step [74/493], Loss: 0.0029\n",
            "Epoch [95/100], Step [75/493], Loss: 0.0141\n",
            "Epoch [95/100], Step [76/493], Loss: 0.0061\n",
            "Epoch [95/100], Step [77/493], Loss: 0.0092\n",
            "Epoch [95/100], Step [78/493], Loss: 0.2699\n",
            "Epoch [95/100], Step [79/493], Loss: 0.0240\n",
            "Epoch [95/100], Step [80/493], Loss: 0.0039\n",
            "Epoch [95/100], Step [81/493], Loss: 0.0685\n",
            "Epoch [95/100], Step [82/493], Loss: 0.0109\n",
            "Epoch [95/100], Step [83/493], Loss: 0.0206\n",
            "Epoch [95/100], Step [84/493], Loss: 0.0172\n",
            "Epoch [95/100], Step [85/493], Loss: 0.0103\n",
            "Epoch [95/100], Step [86/493], Loss: 0.0066\n",
            "Epoch [95/100], Step [87/493], Loss: 0.0080\n",
            "Epoch [95/100], Step [88/493], Loss: 0.0099\n",
            "Epoch [95/100], Step [89/493], Loss: 0.0018\n",
            "Epoch [95/100], Step [90/493], Loss: 0.0099\n",
            "Epoch [95/100], Step [91/493], Loss: 0.0033\n",
            "Epoch [95/100], Step [92/493], Loss: 0.0181\n",
            "Epoch [95/100], Step [93/493], Loss: 0.0040\n",
            "Epoch [95/100], Step [94/493], Loss: 0.1790\n",
            "Epoch [95/100], Step [95/493], Loss: 0.0255\n",
            "Epoch [95/100], Step [96/493], Loss: 0.0230\n",
            "Epoch [95/100], Step [97/493], Loss: 0.0042\n",
            "Epoch [95/100], Step [98/493], Loss: 0.0154\n",
            "Epoch [95/100], Step [99/493], Loss: 0.0062\n",
            "Epoch [95/100], Step [100/493], Loss: 0.0132\n",
            "Epoch [95/100], Step [101/493], Loss: 0.0041\n",
            "Epoch [95/100], Step [102/493], Loss: 0.0094\n",
            "Epoch [95/100], Step [103/493], Loss: 0.0330\n",
            "Epoch [95/100], Step [104/493], Loss: 0.2135\n",
            "Epoch [95/100], Step [105/493], Loss: 0.0016\n",
            "Epoch [95/100], Step [106/493], Loss: 0.0070\n",
            "Epoch [95/100], Step [107/493], Loss: 0.0686\n",
            "Epoch [95/100], Step [108/493], Loss: 0.0063\n",
            "Epoch [95/100], Step [109/493], Loss: 0.0056\n",
            "Epoch [95/100], Step [110/493], Loss: 0.0179\n",
            "Epoch [95/100], Step [111/493], Loss: 0.0033\n",
            "Epoch [95/100], Step [112/493], Loss: 0.0030\n",
            "Epoch [95/100], Step [113/493], Loss: 0.0016\n",
            "Epoch [95/100], Step [114/493], Loss: 0.0112\n",
            "Epoch [95/100], Step [115/493], Loss: 0.0087\n",
            "Epoch [95/100], Step [116/493], Loss: 0.0067\n",
            "Epoch [95/100], Step [117/493], Loss: 0.0022\n",
            "Epoch [95/100], Step [118/493], Loss: 0.0065\n",
            "Epoch [95/100], Step [119/493], Loss: 0.0077\n",
            "Epoch [95/100], Step [120/493], Loss: 0.0081\n",
            "Epoch [95/100], Step [121/493], Loss: 0.0305\n",
            "Epoch [95/100], Step [122/493], Loss: 0.0073\n",
            "Epoch [95/100], Step [123/493], Loss: 0.0110\n",
            "Epoch [95/100], Step [124/493], Loss: 0.0071\n",
            "Epoch [95/100], Step [125/493], Loss: 0.0413\n",
            "Epoch [95/100], Step [126/493], Loss: 0.0143\n",
            "Epoch [95/100], Step [127/493], Loss: 0.0031\n",
            "Epoch [95/100], Step [128/493], Loss: 0.0154\n",
            "Epoch [95/100], Step [129/493], Loss: 0.0084\n",
            "Epoch [95/100], Step [130/493], Loss: 0.0027\n",
            "Epoch [95/100], Step [131/493], Loss: 0.0035\n",
            "Epoch [95/100], Step [132/493], Loss: 0.0097\n",
            "Epoch [95/100], Step [133/493], Loss: 0.0132\n",
            "Epoch [95/100], Step [134/493], Loss: 0.0097\n",
            "Epoch [95/100], Step [135/493], Loss: 0.0139\n",
            "Epoch [95/100], Step [136/493], Loss: 0.0091\n",
            "Epoch [95/100], Step [137/493], Loss: 0.0195\n",
            "Epoch [95/100], Step [138/493], Loss: 0.0597\n",
            "Epoch [95/100], Step [139/493], Loss: 0.0250\n",
            "Epoch [95/100], Step [140/493], Loss: 0.0289\n",
            "Epoch [95/100], Step [141/493], Loss: 0.0128\n",
            "Epoch [95/100], Step [142/493], Loss: 0.0095\n",
            "Epoch [95/100], Step [143/493], Loss: 0.0015\n",
            "Epoch [95/100], Step [144/493], Loss: 0.0210\n",
            "Epoch [95/100], Step [145/493], Loss: 0.0088\n",
            "Epoch [95/100], Step [146/493], Loss: 0.0096\n",
            "Epoch [95/100], Step [147/493], Loss: 0.0298\n",
            "Epoch [95/100], Step [148/493], Loss: 0.0117\n",
            "Epoch [95/100], Step [149/493], Loss: 0.0253\n",
            "Epoch [95/100], Step [150/493], Loss: 0.0870\n",
            "Epoch [95/100], Step [151/493], Loss: 0.0229\n",
            "Epoch [95/100], Step [152/493], Loss: 0.0360\n",
            "Epoch [95/100], Step [153/493], Loss: 0.0116\n",
            "Epoch [95/100], Step [154/493], Loss: 0.0067\n",
            "Epoch [95/100], Step [155/493], Loss: 0.0027\n",
            "Epoch [95/100], Step [156/493], Loss: 0.0022\n",
            "Epoch [95/100], Step [157/493], Loss: 0.0066\n",
            "Epoch [95/100], Step [158/493], Loss: 0.0066\n",
            "Epoch [95/100], Step [159/493], Loss: 0.0022\n",
            "Epoch [95/100], Step [160/493], Loss: 0.0043\n",
            "Epoch [95/100], Step [161/493], Loss: 0.0138\n",
            "Epoch [95/100], Step [162/493], Loss: 0.0032\n",
            "Epoch [95/100], Step [163/493], Loss: 0.0039\n",
            "Epoch [95/100], Step [164/493], Loss: 0.0158\n",
            "Epoch [95/100], Step [165/493], Loss: 0.0015\n",
            "Epoch [95/100], Step [166/493], Loss: 0.0290\n",
            "Epoch [95/100], Step [167/493], Loss: 0.2259\n",
            "Epoch [95/100], Step [168/493], Loss: 0.0280\n",
            "Epoch [95/100], Step [169/493], Loss: 0.0080\n",
            "Epoch [95/100], Step [170/493], Loss: 0.0053\n",
            "Epoch [95/100], Step [171/493], Loss: 0.0059\n",
            "Epoch [95/100], Step [172/493], Loss: 0.0068\n",
            "Epoch [95/100], Step [173/493], Loss: 0.0097\n",
            "Epoch [95/100], Step [174/493], Loss: 0.0047\n",
            "Epoch [95/100], Step [175/493], Loss: 0.0037\n",
            "Epoch [95/100], Step [176/493], Loss: 0.0578\n",
            "Epoch [95/100], Step [177/493], Loss: 0.0019\n",
            "Epoch [95/100], Step [178/493], Loss: 0.0032\n",
            "Epoch [95/100], Step [179/493], Loss: 0.0045\n",
            "Epoch [95/100], Step [180/493], Loss: 0.0054\n",
            "Epoch [95/100], Step [181/493], Loss: 0.0043\n",
            "Epoch [95/100], Step [182/493], Loss: 0.0070\n",
            "Epoch [95/100], Step [183/493], Loss: 0.0039\n",
            "Epoch [95/100], Step [184/493], Loss: 0.0060\n",
            "Epoch [95/100], Step [185/493], Loss: 0.0036\n",
            "Epoch [95/100], Step [186/493], Loss: 0.0065\n",
            "Epoch [95/100], Step [187/493], Loss: 0.0140\n",
            "Epoch [95/100], Step [188/493], Loss: 0.0058\n",
            "Epoch [95/100], Step [189/493], Loss: 0.0057\n",
            "Epoch [95/100], Step [190/493], Loss: 0.0114\n",
            "Epoch [95/100], Step [191/493], Loss: 0.0074\n",
            "Epoch [95/100], Step [192/493], Loss: 0.0109\n",
            "Epoch [95/100], Step [193/493], Loss: 0.1575\n",
            "Epoch [95/100], Step [194/493], Loss: 0.2338\n",
            "Epoch [95/100], Step [195/493], Loss: 0.0055\n",
            "Epoch [95/100], Step [196/493], Loss: 0.0010\n",
            "Epoch [95/100], Step [197/493], Loss: 0.2286\n",
            "Epoch [95/100], Step [198/493], Loss: 0.0098\n",
            "Epoch [95/100], Step [199/493], Loss: 0.0277\n",
            "Epoch [95/100], Step [200/493], Loss: 0.0070\n",
            "Epoch [95/100], Step [201/493], Loss: 0.0069\n",
            "Epoch [95/100], Step [202/493], Loss: 0.0657\n",
            "Epoch [95/100], Step [203/493], Loss: 0.0264\n",
            "Epoch [95/100], Step [204/493], Loss: 0.0039\n",
            "Epoch [95/100], Step [205/493], Loss: 0.0024\n",
            "Epoch [95/100], Step [206/493], Loss: 0.0085\n",
            "Epoch [95/100], Step [207/493], Loss: 0.0049\n",
            "Epoch [95/100], Step [208/493], Loss: 0.0037\n",
            "Epoch [95/100], Step [209/493], Loss: 0.0259\n",
            "Epoch [95/100], Step [210/493], Loss: 0.0102\n",
            "Epoch [95/100], Step [211/493], Loss: 0.0133\n",
            "Epoch [95/100], Step [212/493], Loss: 0.0103\n",
            "Epoch [95/100], Step [213/493], Loss: 0.0029\n",
            "Epoch [95/100], Step [214/493], Loss: 0.0109\n",
            "Epoch [95/100], Step [215/493], Loss: 0.0050\n",
            "Epoch [95/100], Step [216/493], Loss: 0.0308\n",
            "Epoch [95/100], Step [217/493], Loss: 0.0035\n",
            "Epoch [95/100], Step [218/493], Loss: 0.0100\n",
            "Epoch [95/100], Step [219/493], Loss: 0.0208\n",
            "Epoch [95/100], Step [220/493], Loss: 0.0077\n",
            "Epoch [95/100], Step [221/493], Loss: 0.0044\n",
            "Epoch [95/100], Step [222/493], Loss: 0.0001\n",
            "Epoch [95/100], Step [223/493], Loss: 0.0054\n",
            "Epoch [95/100], Step [224/493], Loss: 0.0178\n",
            "Epoch [95/100], Step [225/493], Loss: 0.0893\n",
            "Epoch [95/100], Step [226/493], Loss: 0.0046\n",
            "Epoch [95/100], Step [227/493], Loss: 0.2245\n",
            "Epoch [95/100], Step [228/493], Loss: 0.0071\n",
            "Epoch [95/100], Step [229/493], Loss: 0.0096\n",
            "Epoch [95/100], Step [230/493], Loss: 0.0124\n",
            "Epoch [95/100], Step [231/493], Loss: 0.0081\n",
            "Epoch [95/100], Step [232/493], Loss: 0.1971\n",
            "Epoch [95/100], Step [233/493], Loss: 0.0035\n",
            "Epoch [95/100], Step [234/493], Loss: 0.0071\n",
            "Epoch [95/100], Step [235/493], Loss: 0.0081\n",
            "Epoch [95/100], Step [236/493], Loss: 0.0096\n",
            "Epoch [95/100], Step [237/493], Loss: 0.0116\n",
            "Epoch [95/100], Step [238/493], Loss: 0.0005\n",
            "Epoch [95/100], Step [239/493], Loss: 0.0010\n",
            "Epoch [95/100], Step [240/493], Loss: 0.0031\n",
            "Epoch [95/100], Step [241/493], Loss: 0.0424\n",
            "Epoch [95/100], Step [242/493], Loss: 0.0077\n",
            "Epoch [95/100], Step [243/493], Loss: 0.0070\n",
            "Epoch [95/100], Step [244/493], Loss: 0.0051\n",
            "Epoch [95/100], Step [245/493], Loss: 0.0062\n",
            "Epoch [95/100], Step [246/493], Loss: 0.0403\n",
            "Epoch [95/100], Step [247/493], Loss: 0.0003\n",
            "Epoch [95/100], Step [248/493], Loss: 0.0105\n",
            "Epoch [95/100], Step [249/493], Loss: 0.0163\n",
            "Epoch [95/100], Step [250/493], Loss: 0.0141\n",
            "Epoch [95/100], Step [251/493], Loss: 0.0087\n",
            "Epoch [95/100], Step [252/493], Loss: 0.0057\n",
            "Epoch [95/100], Step [253/493], Loss: 0.0124\n",
            "Epoch [95/100], Step [254/493], Loss: 0.0129\n",
            "Epoch [95/100], Step [255/493], Loss: 0.0124\n",
            "Epoch [95/100], Step [256/493], Loss: 0.0149\n",
            "Epoch [95/100], Step [257/493], Loss: 0.0033\n",
            "Epoch [95/100], Step [258/493], Loss: 0.0081\n",
            "Epoch [95/100], Step [259/493], Loss: 0.0130\n",
            "Epoch [95/100], Step [260/493], Loss: 0.0101\n",
            "Epoch [95/100], Step [261/493], Loss: 0.0244\n",
            "Epoch [95/100], Step [262/493], Loss: 0.0093\n",
            "Epoch [95/100], Step [263/493], Loss: 0.0075\n",
            "Epoch [95/100], Step [264/493], Loss: 0.0058\n",
            "Epoch [95/100], Step [265/493], Loss: 0.2984\n",
            "Epoch [95/100], Step [266/493], Loss: 0.0227\n",
            "Epoch [95/100], Step [267/493], Loss: 0.0011\n",
            "Epoch [95/100], Step [268/493], Loss: 0.0191\n",
            "Epoch [95/100], Step [269/493], Loss: 0.0055\n",
            "Epoch [95/100], Step [270/493], Loss: 0.1099\n",
            "Epoch [95/100], Step [271/493], Loss: 0.0144\n",
            "Epoch [95/100], Step [272/493], Loss: 0.0103\n",
            "Epoch [95/100], Step [273/493], Loss: 0.0445\n",
            "Epoch [95/100], Step [274/493], Loss: 0.0058\n",
            "Epoch [95/100], Step [275/493], Loss: 0.0330\n",
            "Epoch [95/100], Step [276/493], Loss: 0.0145\n",
            "Epoch [95/100], Step [277/493], Loss: 0.0104\n",
            "Epoch [95/100], Step [278/493], Loss: 0.0080\n",
            "Epoch [95/100], Step [279/493], Loss: 0.0110\n",
            "Epoch [95/100], Step [280/493], Loss: 0.0040\n",
            "Epoch [95/100], Step [281/493], Loss: 0.0325\n",
            "Epoch [95/100], Step [282/493], Loss: 0.0102\n",
            "Epoch [95/100], Step [283/493], Loss: 0.0045\n",
            "Epoch [95/100], Step [284/493], Loss: 0.0295\n",
            "Epoch [95/100], Step [285/493], Loss: 0.0072\n",
            "Epoch [95/100], Step [286/493], Loss: 0.0170\n",
            "Epoch [95/100], Step [287/493], Loss: 0.0305\n",
            "Epoch [95/100], Step [288/493], Loss: 0.0029\n",
            "Epoch [95/100], Step [289/493], Loss: 0.0100\n",
            "Epoch [95/100], Step [290/493], Loss: 0.0344\n",
            "Epoch [95/100], Step [291/493], Loss: 0.0046\n",
            "Epoch [95/100], Step [292/493], Loss: 0.0060\n",
            "Epoch [95/100], Step [293/493], Loss: 0.1666\n",
            "Epoch [95/100], Step [294/493], Loss: 0.0068\n",
            "Epoch [95/100], Step [295/493], Loss: 0.0277\n",
            "Epoch [95/100], Step [296/493], Loss: 0.0075\n",
            "Epoch [95/100], Step [297/493], Loss: 0.0074\n",
            "Epoch [95/100], Step [298/493], Loss: 0.0023\n",
            "Epoch [95/100], Step [299/493], Loss: 0.0041\n",
            "Epoch [95/100], Step [300/493], Loss: 0.0141\n",
            "Epoch [95/100], Step [301/493], Loss: 0.0057\n",
            "Epoch [95/100], Step [302/493], Loss: 0.0153\n",
            "Epoch [95/100], Step [303/493], Loss: 0.0084\n",
            "Epoch [95/100], Step [304/493], Loss: 0.1910\n",
            "Epoch [95/100], Step [305/493], Loss: 0.0045\n",
            "Epoch [95/100], Step [306/493], Loss: 0.0128\n",
            "Epoch [95/100], Step [307/493], Loss: 0.0121\n",
            "Epoch [95/100], Step [308/493], Loss: 0.0040\n",
            "Epoch [95/100], Step [309/493], Loss: 0.0058\n",
            "Epoch [95/100], Step [310/493], Loss: 0.0391\n",
            "Epoch [95/100], Step [311/493], Loss: 0.0027\n",
            "Epoch [95/100], Step [312/493], Loss: 0.0074\n",
            "Epoch [95/100], Step [313/493], Loss: 0.0167\n",
            "Epoch [95/100], Step [314/493], Loss: 0.0096\n",
            "Epoch [95/100], Step [315/493], Loss: 0.0393\n",
            "Epoch [95/100], Step [316/493], Loss: 0.0095\n",
            "Epoch [95/100], Step [317/493], Loss: 0.0023\n",
            "Epoch [95/100], Step [318/493], Loss: 0.0316\n",
            "Epoch [95/100], Step [319/493], Loss: 0.0330\n",
            "Epoch [95/100], Step [320/493], Loss: 0.0128\n",
            "Epoch [95/100], Step [321/493], Loss: 0.0339\n",
            "Epoch [95/100], Step [322/493], Loss: 0.0034\n",
            "Epoch [95/100], Step [323/493], Loss: 0.0300\n",
            "Epoch [95/100], Step [324/493], Loss: 0.0100\n",
            "Epoch [95/100], Step [325/493], Loss: 0.0117\n",
            "Epoch [95/100], Step [326/493], Loss: 0.0258\n",
            "Epoch [95/100], Step [327/493], Loss: 0.0061\n",
            "Epoch [95/100], Step [328/493], Loss: 0.0068\n",
            "Epoch [95/100], Step [329/493], Loss: 0.0079\n",
            "Epoch [95/100], Step [330/493], Loss: 0.1114\n",
            "Epoch [95/100], Step [331/493], Loss: 0.0382\n",
            "Epoch [95/100], Step [332/493], Loss: 0.0079\n",
            "Epoch [95/100], Step [333/493], Loss: 0.4792\n",
            "Epoch [95/100], Step [334/493], Loss: 0.0179\n",
            "Epoch [95/100], Step [335/493], Loss: 0.0054\n",
            "Epoch [95/100], Step [336/493], Loss: 0.0126\n",
            "Epoch [95/100], Step [337/493], Loss: 0.0057\n",
            "Epoch [95/100], Step [338/493], Loss: 0.0200\n",
            "Epoch [95/100], Step [339/493], Loss: 0.0140\n",
            "Epoch [95/100], Step [340/493], Loss: 0.0042\n",
            "Epoch [95/100], Step [341/493], Loss: 0.0048\n",
            "Epoch [95/100], Step [342/493], Loss: 0.0434\n",
            "Epoch [95/100], Step [343/493], Loss: 0.0123\n",
            "Epoch [95/100], Step [344/493], Loss: 0.0075\n",
            "Epoch [95/100], Step [345/493], Loss: 0.0033\n",
            "Epoch [95/100], Step [346/493], Loss: 0.0060\n",
            "Epoch [95/100], Step [347/493], Loss: 0.0094\n",
            "Epoch [95/100], Step [348/493], Loss: 0.0023\n",
            "Epoch [95/100], Step [349/493], Loss: 0.0202\n",
            "Epoch [95/100], Step [350/493], Loss: 0.0412\n",
            "Epoch [95/100], Step [351/493], Loss: 0.0025\n",
            "Epoch [95/100], Step [352/493], Loss: 0.0252\n",
            "Epoch [95/100], Step [353/493], Loss: 0.0007\n",
            "Epoch [95/100], Step [354/493], Loss: 0.0175\n",
            "Epoch [95/100], Step [355/493], Loss: 0.0131\n",
            "Epoch [95/100], Step [356/493], Loss: 0.0030\n",
            "Epoch [95/100], Step [357/493], Loss: 0.1683\n",
            "Epoch [95/100], Step [358/493], Loss: 0.0011\n",
            "Epoch [95/100], Step [359/493], Loss: 0.0077\n",
            "Epoch [95/100], Step [360/493], Loss: 0.0156\n",
            "Epoch [95/100], Step [361/493], Loss: 0.0078\n",
            "Epoch [95/100], Step [362/493], Loss: 0.0121\n",
            "Epoch [95/100], Step [363/493], Loss: 0.0001\n",
            "Epoch [95/100], Step [364/493], Loss: 0.0032\n",
            "Epoch [95/100], Step [365/493], Loss: 0.0097\n",
            "Epoch [95/100], Step [366/493], Loss: 0.0052\n",
            "Epoch [95/100], Step [367/493], Loss: 0.0014\n",
            "Epoch [95/100], Step [368/493], Loss: 0.0083\n",
            "Epoch [95/100], Step [369/493], Loss: 0.0094\n",
            "Epoch [95/100], Step [370/493], Loss: 0.0086\n",
            "Epoch [95/100], Step [371/493], Loss: 0.0047\n",
            "Epoch [95/100], Step [372/493], Loss: 0.0029\n",
            "Epoch [95/100], Step [373/493], Loss: 0.0035\n",
            "Epoch [95/100], Step [374/493], Loss: 0.0014\n",
            "Epoch [95/100], Step [375/493], Loss: 0.0077\n",
            "Epoch [95/100], Step [376/493], Loss: 0.0040\n",
            "Epoch [95/100], Step [377/493], Loss: 0.0121\n",
            "Epoch [95/100], Step [378/493], Loss: 0.0029\n",
            "Epoch [95/100], Step [379/493], Loss: 0.0097\n",
            "Epoch [95/100], Step [380/493], Loss: 0.0086\n",
            "Epoch [95/100], Step [381/493], Loss: 0.0092\n",
            "Epoch [95/100], Step [382/493], Loss: 0.0147\n",
            "Epoch [95/100], Step [383/493], Loss: 0.0089\n",
            "Epoch [95/100], Step [384/493], Loss: 0.0017\n",
            "Epoch [95/100], Step [385/493], Loss: 0.0049\n",
            "Epoch [95/100], Step [386/493], Loss: 0.0074\n",
            "Epoch [95/100], Step [387/493], Loss: 0.0382\n",
            "Epoch [95/100], Step [388/493], Loss: 0.0131\n",
            "Epoch [95/100], Step [389/493], Loss: 0.0058\n",
            "Epoch [95/100], Step [390/493], Loss: 0.0046\n",
            "Epoch [95/100], Step [391/493], Loss: 0.0092\n",
            "Epoch [95/100], Step [392/493], Loss: 0.0093\n",
            "Epoch [95/100], Step [393/493], Loss: 0.0340\n",
            "Epoch [95/100], Step [394/493], Loss: 0.0144\n",
            "Epoch [95/100], Step [395/493], Loss: 0.0084\n",
            "Epoch [95/100], Step [396/493], Loss: 0.0131\n",
            "Epoch [95/100], Step [397/493], Loss: 0.0078\n",
            "Epoch [95/100], Step [398/493], Loss: 0.5350\n",
            "Epoch [95/100], Step [399/493], Loss: 0.0253\n",
            "Epoch [95/100], Step [400/493], Loss: 0.0054\n",
            "Epoch [95/100], Step [401/493], Loss: 0.0143\n",
            "Epoch [95/100], Step [402/493], Loss: 0.0003\n",
            "Epoch [95/100], Step [403/493], Loss: 0.0055\n",
            "Epoch [95/100], Step [404/493], Loss: 0.0225\n",
            "Epoch [95/100], Step [405/493], Loss: 0.0014\n",
            "Epoch [95/100], Step [406/493], Loss: 0.0074\n",
            "Epoch [95/100], Step [407/493], Loss: 0.0097\n",
            "Epoch [95/100], Step [408/493], Loss: 0.0079\n",
            "Epoch [95/100], Step [409/493], Loss: 0.0166\n",
            "Epoch [95/100], Step [410/493], Loss: 0.0070\n",
            "Epoch [95/100], Step [411/493], Loss: 0.2014\n",
            "Epoch [95/100], Step [412/493], Loss: 0.0136\n",
            "Epoch [95/100], Step [413/493], Loss: 0.0031\n",
            "Epoch [95/100], Step [414/493], Loss: 0.0029\n",
            "Epoch [95/100], Step [415/493], Loss: 0.3149\n",
            "Epoch [95/100], Step [416/493], Loss: 0.0117\n",
            "Epoch [95/100], Step [417/493], Loss: 0.1160\n",
            "Epoch [95/100], Step [418/493], Loss: 0.0044\n",
            "Epoch [95/100], Step [419/493], Loss: 0.0112\n",
            "Epoch [95/100], Step [420/493], Loss: 0.0131\n",
            "Epoch [95/100], Step [421/493], Loss: 0.0059\n",
            "Epoch [95/100], Step [422/493], Loss: 0.0085\n",
            "Epoch [95/100], Step [423/493], Loss: 0.0043\n",
            "Epoch [95/100], Step [424/493], Loss: 0.1317\n",
            "Epoch [95/100], Step [425/493], Loss: 0.1024\n",
            "Epoch [95/100], Step [426/493], Loss: 0.0228\n",
            "Epoch [95/100], Step [427/493], Loss: 0.0105\n",
            "Epoch [95/100], Step [428/493], Loss: 0.0123\n",
            "Epoch [95/100], Step [429/493], Loss: 0.0117\n",
            "Epoch [95/100], Step [430/493], Loss: 0.0838\n",
            "Epoch [95/100], Step [431/493], Loss: 0.0280\n",
            "Epoch [95/100], Step [432/493], Loss: 0.0168\n",
            "Epoch [95/100], Step [433/493], Loss: 0.0059\n",
            "Epoch [95/100], Step [434/493], Loss: 0.0069\n",
            "Epoch [95/100], Step [435/493], Loss: 0.0074\n",
            "Epoch [95/100], Step [436/493], Loss: 0.0109\n",
            "Epoch [95/100], Step [437/493], Loss: 0.0042\n",
            "Epoch [95/100], Step [438/493], Loss: 0.0077\n",
            "Epoch [95/100], Step [439/493], Loss: 0.0141\n",
            "Epoch [95/100], Step [440/493], Loss: 0.1286\n",
            "Epoch [95/100], Step [441/493], Loss: 0.0072\n",
            "Epoch [95/100], Step [442/493], Loss: 0.0045\n",
            "Epoch [95/100], Step [443/493], Loss: 0.0264\n",
            "Epoch [95/100], Step [444/493], Loss: 0.0054\n",
            "Epoch [95/100], Step [445/493], Loss: 0.0268\n",
            "Epoch [95/100], Step [446/493], Loss: 0.0117\n",
            "Epoch [95/100], Step [447/493], Loss: 0.0050\n",
            "Epoch [95/100], Step [448/493], Loss: 0.0021\n",
            "Epoch [95/100], Step [449/493], Loss: 0.0029\n",
            "Epoch [95/100], Step [450/493], Loss: 0.0340\n",
            "Epoch [95/100], Step [451/493], Loss: 0.0092\n",
            "Epoch [95/100], Step [452/493], Loss: 0.0072\n",
            "Epoch [95/100], Step [453/493], Loss: 0.0074\n",
            "Epoch [95/100], Step [454/493], Loss: 0.0230\n",
            "Epoch [95/100], Step [455/493], Loss: 0.0140\n",
            "Epoch [95/100], Step [456/493], Loss: 0.0108\n",
            "Epoch [95/100], Step [457/493], Loss: 0.0065\n",
            "Epoch [95/100], Step [458/493], Loss: 0.0133\n",
            "Epoch [95/100], Step [459/493], Loss: 0.0263\n",
            "Epoch [95/100], Step [460/493], Loss: 0.0023\n",
            "Epoch [95/100], Step [461/493], Loss: 0.0011\n",
            "Epoch [95/100], Step [462/493], Loss: 0.0070\n",
            "Epoch [95/100], Step [463/493], Loss: 0.0060\n",
            "Epoch [95/100], Step [464/493], Loss: 0.0081\n",
            "Epoch [95/100], Step [465/493], Loss: 0.0092\n",
            "Epoch [95/100], Step [466/493], Loss: 0.0084\n",
            "Epoch [95/100], Step [467/493], Loss: 0.0072\n",
            "Epoch [95/100], Step [468/493], Loss: 0.0770\n",
            "Epoch [95/100], Step [469/493], Loss: 0.0472\n",
            "Epoch [95/100], Step [470/493], Loss: 0.0042\n",
            "Epoch [95/100], Step [471/493], Loss: 0.0122\n",
            "Epoch [95/100], Step [472/493], Loss: 0.0014\n",
            "Epoch [95/100], Step [473/493], Loss: 0.0045\n",
            "Epoch [95/100], Step [474/493], Loss: 0.0018\n",
            "Epoch [95/100], Step [475/493], Loss: 0.0131\n",
            "Epoch [95/100], Step [476/493], Loss: 0.0083\n",
            "Epoch [95/100], Step [477/493], Loss: 0.0032\n",
            "Epoch [95/100], Step [478/493], Loss: 0.0146\n",
            "Epoch [95/100], Step [479/493], Loss: 0.0456\n",
            "Epoch [95/100], Step [480/493], Loss: 0.0833\n",
            "Epoch [95/100], Step [481/493], Loss: 0.0065\n",
            "Epoch [95/100], Step [482/493], Loss: 0.0058\n",
            "Epoch [95/100], Step [483/493], Loss: 0.0081\n",
            "Epoch [95/100], Step [484/493], Loss: 0.0145\n",
            "Epoch [95/100], Step [485/493], Loss: 0.0059\n",
            "Epoch [95/100], Step [486/493], Loss: 0.0174\n",
            "Epoch [95/100], Step [487/493], Loss: 0.0369\n",
            "Epoch [95/100], Step [488/493], Loss: 0.0138\n",
            "Epoch [95/100], Step [489/493], Loss: 0.0147\n",
            "Epoch [95/100], Step [490/493], Loss: 0.0078\n",
            "Epoch [95/100], Step [491/493], Loss: 0.0073\n",
            "Epoch [95/100], Step [492/493], Loss: 0.0073\n",
            "Epoch [95/100], Step [493/493], Loss: 0.0121\n",
            "Epoch [96/100], Step [1/493], Loss: 0.0076\n",
            "Epoch [96/100], Step [2/493], Loss: 0.0119\n",
            "Epoch [96/100], Step [3/493], Loss: 0.0074\n",
            "Epoch [96/100], Step [4/493], Loss: 0.0030\n",
            "Epoch [96/100], Step [5/493], Loss: 0.0097\n",
            "Epoch [96/100], Step [6/493], Loss: 0.0092\n",
            "Epoch [96/100], Step [7/493], Loss: 0.2984\n",
            "Epoch [96/100], Step [8/493], Loss: 0.0051\n",
            "Epoch [96/100], Step [9/493], Loss: 0.0059\n",
            "Epoch [96/100], Step [10/493], Loss: 0.0017\n",
            "Epoch [96/100], Step [11/493], Loss: 0.0058\n",
            "Epoch [96/100], Step [12/493], Loss: 0.0132\n",
            "Epoch [96/100], Step [13/493], Loss: 0.0434\n",
            "Epoch [96/100], Step [14/493], Loss: 0.0091\n",
            "Epoch [96/100], Step [15/493], Loss: 0.0059\n",
            "Epoch [96/100], Step [16/493], Loss: 0.0141\n",
            "Epoch [96/100], Step [17/493], Loss: 0.0268\n",
            "Epoch [96/100], Step [18/493], Loss: 0.0081\n",
            "Epoch [96/100], Step [19/493], Loss: 0.0051\n",
            "Epoch [96/100], Step [20/493], Loss: 0.0097\n",
            "Epoch [96/100], Step [21/493], Loss: 0.0284\n",
            "Epoch [96/100], Step [22/493], Loss: 0.0072\n",
            "Epoch [96/100], Step [23/493], Loss: 0.0382\n",
            "Epoch [96/100], Step [24/493], Loss: 0.0059\n",
            "Epoch [96/100], Step [25/493], Loss: 0.0065\n",
            "Epoch [96/100], Step [26/493], Loss: 0.0080\n",
            "Epoch [96/100], Step [27/493], Loss: 0.0054\n",
            "Epoch [96/100], Step [28/493], Loss: 0.0195\n",
            "Epoch [96/100], Step [29/493], Loss: 0.0054\n",
            "Epoch [96/100], Step [30/493], Loss: 0.0046\n",
            "Epoch [96/100], Step [31/493], Loss: 0.0088\n",
            "Epoch [96/100], Step [32/493], Loss: 0.0037\n",
            "Epoch [96/100], Step [33/493], Loss: 0.0078\n",
            "Epoch [96/100], Step [34/493], Loss: 0.0073\n",
            "Epoch [96/100], Step [35/493], Loss: 0.0131\n",
            "Epoch [96/100], Step [36/493], Loss: 0.0253\n",
            "Epoch [96/100], Step [37/493], Loss: 0.0033\n",
            "Epoch [96/100], Step [38/493], Loss: 0.0056\n",
            "Epoch [96/100], Step [39/493], Loss: 0.0074\n",
            "Epoch [96/100], Step [40/493], Loss: 0.0094\n",
            "Epoch [96/100], Step [41/493], Loss: 0.0277\n",
            "Epoch [96/100], Step [42/493], Loss: 0.0143\n",
            "Epoch [96/100], Step [43/493], Loss: 0.0080\n",
            "Epoch [96/100], Step [44/493], Loss: 0.0062\n",
            "Epoch [96/100], Step [45/493], Loss: 0.0040\n",
            "Epoch [96/100], Step [46/493], Loss: 0.0003\n",
            "Epoch [96/100], Step [47/493], Loss: 0.0144\n",
            "Epoch [96/100], Step [48/493], Loss: 0.0105\n",
            "Epoch [96/100], Step [49/493], Loss: 0.0133\n",
            "Epoch [96/100], Step [50/493], Loss: 0.0031\n",
            "Epoch [96/100], Step [51/493], Loss: 0.0069\n",
            "Epoch [96/100], Step [52/493], Loss: 0.0068\n",
            "Epoch [96/100], Step [53/493], Loss: 0.0244\n",
            "Epoch [96/100], Step [54/493], Loss: 0.2014\n",
            "Epoch [96/100], Step [55/493], Loss: 0.0073\n",
            "Epoch [96/100], Step [56/493], Loss: 0.0117\n",
            "Epoch [96/100], Step [57/493], Loss: 0.0048\n",
            "Epoch [96/100], Step [58/493], Loss: 0.0138\n",
            "Epoch [96/100], Step [59/493], Loss: 0.0229\n",
            "Epoch [96/100], Step [60/493], Loss: 0.0005\n",
            "Epoch [96/100], Step [61/493], Loss: 0.0059\n",
            "Epoch [96/100], Step [62/493], Loss: 0.0075\n",
            "Epoch [96/100], Step [63/493], Loss: 0.0117\n",
            "Epoch [96/100], Step [64/493], Loss: 0.0870\n",
            "Epoch [96/100], Step [65/493], Loss: 0.0047\n",
            "Epoch [96/100], Step [66/493], Loss: 0.0305\n",
            "Epoch [96/100], Step [67/493], Loss: 0.0129\n",
            "Epoch [96/100], Step [68/493], Loss: 0.0068\n",
            "Epoch [96/100], Step [69/493], Loss: 0.0175\n",
            "Epoch [96/100], Step [70/493], Loss: 0.1971\n",
            "Epoch [96/100], Step [71/493], Loss: 0.0029\n",
            "Epoch [96/100], Step [72/493], Loss: 0.0136\n",
            "Epoch [96/100], Step [73/493], Loss: 0.0129\n",
            "Epoch [96/100], Step [74/493], Loss: 0.0060\n",
            "Epoch [96/100], Step [75/493], Loss: 0.0178\n",
            "Epoch [96/100], Step [76/493], Loss: 0.0041\n",
            "Epoch [96/100], Step [77/493], Loss: 0.0071\n",
            "Epoch [96/100], Step [78/493], Loss: 0.0045\n",
            "Epoch [96/100], Step [79/493], Loss: 0.0036\n",
            "Epoch [96/100], Step [80/493], Loss: 0.0081\n",
            "Epoch [96/100], Step [81/493], Loss: 0.0093\n",
            "Epoch [96/100], Step [82/493], Loss: 0.0077\n",
            "Epoch [96/100], Step [83/493], Loss: 0.0018\n",
            "Epoch [96/100], Step [84/493], Loss: 0.0289\n",
            "Epoch [96/100], Step [85/493], Loss: 0.0014\n",
            "Epoch [96/100], Step [86/493], Loss: 0.0032\n",
            "Epoch [96/100], Step [87/493], Loss: 0.0051\n",
            "Epoch [96/100], Step [88/493], Loss: 0.0055\n",
            "Epoch [96/100], Step [89/493], Loss: 0.0330\n",
            "Epoch [96/100], Step [90/493], Loss: 0.0031\n",
            "Epoch [96/100], Step [91/493], Loss: 0.0045\n",
            "Epoch [96/100], Step [92/493], Loss: 0.0049\n",
            "Epoch [96/100], Step [93/493], Loss: 0.0041\n",
            "Epoch [96/100], Step [94/493], Loss: 0.0180\n",
            "Epoch [96/100], Step [95/493], Loss: 0.0066\n",
            "Epoch [96/100], Step [96/493], Loss: 0.0209\n",
            "Epoch [96/100], Step [97/493], Loss: 0.0044\n",
            "Epoch [96/100], Step [98/493], Loss: 0.0097\n",
            "Epoch [96/100], Step [99/493], Loss: 0.0145\n",
            "Epoch [96/100], Step [100/493], Loss: 0.0102\n",
            "Epoch [96/100], Step [101/493], Loss: 0.0042\n",
            "Epoch [96/100], Step [102/493], Loss: 0.0003\n",
            "Epoch [96/100], Step [103/493], Loss: 0.0048\n",
            "Epoch [96/100], Step [104/493], Loss: 0.0098\n",
            "Epoch [96/100], Step [105/493], Loss: 0.0100\n",
            "Epoch [96/100], Step [106/493], Loss: 0.0268\n",
            "Epoch [96/100], Step [107/493], Loss: 0.0110\n",
            "Epoch [96/100], Step [108/493], Loss: 0.0170\n",
            "Epoch [96/100], Step [109/493], Loss: 0.0069\n",
            "Epoch [96/100], Step [110/493], Loss: 0.0445\n",
            "Epoch [96/100], Step [111/493], Loss: 0.0029\n",
            "Epoch [96/100], Step [112/493], Loss: 0.0153\n",
            "Epoch [96/100], Step [113/493], Loss: 0.0316\n",
            "Epoch [96/100], Step [114/493], Loss: 0.0045\n",
            "Epoch [96/100], Step [115/493], Loss: 0.0033\n",
            "Epoch [96/100], Step [116/493], Loss: 0.0043\n",
            "Epoch [96/100], Step [117/493], Loss: 0.0067\n",
            "Epoch [96/100], Step [118/493], Loss: 0.0149\n",
            "Epoch [96/100], Step [119/493], Loss: 0.0096\n",
            "Epoch [96/100], Step [120/493], Loss: 0.0074\n",
            "Epoch [96/100], Step [121/493], Loss: 0.0060\n",
            "Epoch [96/100], Step [122/493], Loss: 0.0102\n",
            "Epoch [96/100], Step [123/493], Loss: 0.0045\n",
            "Epoch [96/100], Step [124/493], Loss: 0.0033\n",
            "Epoch [96/100], Step [125/493], Loss: 0.0087\n",
            "Epoch [96/100], Step [126/493], Loss: 0.0340\n",
            "Epoch [96/100], Step [127/493], Loss: 0.0040\n",
            "Epoch [96/100], Step [128/493], Loss: 0.0128\n",
            "Epoch [96/100], Step [129/493], Loss: 0.0657\n",
            "Epoch [96/100], Step [130/493], Loss: 0.1099\n",
            "Epoch [96/100], Step [131/493], Loss: 0.0057\n",
            "Epoch [96/100], Step [132/493], Loss: 0.0230\n",
            "Epoch [96/100], Step [133/493], Loss: 0.0124\n",
            "Epoch [96/100], Step [134/493], Loss: 0.0163\n",
            "Epoch [96/100], Step [135/493], Loss: 0.0228\n",
            "Epoch [96/100], Step [136/493], Loss: 0.0075\n",
            "Epoch [96/100], Step [137/493], Loss: 0.0097\n",
            "Epoch [96/100], Step [138/493], Loss: 0.0033\n",
            "Epoch [96/100], Step [139/493], Loss: 0.0952\n",
            "Epoch [96/100], Step [140/493], Loss: 0.0047\n",
            "Epoch [96/100], Step [141/493], Loss: 0.1790\n",
            "Epoch [96/100], Step [142/493], Loss: 0.0253\n",
            "Epoch [96/100], Step [143/493], Loss: 0.0043\n",
            "Epoch [96/100], Step [144/493], Loss: 0.0133\n",
            "Epoch [96/100], Step [145/493], Loss: 0.0166\n",
            "Epoch [96/100], Step [146/493], Loss: 0.0089\n",
            "Epoch [96/100], Step [147/493], Loss: 0.0103\n",
            "Epoch [96/100], Step [148/493], Loss: 0.0033\n",
            "Epoch [96/100], Step [149/493], Loss: 0.0206\n",
            "Epoch [96/100], Step [150/493], Loss: 0.0227\n",
            "Epoch [96/100], Step [151/493], Loss: 0.0029\n",
            "Epoch [96/100], Step [152/493], Loss: 0.0043\n",
            "Epoch [96/100], Step [153/493], Loss: 0.0250\n",
            "Epoch [96/100], Step [154/493], Loss: 0.5350\n",
            "Epoch [96/100], Step [155/493], Loss: 0.0121\n",
            "Epoch [96/100], Step [156/493], Loss: 0.0063\n",
            "Epoch [96/100], Step [157/493], Loss: 0.0068\n",
            "Epoch [96/100], Step [158/493], Loss: 0.0031\n",
            "Epoch [96/100], Step [159/493], Loss: 0.0042\n",
            "Epoch [96/100], Step [160/493], Loss: 0.0034\n",
            "Epoch [96/100], Step [161/493], Loss: 0.0078\n",
            "Epoch [96/100], Step [162/493], Loss: 0.0093\n",
            "Epoch [96/100], Step [163/493], Loss: 0.0132\n",
            "Epoch [96/100], Step [164/493], Loss: 0.0169\n",
            "Epoch [96/100], Step [165/493], Loss: 0.0058\n",
            "Epoch [96/100], Step [166/493], Loss: 0.0060\n",
            "Epoch [96/100], Step [167/493], Loss: 0.0092\n",
            "Epoch [96/100], Step [168/493], Loss: 0.0114\n",
            "Epoch [96/100], Step [169/493], Loss: 0.0002\n",
            "Epoch [96/100], Step [170/493], Loss: 0.0147\n",
            "Epoch [96/100], Step [171/493], Loss: 0.2135\n",
            "Epoch [96/100], Step [172/493], Loss: 0.0061\n",
            "Epoch [96/100], Step [173/493], Loss: 0.0079\n",
            "Epoch [96/100], Step [174/493], Loss: 0.0037\n",
            "Epoch [96/100], Step [175/493], Loss: 0.0128\n",
            "Epoch [96/100], Step [176/493], Loss: 0.0117\n",
            "Epoch [96/100], Step [177/493], Loss: 0.0095\n",
            "Epoch [96/100], Step [178/493], Loss: 0.0086\n",
            "Epoch [96/100], Step [179/493], Loss: 0.0014\n",
            "Epoch [96/100], Step [180/493], Loss: 0.0210\n",
            "Epoch [96/100], Step [181/493], Loss: 0.0200\n",
            "Epoch [96/100], Step [182/493], Loss: 0.0035\n",
            "Epoch [96/100], Step [183/493], Loss: 0.0059\n",
            "Epoch [96/100], Step [184/493], Loss: 0.0308\n",
            "Epoch [96/100], Step [185/493], Loss: 0.0099\n",
            "Epoch [96/100], Step [186/493], Loss: 0.2338\n",
            "Epoch [96/100], Step [187/493], Loss: 0.0325\n",
            "Epoch [96/100], Step [188/493], Loss: 0.0770\n",
            "Epoch [96/100], Step [189/493], Loss: 0.0124\n",
            "Epoch [96/100], Step [190/493], Loss: 0.0174\n",
            "Epoch [96/100], Step [191/493], Loss: 0.0263\n",
            "Epoch [96/100], Step [192/493], Loss: 0.0087\n",
            "Epoch [96/100], Step [193/493], Loss: 0.0100\n",
            "Epoch [96/100], Step [194/493], Loss: 0.0140\n",
            "Epoch [96/100], Step [195/493], Loss: 0.0344\n",
            "Epoch [96/100], Step [196/493], Loss: 0.0300\n",
            "Epoch [96/100], Step [197/493], Loss: 0.0050\n",
            "Epoch [96/100], Step [198/493], Loss: 0.0154\n",
            "Epoch [96/100], Step [199/493], Loss: 0.0049\n",
            "Epoch [96/100], Step [200/493], Loss: 0.0145\n",
            "Epoch [96/100], Step [201/493], Loss: 0.0018\n",
            "Epoch [96/100], Step [202/493], Loss: 0.0023\n",
            "Epoch [96/100], Step [203/493], Loss: 0.0074\n",
            "Epoch [96/100], Step [204/493], Loss: 0.0403\n",
            "Epoch [96/100], Step [205/493], Loss: 0.0039\n",
            "Epoch [96/100], Step [206/493], Loss: 0.0230\n",
            "Epoch [96/100], Step [207/493], Loss: 0.0050\n",
            "Epoch [96/100], Step [208/493], Loss: 0.0369\n",
            "Epoch [96/100], Step [209/493], Loss: 0.0103\n",
            "Epoch [96/100], Step [210/493], Loss: 0.0339\n",
            "Epoch [96/100], Step [211/493], Loss: 0.0194\n",
            "Epoch [96/100], Step [212/493], Loss: 0.0109\n",
            "Epoch [96/100], Step [213/493], Loss: 0.0084\n",
            "Epoch [96/100], Step [214/493], Loss: 0.0073\n",
            "Epoch [96/100], Step [215/493], Loss: 0.0027\n",
            "Epoch [96/100], Step [216/493], Loss: 0.0045\n",
            "Epoch [96/100], Step [217/493], Loss: 0.0578\n",
            "Epoch [96/100], Step [218/493], Loss: 0.0112\n",
            "Epoch [96/100], Step [219/493], Loss: 0.0099\n",
            "Epoch [96/100], Step [220/493], Loss: 0.0019\n",
            "Epoch [96/100], Step [221/493], Loss: 0.2699\n",
            "Epoch [96/100], Step [222/493], Loss: 0.0023\n",
            "Epoch [96/100], Step [223/493], Loss: 0.0072\n",
            "Epoch [96/100], Step [224/493], Loss: 0.0144\n",
            "Epoch [96/100], Step [225/493], Loss: 0.0103\n",
            "Epoch [96/100], Step [226/493], Loss: 0.0391\n",
            "Epoch [96/100], Step [227/493], Loss: 0.0011\n",
            "Epoch [96/100], Step [228/493], Loss: 0.0046\n",
            "Epoch [96/100], Step [229/493], Loss: 0.0081\n",
            "Epoch [96/100], Step [230/493], Loss: 0.0037\n",
            "Epoch [96/100], Step [231/493], Loss: 0.0036\n",
            "Epoch [96/100], Step [232/493], Loss: 0.0838\n",
            "Epoch [96/100], Step [233/493], Loss: 0.0078\n",
            "Epoch [96/100], Step [234/493], Loss: 0.0126\n",
            "Epoch [96/100], Step [235/493], Loss: 0.0179\n",
            "Epoch [96/100], Step [236/493], Loss: 0.0077\n",
            "Epoch [96/100], Step [237/493], Loss: 0.0053\n",
            "Epoch [96/100], Step [238/493], Loss: 0.0080\n",
            "Epoch [96/100], Step [239/493], Loss: 0.0083\n",
            "Epoch [96/100], Step [240/493], Loss: 0.0077\n",
            "Epoch [96/100], Step [241/493], Loss: 0.0055\n",
            "Epoch [96/100], Step [242/493], Loss: 0.0109\n",
            "Epoch [96/100], Step [243/493], Loss: 0.0085\n",
            "Epoch [96/100], Step [244/493], Loss: 0.0085\n",
            "Epoch [96/100], Step [245/493], Loss: 0.0360\n",
            "Epoch [96/100], Step [246/493], Loss: 0.0072\n",
            "Epoch [96/100], Step [247/493], Loss: 0.0330\n",
            "Epoch [96/100], Step [248/493], Loss: 0.0057\n",
            "Epoch [96/100], Step [249/493], Loss: 0.0104\n",
            "Epoch [96/100], Step [250/493], Loss: 0.0092\n",
            "Epoch [96/100], Step [251/493], Loss: 0.0064\n",
            "Epoch [96/100], Step [252/493], Loss: 0.0083\n",
            "Epoch [96/100], Step [253/493], Loss: 0.0075\n",
            "Epoch [96/100], Step [254/493], Loss: 0.0001\n",
            "Epoch [96/100], Step [255/493], Loss: 0.2245\n",
            "Epoch [96/100], Step [256/493], Loss: 0.0018\n",
            "Epoch [96/100], Step [257/493], Loss: 0.0015\n",
            "Epoch [96/100], Step [258/493], Loss: 0.0073\n",
            "Epoch [96/100], Step [259/493], Loss: 0.0277\n",
            "Epoch [96/100], Step [260/493], Loss: 0.0072\n",
            "Epoch [96/100], Step [261/493], Loss: 0.0156\n",
            "Epoch [96/100], Step [262/493], Loss: 0.0058\n",
            "Epoch [96/100], Step [263/493], Loss: 0.0084\n",
            "Epoch [96/100], Step [264/493], Loss: 0.0154\n",
            "Epoch [96/100], Step [265/493], Loss: 0.0022\n",
            "Epoch [96/100], Step [266/493], Loss: 0.0139\n",
            "Epoch [96/100], Step [267/493], Loss: 0.0100\n",
            "Epoch [96/100], Step [268/493], Loss: 0.0078\n",
            "Epoch [96/100], Step [269/493], Loss: 0.0040\n",
            "Epoch [96/100], Step [270/493], Loss: 0.0158\n",
            "Epoch [96/100], Step [271/493], Loss: 0.0074\n",
            "Epoch [96/100], Step [272/493], Loss: 0.0055\n",
            "Epoch [96/100], Step [273/493], Loss: 0.0065\n",
            "Epoch [96/100], Step [274/493], Loss: 0.0096\n",
            "Epoch [96/100], Step [275/493], Loss: 0.0058\n",
            "Epoch [96/100], Step [276/493], Loss: 0.0007\n",
            "Epoch [96/100], Step [277/493], Loss: 0.3149\n",
            "Epoch [96/100], Step [278/493], Loss: 0.0128\n",
            "Epoch [96/100], Step [279/493], Loss: 0.0019\n",
            "Epoch [96/100], Step [280/493], Loss: 0.0044\n",
            "Epoch [96/100], Step [281/493], Loss: 0.0052\n",
            "Epoch [96/100], Step [282/493], Loss: 0.0070\n",
            "Epoch [96/100], Step [283/493], Loss: 0.0208\n",
            "Epoch [96/100], Step [284/493], Loss: 0.0096\n",
            "Epoch [96/100], Step [285/493], Loss: 0.0240\n",
            "Epoch [96/100], Step [286/493], Loss: 0.0121\n",
            "Epoch [96/100], Step [287/493], Loss: 0.0086\n",
            "Epoch [96/100], Step [288/493], Loss: 0.0029\n",
            "Epoch [96/100], Step [289/493], Loss: 0.0067\n",
            "Epoch [96/100], Step [290/493], Loss: 0.0255\n",
            "Epoch [96/100], Step [291/493], Loss: 0.0087\n",
            "Epoch [96/100], Step [292/493], Loss: 0.0074\n",
            "Epoch [96/100], Step [293/493], Loss: 0.0259\n",
            "Epoch [96/100], Step [294/493], Loss: 0.1317\n",
            "Epoch [96/100], Step [295/493], Loss: 0.0101\n",
            "Epoch [96/100], Step [296/493], Loss: 0.0116\n",
            "Epoch [96/100], Step [297/493], Loss: 0.0124\n",
            "Epoch [96/100], Step [298/493], Loss: 0.0105\n",
            "Epoch [96/100], Step [299/493], Loss: 0.0298\n",
            "Epoch [96/100], Step [300/493], Loss: 0.0295\n",
            "Epoch [96/100], Step [301/493], Loss: 0.0131\n",
            "Epoch [96/100], Step [302/493], Loss: 0.0023\n",
            "Epoch [96/100], Step [303/493], Loss: 0.0070\n",
            "Epoch [96/100], Step [304/493], Loss: 0.0062\n",
            "Epoch [96/100], Step [305/493], Loss: 0.0100\n",
            "Epoch [96/100], Step [306/493], Loss: 0.0081\n",
            "Epoch [96/100], Step [307/493], Loss: 0.0179\n",
            "Epoch [96/100], Step [308/493], Loss: 0.0118\n",
            "Epoch [96/100], Step [309/493], Loss: 0.0099\n",
            "Epoch [96/100], Step [310/493], Loss: 0.0058\n",
            "Epoch [96/100], Step [311/493], Loss: 0.0168\n",
            "Epoch [96/100], Step [312/493], Loss: 0.0035\n",
            "Epoch [96/100], Step [313/493], Loss: 0.0043\n",
            "Epoch [96/100], Step [314/493], Loss: 0.0097\n",
            "Epoch [96/100], Step [315/493], Loss: 0.1575\n",
            "Epoch [96/100], Step [316/493], Loss: 0.0016\n",
            "Epoch [96/100], Step [317/493], Loss: 0.1160\n",
            "Epoch [96/100], Step [318/493], Loss: 0.0086\n",
            "Epoch [96/100], Step [319/493], Loss: 0.2286\n",
            "Epoch [96/100], Step [320/493], Loss: 0.0054\n",
            "Epoch [96/100], Step [321/493], Loss: 0.0010\n",
            "Epoch [96/100], Step [322/493], Loss: 0.0833\n",
            "Epoch [96/100], Step [323/493], Loss: 0.0070\n",
            "Epoch [96/100], Step [324/493], Loss: 0.0062\n",
            "Epoch [96/100], Step [325/493], Loss: 0.0054\n",
            "Epoch [96/100], Step [326/493], Loss: 0.0140\n",
            "Epoch [96/100], Step [327/493], Loss: 0.0071\n",
            "Epoch [96/100], Step [328/493], Loss: 0.0280\n",
            "Epoch [96/100], Step [329/493], Loss: 0.0117\n",
            "Epoch [96/100], Step [330/493], Loss: 0.1114\n",
            "Epoch [96/100], Step [331/493], Loss: 0.1286\n",
            "Epoch [96/100], Step [332/493], Loss: 0.1181\n",
            "Epoch [96/100], Step [333/493], Loss: 0.1666\n",
            "Epoch [96/100], Step [334/493], Loss: 0.0081\n",
            "Epoch [96/100], Step [335/493], Loss: 0.0001\n",
            "Epoch [96/100], Step [336/493], Loss: 0.0412\n",
            "Epoch [96/100], Step [337/493], Loss: 0.0147\n",
            "Epoch [96/100], Step [338/493], Loss: 0.0116\n",
            "Epoch [96/100], Step [339/493], Loss: 0.0077\n",
            "Epoch [96/100], Step [340/493], Loss: 0.0055\n",
            "Epoch [96/100], Step [341/493], Loss: 0.0023\n",
            "Epoch [96/100], Step [342/493], Loss: 0.0058\n",
            "Epoch [96/100], Step [343/493], Loss: 0.1910\n",
            "Epoch [96/100], Step [344/493], Loss: 0.0079\n",
            "Epoch [96/100], Step [345/493], Loss: 0.0094\n",
            "Epoch [96/100], Step [346/493], Loss: 0.0154\n",
            "Epoch [96/100], Step [347/493], Loss: 0.0080\n",
            "Epoch [96/100], Step [348/493], Loss: 0.0039\n",
            "Epoch [96/100], Step [349/493], Loss: 0.0039\n",
            "Epoch [96/100], Step [350/493], Loss: 0.0054\n",
            "Epoch [96/100], Step [351/493], Loss: 0.0033\n",
            "Epoch [96/100], Step [352/493], Loss: 0.0014\n",
            "Epoch [96/100], Step [353/493], Loss: 0.0012\n",
            "Epoch [96/100], Step [354/493], Loss: 0.0022\n",
            "Epoch [96/100], Step [355/493], Loss: 0.0252\n",
            "Epoch [96/100], Step [356/493], Loss: 0.0057\n",
            "Epoch [96/100], Step [357/493], Loss: 0.0029\n",
            "Epoch [96/100], Step [358/493], Loss: 0.0123\n",
            "Epoch [96/100], Step [359/493], Loss: 0.0109\n",
            "Epoch [96/100], Step [360/493], Loss: 0.0060\n",
            "Epoch [96/100], Step [361/493], Loss: 0.0070\n",
            "Epoch [96/100], Step [362/493], Loss: 0.0095\n",
            "Epoch [96/100], Step [363/493], Loss: 0.0202\n",
            "Epoch [96/100], Step [364/493], Loss: 0.0096\n",
            "Epoch [96/100], Step [365/493], Loss: 0.0191\n",
            "Epoch [96/100], Step [366/493], Loss: 0.0021\n",
            "Epoch [96/100], Step [367/493], Loss: 0.0112\n",
            "Epoch [96/100], Step [368/493], Loss: 0.0032\n",
            "Epoch [96/100], Step [369/493], Loss: 0.0290\n",
            "Epoch [96/100], Step [370/493], Loss: 0.0072\n",
            "Epoch [96/100], Step [371/493], Loss: 0.0075\n",
            "Epoch [96/100], Step [372/493], Loss: 0.0424\n",
            "Epoch [96/100], Step [373/493], Loss: 0.0263\n",
            "Epoch [96/100], Step [374/493], Loss: 0.0020\n",
            "Epoch [96/100], Step [375/493], Loss: 0.0050\n",
            "Epoch [96/100], Step [376/493], Loss: 0.0077\n",
            "Epoch [96/100], Step [377/493], Loss: 0.0046\n",
            "Epoch [96/100], Step [378/493], Loss: 0.0095\n",
            "Epoch [96/100], Step [379/493], Loss: 0.0141\n",
            "Epoch [96/100], Step [380/493], Loss: 0.0122\n",
            "Epoch [96/100], Step [381/493], Loss: 0.0077\n",
            "Epoch [96/100], Step [382/493], Loss: 0.0031\n",
            "Epoch [96/100], Step [383/493], Loss: 0.0138\n",
            "Epoch [96/100], Step [384/493], Loss: 0.0456\n",
            "Epoch [96/100], Step [385/493], Loss: 0.0258\n",
            "Epoch [96/100], Step [386/493], Loss: 0.0070\n",
            "Epoch [96/100], Step [387/493], Loss: 0.0032\n",
            "Epoch [96/100], Step [388/493], Loss: 0.0015\n",
            "Epoch [96/100], Step [389/493], Loss: 0.0108\n",
            "Epoch [96/100], Step [390/493], Loss: 0.0029\n",
            "Epoch [96/100], Step [391/493], Loss: 0.0146\n",
            "Epoch [96/100], Step [392/493], Loss: 0.0059\n",
            "Epoch [96/100], Step [393/493], Loss: 0.0024\n",
            "Epoch [96/100], Step [394/493], Loss: 0.0042\n",
            "Epoch [96/100], Step [395/493], Loss: 0.0109\n",
            "Epoch [96/100], Step [396/493], Loss: 0.0131\n",
            "Epoch [96/100], Step [397/493], Loss: 0.0079\n",
            "Epoch [96/100], Step [398/493], Loss: 0.0305\n",
            "Epoch [96/100], Step [399/493], Loss: 0.1087\n",
            "Epoch [96/100], Step [400/493], Loss: 0.0393\n",
            "Epoch [96/100], Step [401/493], Loss: 0.0040\n",
            "Epoch [96/100], Step [402/493], Loss: 0.0123\n",
            "Epoch [96/100], Step [403/493], Loss: 0.2259\n",
            "Epoch [96/100], Step [404/493], Loss: 0.0022\n",
            "Epoch [96/100], Step [405/493], Loss: 0.0030\n",
            "Epoch [96/100], Step [406/493], Loss: 0.0022\n",
            "Epoch [96/100], Step [407/493], Loss: 0.0264\n",
            "Epoch [96/100], Step [408/493], Loss: 0.0010\n",
            "Epoch [96/100], Step [409/493], Loss: 0.0016\n",
            "Epoch [96/100], Step [410/493], Loss: 0.0280\n",
            "Epoch [96/100], Step [411/493], Loss: 0.0068\n",
            "Epoch [96/100], Step [412/493], Loss: 0.1857\n",
            "Epoch [96/100], Step [413/493], Loss: 0.0264\n",
            "Epoch [96/100], Step [414/493], Loss: 0.0117\n",
            "Epoch [96/100], Step [415/493], Loss: 0.4792\n",
            "Epoch [96/100], Step [416/493], Loss: 0.0097\n",
            "Epoch [96/100], Step [417/493], Loss: 0.0027\n",
            "Epoch [96/100], Step [418/493], Loss: 0.0029\n",
            "Epoch [96/100], Step [419/493], Loss: 0.0066\n",
            "Epoch [96/100], Step [420/493], Loss: 0.0597\n",
            "Epoch [96/100], Step [421/493], Loss: 0.0098\n",
            "Epoch [96/100], Step [422/493], Loss: 0.0066\n",
            "Epoch [96/100], Step [423/493], Loss: 0.0141\n",
            "Epoch [96/100], Step [424/493], Loss: 0.0413\n",
            "Epoch [96/100], Step [425/493], Loss: 0.0110\n",
            "Epoch [96/100], Step [426/493], Loss: 0.0001\n",
            "Epoch [96/100], Step [427/493], Loss: 0.0059\n",
            "Epoch [96/100], Step [428/493], Loss: 0.0026\n",
            "Epoch [96/100], Step [429/493], Loss: 0.1024\n",
            "Epoch [96/100], Step [430/493], Loss: 0.0161\n",
            "Epoch [96/100], Step [431/493], Loss: 0.1683\n",
            "Epoch [96/100], Step [432/493], Loss: 0.0092\n",
            "Epoch [96/100], Step [433/493], Loss: 0.0005\n",
            "Epoch [96/100], Step [434/493], Loss: 0.0167\n",
            "Epoch [96/100], Step [435/493], Loss: 0.0065\n",
            "Epoch [96/100], Step [436/493], Loss: 0.0071\n",
            "Epoch [96/100], Step [437/493], Loss: 0.0330\n",
            "Epoch [96/100], Step [438/493], Loss: 0.0094\n",
            "Epoch [96/100], Step [439/493], Loss: 0.0131\n",
            "Epoch [96/100], Step [440/493], Loss: 0.0067\n",
            "Epoch [96/100], Step [441/493], Loss: 0.0131\n",
            "Epoch [96/100], Step [442/493], Loss: 0.0084\n",
            "Epoch [96/100], Step [443/493], Loss: 0.0074\n",
            "Epoch [96/100], Step [444/493], Loss: 0.0121\n",
            "Epoch [96/100], Step [445/493], Loss: 0.0382\n",
            "Epoch [96/100], Step [446/493], Loss: 0.0035\n",
            "Epoch [96/100], Step [447/493], Loss: 0.0140\n",
            "Epoch [96/100], Step [448/493], Loss: 0.0172\n",
            "Epoch [96/100], Step [449/493], Loss: 0.0067\n",
            "Epoch [96/100], Step [450/493], Loss: 0.0686\n",
            "Epoch [96/100], Step [451/493], Loss: 0.0130\n",
            "Epoch [96/100], Step [452/493], Loss: 0.0084\n",
            "Epoch [96/100], Step [453/493], Loss: 0.0065\n",
            "Epoch [96/100], Step [454/493], Loss: 0.0035\n",
            "Epoch [96/100], Step [455/493], Loss: 0.0045\n",
            "Epoch [96/100], Step [456/493], Loss: 0.0097\n",
            "Epoch [96/100], Step [457/493], Loss: 0.0032\n",
            "Epoch [96/100], Step [458/493], Loss: 0.0264\n",
            "Epoch [96/100], Step [459/493], Loss: 0.0057\n",
            "Epoch [96/100], Step [460/493], Loss: 0.0175\n",
            "Epoch [96/100], Step [461/493], Loss: 0.0872\n",
            "Epoch [96/100], Step [462/493], Loss: 0.0141\n",
            "Epoch [96/100], Step [463/493], Loss: 0.0143\n",
            "Epoch [96/100], Step [464/493], Loss: 0.0057\n",
            "Epoch [96/100], Step [465/493], Loss: 0.0121\n",
            "Epoch [96/100], Step [466/493], Loss: 0.0893\n",
            "Epoch [96/100], Step [467/493], Loss: 0.0011\n",
            "Epoch [96/100], Step [468/493], Loss: 0.0253\n",
            "Epoch [96/100], Step [469/493], Loss: 0.0077\n",
            "Epoch [96/100], Step [470/493], Loss: 0.0092\n",
            "Epoch [96/100], Step [471/493], Loss: 0.0070\n",
            "Epoch [96/100], Step [472/493], Loss: 0.0685\n",
            "Epoch [96/100], Step [473/493], Loss: 0.0136\n",
            "Epoch [96/100], Step [474/493], Loss: 0.0181\n",
            "Epoch [96/100], Step [475/493], Loss: 0.0225\n",
            "Epoch [96/100], Step [476/493], Loss: 0.0340\n",
            "Epoch [96/100], Step [477/493], Loss: 0.0063\n",
            "Epoch [96/100], Step [478/493], Loss: 0.0092\n",
            "Epoch [96/100], Step [479/493], Loss: 0.0061\n",
            "Epoch [96/100], Step [480/493], Loss: 0.0042\n",
            "Epoch [96/100], Step [481/493], Loss: 0.0018\n",
            "Epoch [96/100], Step [482/493], Loss: 0.0083\n",
            "Epoch [96/100], Step [483/493], Loss: 0.0123\n",
            "Epoch [96/100], Step [484/493], Loss: 0.0014\n",
            "Epoch [96/100], Step [485/493], Loss: 0.0081\n",
            "Epoch [96/100], Step [486/493], Loss: 0.0060\n",
            "Epoch [96/100], Step [487/493], Loss: 0.0025\n",
            "Epoch [96/100], Step [488/493], Loss: 0.0047\n",
            "Epoch [96/100], Step [489/493], Loss: 0.0027\n",
            "Epoch [96/100], Step [490/493], Loss: 0.0011\n",
            "Epoch [96/100], Step [491/493], Loss: 0.0039\n",
            "Epoch [96/100], Step [492/493], Loss: 0.0472\n",
            "Epoch [96/100], Step [493/493], Loss: 0.0065\n",
            "Epoch [97/100], Step [1/493], Loss: 0.0144\n",
            "Epoch [97/100], Step [2/493], Loss: 0.0077\n",
            "Epoch [97/100], Step [3/493], Loss: 0.0097\n",
            "Epoch [97/100], Step [4/493], Loss: 0.0047\n",
            "Epoch [97/100], Step [5/493], Loss: 0.0253\n",
            "Epoch [97/100], Step [6/493], Loss: 0.0102\n",
            "Epoch [97/100], Step [7/493], Loss: 0.0225\n",
            "Epoch [97/100], Step [8/493], Loss: 0.0030\n",
            "Epoch [97/100], Step [9/493], Loss: 0.0002\n",
            "Epoch [97/100], Step [10/493], Loss: 0.0298\n",
            "Epoch [97/100], Step [11/493], Loss: 0.0071\n",
            "Epoch [97/100], Step [12/493], Loss: 0.0057\n",
            "Epoch [97/100], Step [13/493], Loss: 0.0024\n",
            "Epoch [97/100], Step [14/493], Loss: 0.0083\n",
            "Epoch [97/100], Step [15/493], Loss: 0.0072\n",
            "Epoch [97/100], Step [16/493], Loss: 0.0194\n",
            "Epoch [97/100], Step [17/493], Loss: 0.0100\n",
            "Epoch [97/100], Step [18/493], Loss: 0.0092\n",
            "Epoch [97/100], Step [19/493], Loss: 0.0147\n",
            "Epoch [97/100], Step [20/493], Loss: 0.0059\n",
            "Epoch [97/100], Step [21/493], Loss: 0.0145\n",
            "Epoch [97/100], Step [22/493], Loss: 0.0016\n",
            "Epoch [97/100], Step [23/493], Loss: 0.0072\n",
            "Epoch [97/100], Step [24/493], Loss: 0.0077\n",
            "Epoch [97/100], Step [25/493], Loss: 0.0100\n",
            "Epoch [97/100], Step [26/493], Loss: 0.0043\n",
            "Epoch [97/100], Step [27/493], Loss: 0.0085\n",
            "Epoch [97/100], Step [28/493], Loss: 0.0068\n",
            "Epoch [97/100], Step [29/493], Loss: 0.0023\n",
            "Epoch [97/100], Step [30/493], Loss: 0.0075\n",
            "Epoch [97/100], Step [31/493], Loss: 0.0015\n",
            "Epoch [97/100], Step [32/493], Loss: 0.0133\n",
            "Epoch [97/100], Step [33/493], Loss: 0.0060\n",
            "Epoch [97/100], Step [34/493], Loss: 0.0141\n",
            "Epoch [97/100], Step [35/493], Loss: 0.0340\n",
            "Epoch [97/100], Step [36/493], Loss: 0.0068\n",
            "Epoch [97/100], Step [37/493], Loss: 0.0069\n",
            "Epoch [97/100], Step [38/493], Loss: 0.0102\n",
            "Epoch [97/100], Step [39/493], Loss: 0.0200\n",
            "Epoch [97/100], Step [40/493], Loss: 0.2286\n",
            "Epoch [97/100], Step [41/493], Loss: 0.0086\n",
            "Epoch [97/100], Step [42/493], Loss: 0.0109\n",
            "Epoch [97/100], Step [43/493], Loss: 0.0086\n",
            "Epoch [97/100], Step [44/493], Loss: 0.0072\n",
            "Epoch [97/100], Step [45/493], Loss: 0.0060\n",
            "Epoch [97/100], Step [46/493], Loss: 0.0110\n",
            "Epoch [97/100], Step [47/493], Loss: 0.1286\n",
            "Epoch [97/100], Step [48/493], Loss: 0.0325\n",
            "Epoch [97/100], Step [49/493], Loss: 0.0119\n",
            "Epoch [97/100], Step [50/493], Loss: 0.0086\n",
            "Epoch [97/100], Step [51/493], Loss: 0.0143\n",
            "Epoch [97/100], Step [52/493], Loss: 0.0117\n",
            "Epoch [97/100], Step [53/493], Loss: 0.0059\n",
            "Epoch [97/100], Step [54/493], Loss: 0.0170\n",
            "Epoch [97/100], Step [55/493], Loss: 0.0657\n",
            "Epoch [97/100], Step [56/493], Loss: 0.0434\n",
            "Epoch [97/100], Step [57/493], Loss: 0.0011\n",
            "Epoch [97/100], Step [58/493], Loss: 0.1160\n",
            "Epoch [97/100], Step [59/493], Loss: 0.0010\n",
            "Epoch [97/100], Step [60/493], Loss: 0.0016\n",
            "Epoch [97/100], Step [61/493], Loss: 0.0263\n",
            "Epoch [97/100], Step [62/493], Loss: 0.0094\n",
            "Epoch [97/100], Step [63/493], Loss: 0.0195\n",
            "Epoch [97/100], Step [64/493], Loss: 0.0033\n",
            "Epoch [97/100], Step [65/493], Loss: 0.0131\n",
            "Epoch [97/100], Step [66/493], Loss: 0.0163\n",
            "Epoch [97/100], Step [67/493], Loss: 0.0153\n",
            "Epoch [97/100], Step [68/493], Loss: 0.0061\n",
            "Epoch [97/100], Step [69/493], Loss: 0.0058\n",
            "Epoch [97/100], Step [70/493], Loss: 0.0045\n",
            "Epoch [97/100], Step [71/493], Loss: 0.0027\n",
            "Epoch [97/100], Step [72/493], Loss: 0.1575\n",
            "Epoch [97/100], Step [73/493], Loss: 0.0077\n",
            "Epoch [97/100], Step [74/493], Loss: 0.0077\n",
            "Epoch [97/100], Step [75/493], Loss: 0.0042\n",
            "Epoch [97/100], Step [76/493], Loss: 0.0052\n",
            "Epoch [97/100], Step [77/493], Loss: 0.0043\n",
            "Epoch [97/100], Step [78/493], Loss: 0.0393\n",
            "Epoch [97/100], Step [79/493], Loss: 0.0096\n",
            "Epoch [97/100], Step [80/493], Loss: 0.0240\n",
            "Epoch [97/100], Step [81/493], Loss: 0.0067\n",
            "Epoch [97/100], Step [82/493], Loss: 0.0268\n",
            "Epoch [97/100], Step [83/493], Loss: 0.0132\n",
            "Epoch [97/100], Step [84/493], Loss: 0.0149\n",
            "Epoch [97/100], Step [85/493], Loss: 0.0048\n",
            "Epoch [97/100], Step [86/493], Loss: 0.0305\n",
            "Epoch [97/100], Step [87/493], Loss: 0.0264\n",
            "Epoch [97/100], Step [88/493], Loss: 0.2245\n",
            "Epoch [97/100], Step [89/493], Loss: 0.0058\n",
            "Epoch [97/100], Step [90/493], Loss: 0.0156\n",
            "Epoch [97/100], Step [91/493], Loss: 0.0055\n",
            "Epoch [97/100], Step [92/493], Loss: 0.0055\n",
            "Epoch [97/100], Step [93/493], Loss: 0.0058\n",
            "Epoch [97/100], Step [94/493], Loss: 0.0158\n",
            "Epoch [97/100], Step [95/493], Loss: 0.0174\n",
            "Epoch [97/100], Step [96/493], Loss: 0.0015\n",
            "Epoch [97/100], Step [97/493], Loss: 0.0037\n",
            "Epoch [97/100], Step [98/493], Loss: 0.0330\n",
            "Epoch [97/100], Step [99/493], Loss: 0.0175\n",
            "Epoch [97/100], Step [100/493], Loss: 0.0078\n",
            "Epoch [97/100], Step [101/493], Loss: 0.0128\n",
            "Epoch [97/100], Step [102/493], Loss: 0.0045\n",
            "Epoch [97/100], Step [103/493], Loss: 0.0095\n",
            "Epoch [97/100], Step [104/493], Loss: 0.0077\n",
            "Epoch [97/100], Step [105/493], Loss: 0.0064\n",
            "Epoch [97/100], Step [106/493], Loss: 0.0054\n",
            "Epoch [97/100], Step [107/493], Loss: 0.0098\n",
            "Epoch [97/100], Step [108/493], Loss: 0.1971\n",
            "Epoch [97/100], Step [109/493], Loss: 0.0046\n",
            "Epoch [97/100], Step [110/493], Loss: 0.0001\n",
            "Epoch [97/100], Step [111/493], Loss: 0.0032\n",
            "Epoch [97/100], Step [112/493], Loss: 0.0029\n",
            "Epoch [97/100], Step [113/493], Loss: 0.0001\n",
            "Epoch [97/100], Step [114/493], Loss: 0.0094\n",
            "Epoch [97/100], Step [115/493], Loss: 0.0472\n",
            "Epoch [97/100], Step [116/493], Loss: 0.0264\n",
            "Epoch [97/100], Step [117/493], Loss: 0.0096\n",
            "Epoch [97/100], Step [118/493], Loss: 0.0018\n",
            "Epoch [97/100], Step [119/493], Loss: 0.0060\n",
            "Epoch [97/100], Step [120/493], Loss: 0.0070\n",
            "Epoch [97/100], Step [121/493], Loss: 0.0066\n",
            "Epoch [97/100], Step [122/493], Loss: 0.0103\n",
            "Epoch [97/100], Step [123/493], Loss: 0.0027\n",
            "Epoch [97/100], Step [124/493], Loss: 0.0305\n",
            "Epoch [97/100], Step [125/493], Loss: 0.0084\n",
            "Epoch [97/100], Step [126/493], Loss: 0.0058\n",
            "Epoch [97/100], Step [127/493], Loss: 0.0100\n",
            "Epoch [97/100], Step [128/493], Loss: 0.0023\n",
            "Epoch [97/100], Step [129/493], Loss: 0.0097\n",
            "Epoch [97/100], Step [130/493], Loss: 0.0103\n",
            "Epoch [97/100], Step [131/493], Loss: 0.0424\n",
            "Epoch [97/100], Step [132/493], Loss: 0.0268\n",
            "Epoch [97/100], Step [133/493], Loss: 0.0096\n",
            "Epoch [97/100], Step [134/493], Loss: 0.0092\n",
            "Epoch [97/100], Step [135/493], Loss: 0.0077\n",
            "Epoch [97/100], Step [136/493], Loss: 0.1087\n",
            "Epoch [97/100], Step [137/493], Loss: 0.0063\n",
            "Epoch [97/100], Step [138/493], Loss: 0.0169\n",
            "Epoch [97/100], Step [139/493], Loss: 0.0139\n",
            "Epoch [97/100], Step [140/493], Loss: 0.0124\n",
            "Epoch [97/100], Step [141/493], Loss: 0.0074\n",
            "Epoch [97/100], Step [142/493], Loss: 0.0056\n",
            "Epoch [97/100], Step [143/493], Loss: 0.0330\n",
            "Epoch [97/100], Step [144/493], Loss: 0.0045\n",
            "Epoch [97/100], Step [145/493], Loss: 0.0057\n",
            "Epoch [97/100], Step [146/493], Loss: 0.0066\n",
            "Epoch [97/100], Step [147/493], Loss: 0.0017\n",
            "Epoch [97/100], Step [148/493], Loss: 0.0308\n",
            "Epoch [97/100], Step [149/493], Loss: 0.0025\n",
            "Epoch [97/100], Step [150/493], Loss: 0.1024\n",
            "Epoch [97/100], Step [151/493], Loss: 0.0893\n",
            "Epoch [97/100], Step [152/493], Loss: 0.0048\n",
            "Epoch [97/100], Step [153/493], Loss: 0.0252\n",
            "Epoch [97/100], Step [154/493], Loss: 0.0168\n",
            "Epoch [97/100], Step [155/493], Loss: 0.0033\n",
            "Epoch [97/100], Step [156/493], Loss: 0.0277\n",
            "Epoch [97/100], Step [157/493], Loss: 0.0206\n",
            "Epoch [97/100], Step [158/493], Loss: 0.0179\n",
            "Epoch [97/100], Step [159/493], Loss: 0.0074\n",
            "Epoch [97/100], Step [160/493], Loss: 0.0178\n",
            "Epoch [97/100], Step [161/493], Loss: 0.0330\n",
            "Epoch [97/100], Step [162/493], Loss: 0.0097\n",
            "Epoch [97/100], Step [163/493], Loss: 0.0018\n",
            "Epoch [97/100], Step [164/493], Loss: 0.0077\n",
            "Epoch [97/100], Step [165/493], Loss: 0.1181\n",
            "Epoch [97/100], Step [166/493], Loss: 0.0071\n",
            "Epoch [97/100], Step [167/493], Loss: 0.0770\n",
            "Epoch [97/100], Step [168/493], Loss: 0.0121\n",
            "Epoch [97/100], Step [169/493], Loss: 0.0076\n",
            "Epoch [97/100], Step [170/493], Loss: 0.0208\n",
            "Epoch [97/100], Step [171/493], Loss: 0.0054\n",
            "Epoch [97/100], Step [172/493], Loss: 0.2014\n",
            "Epoch [97/100], Step [173/493], Loss: 0.0290\n",
            "Epoch [97/100], Step [174/493], Loss: 0.0081\n",
            "Epoch [97/100], Step [175/493], Loss: 0.0087\n",
            "Epoch [97/100], Step [176/493], Loss: 0.0130\n",
            "Epoch [97/100], Step [177/493], Loss: 0.0145\n",
            "Epoch [97/100], Step [178/493], Loss: 0.0058\n",
            "Epoch [97/100], Step [179/493], Loss: 0.1790\n",
            "Epoch [97/100], Step [180/493], Loss: 0.0097\n",
            "Epoch [97/100], Step [181/493], Loss: 0.0263\n",
            "Epoch [97/100], Step [182/493], Loss: 0.0057\n",
            "Epoch [97/100], Step [183/493], Loss: 0.1099\n",
            "Epoch [97/100], Step [184/493], Loss: 0.0065\n",
            "Epoch [97/100], Step [185/493], Loss: 0.0084\n",
            "Epoch [97/100], Step [186/493], Loss: 0.0059\n",
            "Epoch [97/100], Step [187/493], Loss: 0.0092\n",
            "Epoch [97/100], Step [188/493], Loss: 0.1910\n",
            "Epoch [97/100], Step [189/493], Loss: 0.0045\n",
            "Epoch [97/100], Step [190/493], Loss: 0.0042\n",
            "Epoch [97/100], Step [191/493], Loss: 0.0087\n",
            "Epoch [97/100], Step [192/493], Loss: 0.0124\n",
            "Epoch [97/100], Step [193/493], Loss: 0.0129\n",
            "Epoch [97/100], Step [194/493], Loss: 0.0250\n",
            "Epoch [97/100], Step [195/493], Loss: 0.0005\n",
            "Epoch [97/100], Step [196/493], Loss: 0.0075\n",
            "Epoch [97/100], Step [197/493], Loss: 0.0037\n",
            "Epoch [97/100], Step [198/493], Loss: 0.0059\n",
            "Epoch [97/100], Step [199/493], Loss: 0.0033\n",
            "Epoch [97/100], Step [200/493], Loss: 0.0083\n",
            "Epoch [97/100], Step [201/493], Loss: 0.0074\n",
            "Epoch [97/100], Step [202/493], Loss: 0.0079\n",
            "Epoch [97/100], Step [203/493], Loss: 0.0095\n",
            "Epoch [97/100], Step [204/493], Loss: 0.1683\n",
            "Epoch [97/100], Step [205/493], Loss: 0.0058\n",
            "Epoch [97/100], Step [206/493], Loss: 0.0078\n",
            "Epoch [97/100], Step [207/493], Loss: 0.0084\n",
            "Epoch [97/100], Step [208/493], Loss: 0.0101\n",
            "Epoch [97/100], Step [209/493], Loss: 0.0141\n",
            "Epoch [97/100], Step [210/493], Loss: 0.0027\n",
            "Epoch [97/100], Step [211/493], Loss: 0.0300\n",
            "Epoch [97/100], Step [212/493], Loss: 0.2699\n",
            "Epoch [97/100], Step [213/493], Loss: 0.0040\n",
            "Epoch [97/100], Step [214/493], Loss: 0.0100\n",
            "Epoch [97/100], Step [215/493], Loss: 0.0166\n",
            "Epoch [97/100], Step [216/493], Loss: 0.0121\n",
            "Epoch [97/100], Step [217/493], Loss: 0.0080\n",
            "Epoch [97/100], Step [218/493], Loss: 0.0014\n",
            "Epoch [97/100], Step [219/493], Loss: 0.0138\n",
            "Epoch [97/100], Step [220/493], Loss: 0.0070\n",
            "Epoch [97/100], Step [221/493], Loss: 0.0121\n",
            "Epoch [97/100], Step [222/493], Loss: 0.0051\n",
            "Epoch [97/100], Step [223/493], Loss: 0.0032\n",
            "Epoch [97/100], Step [224/493], Loss: 0.0413\n",
            "Epoch [97/100], Step [225/493], Loss: 0.0255\n",
            "Epoch [97/100], Step [226/493], Loss: 0.0044\n",
            "Epoch [97/100], Step [227/493], Loss: 0.0094\n",
            "Epoch [97/100], Step [228/493], Loss: 0.0012\n",
            "Epoch [97/100], Step [229/493], Loss: 0.0144\n",
            "Epoch [97/100], Step [230/493], Loss: 0.0041\n",
            "Epoch [97/100], Step [231/493], Loss: 0.0686\n",
            "Epoch [97/100], Step [232/493], Loss: 0.0098\n",
            "Epoch [97/100], Step [233/493], Loss: 0.0061\n",
            "Epoch [97/100], Step [234/493], Loss: 0.0087\n",
            "Epoch [97/100], Step [235/493], Loss: 0.0117\n",
            "Epoch [97/100], Step [236/493], Loss: 0.0104\n",
            "Epoch [97/100], Step [237/493], Loss: 0.0051\n",
            "Epoch [97/100], Step [238/493], Loss: 0.0070\n",
            "Epoch [97/100], Step [239/493], Loss: 0.0067\n",
            "Epoch [97/100], Step [240/493], Loss: 0.0022\n",
            "Epoch [97/100], Step [241/493], Loss: 0.0033\n",
            "Epoch [97/100], Step [242/493], Loss: 0.0344\n",
            "Epoch [97/100], Step [243/493], Loss: 0.0096\n",
            "Epoch [97/100], Step [244/493], Loss: 0.0039\n",
            "Epoch [97/100], Step [245/493], Loss: 0.0097\n",
            "Epoch [97/100], Step [246/493], Loss: 0.0112\n",
            "Epoch [97/100], Step [247/493], Loss: 0.0109\n",
            "Epoch [97/100], Step [248/493], Loss: 0.0042\n",
            "Epoch [97/100], Step [249/493], Loss: 0.0031\n",
            "Epoch [97/100], Step [250/493], Loss: 0.0253\n",
            "Epoch [97/100], Step [251/493], Loss: 0.0079\n",
            "Epoch [97/100], Step [252/493], Loss: 0.0035\n",
            "Epoch [97/100], Step [253/493], Loss: 0.0259\n",
            "Epoch [97/100], Step [254/493], Loss: 0.0049\n",
            "Epoch [97/100], Step [255/493], Loss: 0.0029\n",
            "Epoch [97/100], Step [256/493], Loss: 0.0014\n",
            "Epoch [97/100], Step [257/493], Loss: 0.0068\n",
            "Epoch [97/100], Step [258/493], Loss: 0.0045\n",
            "Epoch [97/100], Step [259/493], Loss: 0.0340\n",
            "Epoch [97/100], Step [260/493], Loss: 0.0054\n",
            "Epoch [97/100], Step [261/493], Loss: 0.0040\n",
            "Epoch [97/100], Step [262/493], Loss: 0.0050\n",
            "Epoch [97/100], Step [263/493], Loss: 0.0872\n",
            "Epoch [97/100], Step [264/493], Loss: 0.0003\n",
            "Epoch [97/100], Step [265/493], Loss: 0.0069\n",
            "Epoch [97/100], Step [266/493], Loss: 0.0109\n",
            "Epoch [97/100], Step [267/493], Loss: 0.0289\n",
            "Epoch [97/100], Step [268/493], Loss: 0.0123\n",
            "Epoch [97/100], Step [269/493], Loss: 0.0167\n",
            "Epoch [97/100], Step [270/493], Loss: 0.0031\n",
            "Epoch [97/100], Step [271/493], Loss: 0.0099\n",
            "Epoch [97/100], Step [272/493], Loss: 0.0258\n",
            "Epoch [97/100], Step [273/493], Loss: 0.0018\n",
            "Epoch [97/100], Step [274/493], Loss: 0.0037\n",
            "Epoch [97/100], Step [275/493], Loss: 0.0034\n",
            "Epoch [97/100], Step [276/493], Loss: 0.0081\n",
            "Epoch [97/100], Step [277/493], Loss: 0.0456\n",
            "Epoch [97/100], Step [278/493], Loss: 0.0046\n",
            "Epoch [97/100], Step [279/493], Loss: 0.0382\n",
            "Epoch [97/100], Step [280/493], Loss: 0.0068\n",
            "Epoch [97/100], Step [281/493], Loss: 0.0229\n",
            "Epoch [97/100], Step [282/493], Loss: 0.0105\n",
            "Epoch [97/100], Step [283/493], Loss: 0.0003\n",
            "Epoch [97/100], Step [284/493], Loss: 0.0099\n",
            "Epoch [97/100], Step [285/493], Loss: 0.0093\n",
            "Epoch [97/100], Step [286/493], Loss: 0.0179\n",
            "Epoch [97/100], Step [287/493], Loss: 0.2259\n",
            "Epoch [97/100], Step [288/493], Loss: 0.0065\n",
            "Epoch [97/100], Step [289/493], Loss: 0.0018\n",
            "Epoch [97/100], Step [290/493], Loss: 0.0154\n",
            "Epoch [97/100], Step [291/493], Loss: 0.0022\n",
            "Epoch [97/100], Step [292/493], Loss: 0.0136\n",
            "Epoch [97/100], Step [293/493], Loss: 0.0685\n",
            "Epoch [97/100], Step [294/493], Loss: 0.0146\n",
            "Epoch [97/100], Step [295/493], Loss: 0.0092\n",
            "Epoch [97/100], Step [296/493], Loss: 0.0092\n",
            "Epoch [97/100], Step [297/493], Loss: 0.0026\n",
            "Epoch [97/100], Step [298/493], Loss: 0.0089\n",
            "Epoch [97/100], Step [299/493], Loss: 0.0105\n",
            "Epoch [97/100], Step [300/493], Loss: 0.0230\n",
            "Epoch [97/100], Step [301/493], Loss: 0.0050\n",
            "Epoch [97/100], Step [302/493], Loss: 0.0029\n",
            "Epoch [97/100], Step [303/493], Loss: 0.0097\n",
            "Epoch [97/100], Step [304/493], Loss: 0.0054\n",
            "Epoch [97/100], Step [305/493], Loss: 0.0081\n",
            "Epoch [97/100], Step [306/493], Loss: 0.0339\n",
            "Epoch [97/100], Step [307/493], Loss: 0.0391\n",
            "Epoch [97/100], Step [308/493], Loss: 0.0065\n",
            "Epoch [97/100], Step [309/493], Loss: 0.0014\n",
            "Epoch [97/100], Step [310/493], Loss: 0.0172\n",
            "Epoch [97/100], Step [311/493], Loss: 0.1857\n",
            "Epoch [97/100], Step [312/493], Loss: 0.0382\n",
            "Epoch [97/100], Step [313/493], Loss: 0.0110\n",
            "Epoch [97/100], Step [314/493], Loss: 0.0059\n",
            "Epoch [97/100], Step [315/493], Loss: 0.0081\n",
            "Epoch [97/100], Step [316/493], Loss: 0.0360\n",
            "Epoch [97/100], Step [317/493], Loss: 0.0143\n",
            "Epoch [97/100], Step [318/493], Loss: 0.0108\n",
            "Epoch [97/100], Step [319/493], Loss: 0.0010\n",
            "Epoch [97/100], Step [320/493], Loss: 0.0046\n",
            "Epoch [97/100], Step [321/493], Loss: 0.0053\n",
            "Epoch [97/100], Step [322/493], Loss: 0.0062\n",
            "Epoch [97/100], Step [323/493], Loss: 0.0021\n",
            "Epoch [97/100], Step [324/493], Loss: 0.0095\n",
            "Epoch [97/100], Step [325/493], Loss: 0.0870\n",
            "Epoch [97/100], Step [326/493], Loss: 0.0080\n",
            "Epoch [97/100], Step [327/493], Loss: 0.0031\n",
            "Epoch [97/100], Step [328/493], Loss: 0.0277\n",
            "Epoch [97/100], Step [329/493], Loss: 0.0075\n",
            "Epoch [97/100], Step [330/493], Loss: 0.0035\n",
            "Epoch [97/100], Step [331/493], Loss: 0.0051\n",
            "Epoch [97/100], Step [332/493], Loss: 0.0253\n",
            "Epoch [97/100], Step [333/493], Loss: 0.0022\n",
            "Epoch [97/100], Step [334/493], Loss: 0.0033\n",
            "Epoch [97/100], Step [335/493], Loss: 0.0140\n",
            "Epoch [97/100], Step [336/493], Loss: 0.0029\n",
            "Epoch [97/100], Step [337/493], Loss: 0.0040\n",
            "Epoch [97/100], Step [338/493], Loss: 0.2338\n",
            "Epoch [97/100], Step [339/493], Loss: 0.0020\n",
            "Epoch [97/100], Step [340/493], Loss: 0.2135\n",
            "Epoch [97/100], Step [341/493], Loss: 0.0136\n",
            "Epoch [97/100], Step [342/493], Loss: 0.0060\n",
            "Epoch [97/100], Step [343/493], Loss: 0.0123\n",
            "Epoch [97/100], Step [344/493], Loss: 0.0035\n",
            "Epoch [97/100], Step [345/493], Loss: 0.0078\n",
            "Epoch [97/100], Step [346/493], Loss: 0.0054\n",
            "Epoch [97/100], Step [347/493], Loss: 0.0097\n",
            "Epoch [97/100], Step [348/493], Loss: 0.0133\n",
            "Epoch [97/100], Step [349/493], Loss: 0.0055\n",
            "Epoch [97/100], Step [350/493], Loss: 0.0063\n",
            "Epoch [97/100], Step [351/493], Loss: 0.0044\n",
            "Epoch [97/100], Step [352/493], Loss: 0.0202\n",
            "Epoch [97/100], Step [353/493], Loss: 0.0227\n",
            "Epoch [97/100], Step [354/493], Loss: 0.0070\n",
            "Epoch [97/100], Step [355/493], Loss: 0.0047\n",
            "Epoch [97/100], Step [356/493], Loss: 0.0369\n",
            "Epoch [97/100], Step [357/493], Loss: 0.0131\n",
            "Epoch [97/100], Step [358/493], Loss: 0.0072\n",
            "Epoch [97/100], Step [359/493], Loss: 0.0128\n",
            "Epoch [97/100], Step [360/493], Loss: 0.4792\n",
            "Epoch [97/100], Step [361/493], Loss: 0.0280\n",
            "Epoch [97/100], Step [362/493], Loss: 0.0088\n",
            "Epoch [97/100], Step [363/493], Loss: 0.0244\n",
            "Epoch [97/100], Step [364/493], Loss: 0.0074\n",
            "Epoch [97/100], Step [365/493], Loss: 0.0067\n",
            "Epoch [97/100], Step [366/493], Loss: 0.0147\n",
            "Epoch [97/100], Step [367/493], Loss: 0.0039\n",
            "Epoch [97/100], Step [368/493], Loss: 0.0074\n",
            "Epoch [97/100], Step [369/493], Loss: 0.0070\n",
            "Epoch [97/100], Step [370/493], Loss: 0.0081\n",
            "Epoch [97/100], Step [371/493], Loss: 0.0093\n",
            "Epoch [97/100], Step [372/493], Loss: 0.0007\n",
            "Epoch [97/100], Step [373/493], Loss: 0.0041\n",
            "Epoch [97/100], Step [374/493], Loss: 0.0445\n",
            "Epoch [97/100], Step [375/493], Loss: 0.0057\n",
            "Epoch [97/100], Step [376/493], Loss: 0.0079\n",
            "Epoch [97/100], Step [377/493], Loss: 0.0022\n",
            "Epoch [97/100], Step [378/493], Loss: 0.0112\n",
            "Epoch [97/100], Step [379/493], Loss: 0.0029\n",
            "Epoch [97/100], Step [380/493], Loss: 0.0118\n",
            "Epoch [97/100], Step [381/493], Loss: 0.0032\n",
            "Epoch [97/100], Step [382/493], Loss: 0.0035\n",
            "Epoch [97/100], Step [383/493], Loss: 0.0049\n",
            "Epoch [97/100], Step [384/493], Loss: 0.0154\n",
            "Epoch [97/100], Step [385/493], Loss: 0.0078\n",
            "Epoch [97/100], Step [386/493], Loss: 0.0023\n",
            "Epoch [97/100], Step [387/493], Loss: 0.0129\n",
            "Epoch [97/100], Step [388/493], Loss: 0.0210\n",
            "Epoch [97/100], Step [389/493], Loss: 0.0005\n",
            "Epoch [97/100], Step [390/493], Loss: 0.0116\n",
            "Epoch [97/100], Step [391/493], Loss: 0.0073\n",
            "Epoch [97/100], Step [392/493], Loss: 0.0121\n",
            "Epoch [97/100], Step [393/493], Loss: 0.5350\n",
            "Epoch [97/100], Step [394/493], Loss: 0.0081\n",
            "Epoch [97/100], Step [395/493], Loss: 0.0131\n",
            "Epoch [97/100], Step [396/493], Loss: 0.0050\n",
            "Epoch [97/100], Step [397/493], Loss: 0.0080\n",
            "Epoch [97/100], Step [398/493], Loss: 0.0284\n",
            "Epoch [97/100], Step [399/493], Loss: 0.0175\n",
            "Epoch [97/100], Step [400/493], Loss: 0.0154\n",
            "Epoch [97/100], Step [401/493], Loss: 0.0029\n",
            "Epoch [97/100], Step [402/493], Loss: 0.0011\n",
            "Epoch [97/100], Step [403/493], Loss: 0.0138\n",
            "Epoch [97/100], Step [404/493], Loss: 0.0117\n",
            "Epoch [97/100], Step [405/493], Loss: 0.0019\n",
            "Epoch [97/100], Step [406/493], Loss: 0.0209\n",
            "Epoch [97/100], Step [407/493], Loss: 0.0180\n",
            "Epoch [97/100], Step [408/493], Loss: 0.0074\n",
            "Epoch [97/100], Step [409/493], Loss: 0.0080\n",
            "Epoch [97/100], Step [410/493], Loss: 0.0412\n",
            "Epoch [97/100], Step [411/493], Loss: 0.0141\n",
            "Epoch [97/100], Step [412/493], Loss: 0.0131\n",
            "Epoch [97/100], Step [413/493], Loss: 0.0597\n",
            "Epoch [97/100], Step [414/493], Loss: 0.1114\n",
            "Epoch [97/100], Step [415/493], Loss: 0.0043\n",
            "Epoch [97/100], Step [416/493], Loss: 0.0230\n",
            "Epoch [97/100], Step [417/493], Loss: 0.0033\n",
            "Epoch [97/100], Step [418/493], Loss: 0.0072\n",
            "Epoch [97/100], Step [419/493], Loss: 0.0114\n",
            "Epoch [97/100], Step [420/493], Loss: 0.0228\n",
            "Epoch [97/100], Step [421/493], Loss: 0.0040\n",
            "Epoch [97/100], Step [422/493], Loss: 0.0047\n",
            "Epoch [97/100], Step [423/493], Loss: 0.0316\n",
            "Epoch [97/100], Step [424/493], Loss: 0.0099\n",
            "Epoch [97/100], Step [425/493], Loss: 0.0057\n",
            "Epoch [97/100], Step [426/493], Loss: 0.0833\n",
            "Epoch [97/100], Step [427/493], Loss: 0.0070\n",
            "Epoch [97/100], Step [428/493], Loss: 0.1317\n",
            "Epoch [97/100], Step [429/493], Loss: 0.0065\n",
            "Epoch [97/100], Step [430/493], Loss: 0.0091\n",
            "Epoch [97/100], Step [431/493], Loss: 0.0062\n",
            "Epoch [97/100], Step [432/493], Loss: 0.0073\n",
            "Epoch [97/100], Step [433/493], Loss: 0.1666\n",
            "Epoch [97/100], Step [434/493], Loss: 0.0083\n",
            "Epoch [97/100], Step [435/493], Loss: 0.0126\n",
            "Epoch [97/100], Step [436/493], Loss: 0.0031\n",
            "Epoch [97/100], Step [437/493], Loss: 0.0124\n",
            "Epoch [97/100], Step [438/493], Loss: 0.0128\n",
            "Epoch [97/100], Step [439/493], Loss: 0.0066\n",
            "Epoch [97/100], Step [440/493], Loss: 0.0043\n",
            "Epoch [97/100], Step [441/493], Loss: 0.3149\n",
            "Epoch [97/100], Step [442/493], Loss: 0.0122\n",
            "Epoch [97/100], Step [443/493], Loss: 0.0032\n",
            "Epoch [97/100], Step [444/493], Loss: 0.0014\n",
            "Epoch [97/100], Step [445/493], Loss: 0.0123\n",
            "Epoch [97/100], Step [446/493], Loss: 0.0264\n",
            "Epoch [97/100], Step [447/493], Loss: 0.0181\n",
            "Epoch [97/100], Step [448/493], Loss: 0.0116\n",
            "Epoch [97/100], Step [449/493], Loss: 0.0071\n",
            "Epoch [97/100], Step [450/493], Loss: 0.0140\n",
            "Epoch [97/100], Step [451/493], Loss: 0.0042\n",
            "Epoch [97/100], Step [452/493], Loss: 0.0132\n",
            "Epoch [97/100], Step [453/493], Loss: 0.0075\n",
            "Epoch [97/100], Step [454/493], Loss: 0.0019\n",
            "Epoch [97/100], Step [455/493], Loss: 0.0073\n",
            "Epoch [97/100], Step [456/493], Loss: 0.0062\n",
            "Epoch [97/100], Step [457/493], Loss: 0.0011\n",
            "Epoch [97/100], Step [458/493], Loss: 0.0117\n",
            "Epoch [97/100], Step [459/493], Loss: 0.0295\n",
            "Epoch [97/100], Step [460/493], Loss: 0.0030\n",
            "Epoch [97/100], Step [461/493], Loss: 0.0103\n",
            "Epoch [97/100], Step [462/493], Loss: 0.0029\n",
            "Epoch [97/100], Step [463/493], Loss: 0.0036\n",
            "Epoch [97/100], Step [464/493], Loss: 0.0039\n",
            "Epoch [97/100], Step [465/493], Loss: 0.0023\n",
            "Epoch [97/100], Step [466/493], Loss: 0.0191\n",
            "Epoch [97/100], Step [467/493], Loss: 0.0085\n",
            "Epoch [97/100], Step [468/493], Loss: 0.0092\n",
            "Epoch [97/100], Step [469/493], Loss: 0.0059\n",
            "Epoch [97/100], Step [470/493], Loss: 0.0039\n",
            "Epoch [97/100], Step [471/493], Loss: 0.0280\n",
            "Epoch [97/100], Step [472/493], Loss: 0.0001\n",
            "Epoch [97/100], Step [473/493], Loss: 0.0073\n",
            "Epoch [97/100], Step [474/493], Loss: 0.0065\n",
            "Epoch [97/100], Step [475/493], Loss: 0.0117\n",
            "Epoch [97/100], Step [476/493], Loss: 0.0055\n",
            "Epoch [97/100], Step [477/493], Loss: 0.0403\n",
            "Epoch [97/100], Step [478/493], Loss: 0.0084\n",
            "Epoch [97/100], Step [479/493], Loss: 0.0161\n",
            "Epoch [97/100], Step [480/493], Loss: 0.0067\n",
            "Epoch [97/100], Step [481/493], Loss: 0.0952\n",
            "Epoch [97/100], Step [482/493], Loss: 0.0074\n",
            "Epoch [97/100], Step [483/493], Loss: 0.0838\n",
            "Epoch [97/100], Step [484/493], Loss: 0.0059\n",
            "Epoch [97/100], Step [485/493], Loss: 0.0109\n",
            "Epoch [97/100], Step [486/493], Loss: 0.0036\n",
            "Epoch [97/100], Step [487/493], Loss: 0.0045\n",
            "Epoch [97/100], Step [488/493], Loss: 0.0131\n",
            "Epoch [97/100], Step [489/493], Loss: 0.2984\n",
            "Epoch [97/100], Step [490/493], Loss: 0.0578\n",
            "Epoch [97/100], Step [491/493], Loss: 0.0060\n",
            "Epoch [97/100], Step [492/493], Loss: 0.0141\n",
            "Epoch [97/100], Step [493/493], Loss: 0.0140\n",
            "Epoch [98/100], Step [1/493], Loss: 0.0058\n",
            "Epoch [98/100], Step [2/493], Loss: 0.0095\n",
            "Epoch [98/100], Step [3/493], Loss: 0.0081\n",
            "Epoch [98/100], Step [4/493], Loss: 0.0092\n",
            "Epoch [98/100], Step [5/493], Loss: 0.0121\n",
            "Epoch [98/100], Step [6/493], Loss: 0.0050\n",
            "Epoch [98/100], Step [7/493], Loss: 0.0071\n",
            "Epoch [98/100], Step [8/493], Loss: 0.0065\n",
            "Epoch [98/100], Step [9/493], Loss: 0.0131\n",
            "Epoch [98/100], Step [10/493], Loss: 0.0109\n",
            "Epoch [98/100], Step [11/493], Loss: 0.0172\n",
            "Epoch [98/100], Step [12/493], Loss: 0.0893\n",
            "Epoch [98/100], Step [13/493], Loss: 0.0030\n",
            "Epoch [98/100], Step [14/493], Loss: 0.0003\n",
            "Epoch [98/100], Step [15/493], Loss: 0.0080\n",
            "Epoch [98/100], Step [16/493], Loss: 0.0230\n",
            "Epoch [98/100], Step [17/493], Loss: 0.0023\n",
            "Epoch [98/100], Step [18/493], Loss: 0.0069\n",
            "Epoch [98/100], Step [19/493], Loss: 0.0154\n",
            "Epoch [98/100], Step [20/493], Loss: 0.0290\n",
            "Epoch [98/100], Step [21/493], Loss: 0.0059\n",
            "Epoch [98/100], Step [22/493], Loss: 0.2245\n",
            "Epoch [98/100], Step [23/493], Loss: 0.0047\n",
            "Epoch [98/100], Step [24/493], Loss: 0.0077\n",
            "Epoch [98/100], Step [25/493], Loss: 0.0077\n",
            "Epoch [98/100], Step [26/493], Loss: 0.0084\n",
            "Epoch [98/100], Step [27/493], Loss: 0.0117\n",
            "Epoch [98/100], Step [28/493], Loss: 0.0037\n",
            "Epoch [98/100], Step [29/493], Loss: 0.0040\n",
            "Epoch [98/100], Step [30/493], Loss: 0.0133\n",
            "Epoch [98/100], Step [31/493], Loss: 0.0268\n",
            "Epoch [98/100], Step [32/493], Loss: 0.0018\n",
            "Epoch [98/100], Step [33/493], Loss: 0.0178\n",
            "Epoch [98/100], Step [34/493], Loss: 0.0179\n",
            "Epoch [98/100], Step [35/493], Loss: 0.0059\n",
            "Epoch [98/100], Step [36/493], Loss: 0.0034\n",
            "Epoch [98/100], Step [37/493], Loss: 0.0080\n",
            "Epoch [98/100], Step [38/493], Loss: 0.0123\n",
            "Epoch [98/100], Step [39/493], Loss: 0.0065\n",
            "Epoch [98/100], Step [40/493], Loss: 0.0147\n",
            "Epoch [98/100], Step [41/493], Loss: 0.0070\n",
            "Epoch [98/100], Step [42/493], Loss: 0.0081\n",
            "Epoch [98/100], Step [43/493], Loss: 0.0041\n",
            "Epoch [98/100], Step [44/493], Loss: 0.0011\n",
            "Epoch [98/100], Step [45/493], Loss: 0.0180\n",
            "Epoch [98/100], Step [46/493], Loss: 0.0054\n",
            "Epoch [98/100], Step [47/493], Loss: 0.0067\n",
            "Epoch [98/100], Step [48/493], Loss: 0.0117\n",
            "Epoch [98/100], Step [49/493], Loss: 0.0170\n",
            "Epoch [98/100], Step [50/493], Loss: 0.0044\n",
            "Epoch [98/100], Step [51/493], Loss: 0.0268\n",
            "Epoch [98/100], Step [52/493], Loss: 0.0122\n",
            "Epoch [98/100], Step [53/493], Loss: 0.0061\n",
            "Epoch [98/100], Step [54/493], Loss: 0.0001\n",
            "Epoch [98/100], Step [55/493], Loss: 0.0109\n",
            "Epoch [98/100], Step [56/493], Loss: 0.0032\n",
            "Epoch [98/100], Step [57/493], Loss: 0.0024\n",
            "Epoch [98/100], Step [58/493], Loss: 0.0043\n",
            "Epoch [98/100], Step [59/493], Loss: 0.0073\n",
            "Epoch [98/100], Step [60/493], Loss: 0.0305\n",
            "Epoch [98/100], Step [61/493], Loss: 0.0070\n",
            "Epoch [98/100], Step [62/493], Loss: 0.0044\n",
            "Epoch [98/100], Step [63/493], Loss: 0.0167\n",
            "Epoch [98/100], Step [64/493], Loss: 0.0121\n",
            "Epoch [98/100], Step [65/493], Loss: 0.0094\n",
            "Epoch [98/100], Step [66/493], Loss: 0.0045\n",
            "Epoch [98/100], Step [67/493], Loss: 0.0021\n",
            "Epoch [98/100], Step [68/493], Loss: 0.0029\n",
            "Epoch [98/100], Step [69/493], Loss: 0.0110\n",
            "Epoch [98/100], Step [70/493], Loss: 0.0109\n",
            "Epoch [98/100], Step [71/493], Loss: 0.0037\n",
            "Epoch [98/100], Step [72/493], Loss: 0.0060\n",
            "Epoch [98/100], Step [73/493], Loss: 0.0143\n",
            "Epoch [98/100], Step [74/493], Loss: 0.0195\n",
            "Epoch [98/100], Step [75/493], Loss: 0.0128\n",
            "Epoch [98/100], Step [76/493], Loss: 0.1790\n",
            "Epoch [98/100], Step [77/493], Loss: 0.0194\n",
            "Epoch [98/100], Step [78/493], Loss: 0.0080\n",
            "Epoch [98/100], Step [79/493], Loss: 0.0057\n",
            "Epoch [98/100], Step [80/493], Loss: 0.0330\n",
            "Epoch [98/100], Step [81/493], Loss: 0.0032\n",
            "Epoch [98/100], Step [82/493], Loss: 0.0054\n",
            "Epoch [98/100], Step [83/493], Loss: 0.0102\n",
            "Epoch [98/100], Step [84/493], Loss: 0.0117\n",
            "Epoch [98/100], Step [85/493], Loss: 0.0036\n",
            "Epoch [98/100], Step [86/493], Loss: 0.0051\n",
            "Epoch [98/100], Step [87/493], Loss: 0.0029\n",
            "Epoch [98/100], Step [88/493], Loss: 0.0043\n",
            "Epoch [98/100], Step [89/493], Loss: 0.0096\n",
            "Epoch [98/100], Step [90/493], Loss: 0.0086\n",
            "Epoch [98/100], Step [91/493], Loss: 0.1857\n",
            "Epoch [98/100], Step [92/493], Loss: 0.0039\n",
            "Epoch [98/100], Step [93/493], Loss: 0.2014\n",
            "Epoch [98/100], Step [94/493], Loss: 0.0129\n",
            "Epoch [98/100], Step [95/493], Loss: 0.0049\n",
            "Epoch [98/100], Step [96/493], Loss: 0.0069\n",
            "Epoch [98/100], Step [97/493], Loss: 0.0175\n",
            "Epoch [98/100], Step [98/493], Loss: 0.0300\n",
            "Epoch [98/100], Step [99/493], Loss: 0.0093\n",
            "Epoch [98/100], Step [100/493], Loss: 0.0344\n",
            "Epoch [98/100], Step [101/493], Loss: 0.0091\n",
            "Epoch [98/100], Step [102/493], Loss: 0.0067\n",
            "Epoch [98/100], Step [103/493], Loss: 0.0077\n",
            "Epoch [98/100], Step [104/493], Loss: 0.0263\n",
            "Epoch [98/100], Step [105/493], Loss: 0.0102\n",
            "Epoch [98/100], Step [106/493], Loss: 0.0146\n",
            "Epoch [98/100], Step [107/493], Loss: 0.0057\n",
            "Epoch [98/100], Step [108/493], Loss: 0.0033\n",
            "Epoch [98/100], Step [109/493], Loss: 0.0143\n",
            "Epoch [98/100], Step [110/493], Loss: 0.0081\n",
            "Epoch [98/100], Step [111/493], Loss: 0.0169\n",
            "Epoch [98/100], Step [112/493], Loss: 0.0074\n",
            "Epoch [98/100], Step [113/493], Loss: 0.0058\n",
            "Epoch [98/100], Step [114/493], Loss: 0.0114\n",
            "Epoch [98/100], Step [115/493], Loss: 0.0121\n",
            "Epoch [98/100], Step [116/493], Loss: 0.0080\n",
            "Epoch [98/100], Step [117/493], Loss: 0.0042\n",
            "Epoch [98/100], Step [118/493], Loss: 0.0065\n",
            "Epoch [98/100], Step [119/493], Loss: 0.0250\n",
            "Epoch [98/100], Step [120/493], Loss: 0.0022\n",
            "Epoch [98/100], Step [121/493], Loss: 0.0002\n",
            "Epoch [98/100], Step [122/493], Loss: 0.0099\n",
            "Epoch [98/100], Step [123/493], Loss: 0.0057\n",
            "Epoch [98/100], Step [124/493], Loss: 0.0277\n",
            "Epoch [98/100], Step [125/493], Loss: 0.0054\n",
            "Epoch [98/100], Step [126/493], Loss: 0.0033\n",
            "Epoch [98/100], Step [127/493], Loss: 0.0045\n",
            "Epoch [98/100], Step [128/493], Loss: 0.1099\n",
            "Epoch [98/100], Step [129/493], Loss: 0.0105\n",
            "Epoch [98/100], Step [130/493], Loss: 0.0103\n",
            "Epoch [98/100], Step [131/493], Loss: 0.0026\n",
            "Epoch [98/100], Step [132/493], Loss: 0.0022\n",
            "Epoch [98/100], Step [133/493], Loss: 0.1024\n",
            "Epoch [98/100], Step [134/493], Loss: 0.0062\n",
            "Epoch [98/100], Step [135/493], Loss: 0.0124\n",
            "Epoch [98/100], Step [136/493], Loss: 0.0095\n",
            "Epoch [98/100], Step [137/493], Loss: 0.0295\n",
            "Epoch [98/100], Step [138/493], Loss: 0.0078\n",
            "Epoch [98/100], Step [139/493], Loss: 0.0141\n",
            "Epoch [98/100], Step [140/493], Loss: 0.0046\n",
            "Epoch [98/100], Step [141/493], Loss: 0.0099\n",
            "Epoch [98/100], Step [142/493], Loss: 0.0045\n",
            "Epoch [98/100], Step [143/493], Loss: 0.0138\n",
            "Epoch [98/100], Step [144/493], Loss: 0.0067\n",
            "Epoch [98/100], Step [145/493], Loss: 0.0041\n",
            "Epoch [98/100], Step [146/493], Loss: 0.0228\n",
            "Epoch [98/100], Step [147/493], Loss: 0.0126\n",
            "Epoch [98/100], Step [148/493], Loss: 0.0382\n",
            "Epoch [98/100], Step [149/493], Loss: 0.0123\n",
            "Epoch [98/100], Step [150/493], Loss: 0.0308\n",
            "Epoch [98/100], Step [151/493], Loss: 0.0035\n",
            "Epoch [98/100], Step [152/493], Loss: 0.0018\n",
            "Epoch [98/100], Step [153/493], Loss: 0.0035\n",
            "Epoch [98/100], Step [154/493], Loss: 0.0158\n",
            "Epoch [98/100], Step [155/493], Loss: 0.0118\n",
            "Epoch [98/100], Step [156/493], Loss: 0.0075\n",
            "Epoch [98/100], Step [157/493], Loss: 0.0952\n",
            "Epoch [98/100], Step [158/493], Loss: 0.0033\n",
            "Epoch [98/100], Step [159/493], Loss: 0.0685\n",
            "Epoch [98/100], Step [160/493], Loss: 0.0097\n",
            "Epoch [98/100], Step [161/493], Loss: 0.2259\n",
            "Epoch [98/100], Step [162/493], Loss: 0.0019\n",
            "Epoch [98/100], Step [163/493], Loss: 0.0225\n",
            "Epoch [98/100], Step [164/493], Loss: 0.0100\n",
            "Epoch [98/100], Step [165/493], Loss: 0.0103\n",
            "Epoch [98/100], Step [166/493], Loss: 0.0074\n",
            "Epoch [98/100], Step [167/493], Loss: 0.0060\n",
            "Epoch [98/100], Step [168/493], Loss: 0.0054\n",
            "Epoch [98/100], Step [169/493], Loss: 0.0202\n",
            "Epoch [98/100], Step [170/493], Loss: 0.0833\n",
            "Epoch [98/100], Step [171/493], Loss: 0.0081\n",
            "Epoch [98/100], Step [172/493], Loss: 0.1666\n",
            "Epoch [98/100], Step [173/493], Loss: 0.0029\n",
            "Epoch [98/100], Step [174/493], Loss: 0.0084\n",
            "Epoch [98/100], Step [175/493], Loss: 0.0060\n",
            "Epoch [98/100], Step [176/493], Loss: 0.0141\n",
            "Epoch [98/100], Step [177/493], Loss: 0.0063\n",
            "Epoch [98/100], Step [178/493], Loss: 0.0119\n",
            "Epoch [98/100], Step [179/493], Loss: 0.0140\n",
            "Epoch [98/100], Step [180/493], Loss: 0.0039\n",
            "Epoch [98/100], Step [181/493], Loss: 0.0089\n",
            "Epoch [98/100], Step [182/493], Loss: 0.0117\n",
            "Epoch [98/100], Step [183/493], Loss: 0.2338\n",
            "Epoch [98/100], Step [184/493], Loss: 0.0033\n",
            "Epoch [98/100], Step [185/493], Loss: 0.2984\n",
            "Epoch [98/100], Step [186/493], Loss: 0.0360\n",
            "Epoch [98/100], Step [187/493], Loss: 0.0031\n",
            "Epoch [98/100], Step [188/493], Loss: 0.0031\n",
            "Epoch [98/100], Step [189/493], Loss: 0.0015\n",
            "Epoch [98/100], Step [190/493], Loss: 0.0145\n",
            "Epoch [98/100], Step [191/493], Loss: 0.0093\n",
            "Epoch [98/100], Step [192/493], Loss: 0.0068\n",
            "Epoch [98/100], Step [193/493], Loss: 0.0027\n",
            "Epoch [98/100], Step [194/493], Loss: 0.0094\n",
            "Epoch [98/100], Step [195/493], Loss: 0.0035\n",
            "Epoch [98/100], Step [196/493], Loss: 0.0071\n",
            "Epoch [98/100], Step [197/493], Loss: 0.0424\n",
            "Epoch [98/100], Step [198/493], Loss: 0.0128\n",
            "Epoch [98/100], Step [199/493], Loss: 0.0657\n",
            "Epoch [98/100], Step [200/493], Loss: 0.0101\n",
            "Epoch [98/100], Step [201/493], Loss: 0.0330\n",
            "Epoch [98/100], Step [202/493], Loss: 0.0117\n",
            "Epoch [98/100], Step [203/493], Loss: 0.0047\n",
            "Epoch [98/100], Step [204/493], Loss: 0.0083\n",
            "Epoch [98/100], Step [205/493], Loss: 0.0059\n",
            "Epoch [98/100], Step [206/493], Loss: 0.0005\n",
            "Epoch [98/100], Step [207/493], Loss: 0.0079\n",
            "Epoch [98/100], Step [208/493], Loss: 0.0081\n",
            "Epoch [98/100], Step [209/493], Loss: 0.0022\n",
            "Epoch [98/100], Step [210/493], Loss: 0.0132\n",
            "Epoch [98/100], Step [211/493], Loss: 0.0071\n",
            "Epoch [98/100], Step [212/493], Loss: 0.0112\n",
            "Epoch [98/100], Step [213/493], Loss: 0.0330\n",
            "Epoch [98/100], Step [214/493], Loss: 0.0018\n",
            "Epoch [98/100], Step [215/493], Loss: 0.0073\n",
            "Epoch [98/100], Step [216/493], Loss: 0.0154\n",
            "Epoch [98/100], Step [217/493], Loss: 0.0209\n",
            "Epoch [98/100], Step [218/493], Loss: 0.0133\n",
            "Epoch [98/100], Step [219/493], Loss: 0.0244\n",
            "Epoch [98/100], Step [220/493], Loss: 0.0144\n",
            "Epoch [98/100], Step [221/493], Loss: 0.0181\n",
            "Epoch [98/100], Step [222/493], Loss: 0.1087\n",
            "Epoch [98/100], Step [223/493], Loss: 0.0011\n",
            "Epoch [98/100], Step [224/493], Loss: 0.0059\n",
            "Epoch [98/100], Step [225/493], Loss: 0.0339\n",
            "Epoch [98/100], Step [226/493], Loss: 0.0010\n",
            "Epoch [98/100], Step [227/493], Loss: 0.0055\n",
            "Epoch [98/100], Step [228/493], Loss: 0.0154\n",
            "Epoch [98/100], Step [229/493], Loss: 0.0048\n",
            "Epoch [98/100], Step [230/493], Loss: 0.0210\n",
            "Epoch [98/100], Step [231/493], Loss: 0.0083\n",
            "Epoch [98/100], Step [232/493], Loss: 0.0029\n",
            "Epoch [98/100], Step [233/493], Loss: 0.0289\n",
            "Epoch [98/100], Step [234/493], Loss: 0.0277\n",
            "Epoch [98/100], Step [235/493], Loss: 0.0014\n",
            "Epoch [98/100], Step [236/493], Loss: 0.0085\n",
            "Epoch [98/100], Step [237/493], Loss: 0.0058\n",
            "Epoch [98/100], Step [238/493], Loss: 0.0039\n",
            "Epoch [98/100], Step [239/493], Loss: 0.0027\n",
            "Epoch [98/100], Step [240/493], Loss: 0.0163\n",
            "Epoch [98/100], Step [241/493], Loss: 0.0078\n",
            "Epoch [98/100], Step [242/493], Loss: 0.0259\n",
            "Epoch [98/100], Step [243/493], Loss: 0.0084\n",
            "Epoch [98/100], Step [244/493], Loss: 0.0129\n",
            "Epoch [98/100], Step [245/493], Loss: 0.0072\n",
            "Epoch [98/100], Step [246/493], Loss: 0.0141\n",
            "Epoch [98/100], Step [247/493], Loss: 0.0057\n",
            "Epoch [98/100], Step [248/493], Loss: 0.0870\n",
            "Epoch [98/100], Step [249/493], Loss: 0.0403\n",
            "Epoch [98/100], Step [250/493], Loss: 0.0340\n",
            "Epoch [98/100], Step [251/493], Loss: 0.1910\n",
            "Epoch [98/100], Step [252/493], Loss: 0.0156\n",
            "Epoch [98/100], Step [253/493], Loss: 0.0050\n",
            "Epoch [98/100], Step [254/493], Loss: 0.0098\n",
            "Epoch [98/100], Step [255/493], Loss: 0.0131\n",
            "Epoch [98/100], Step [256/493], Loss: 0.0252\n",
            "Epoch [98/100], Step [257/493], Loss: 0.0016\n",
            "Epoch [98/100], Step [258/493], Loss: 0.0049\n",
            "Epoch [98/100], Step [259/493], Loss: 0.0042\n",
            "Epoch [98/100], Step [260/493], Loss: 0.0263\n",
            "Epoch [98/100], Step [261/493], Loss: 0.0066\n",
            "Epoch [98/100], Step [262/493], Loss: 0.0136\n",
            "Epoch [98/100], Step [263/493], Loss: 0.0087\n",
            "Epoch [98/100], Step [264/493], Loss: 0.0174\n",
            "Epoch [98/100], Step [265/493], Loss: 0.0074\n",
            "Epoch [98/100], Step [266/493], Loss: 0.0042\n",
            "Epoch [98/100], Step [267/493], Loss: 0.0280\n",
            "Epoch [98/100], Step [268/493], Loss: 0.0092\n",
            "Epoch [98/100], Step [269/493], Loss: 0.0007\n",
            "Epoch [98/100], Step [270/493], Loss: 0.0086\n",
            "Epoch [98/100], Step [271/493], Loss: 0.0092\n",
            "Epoch [98/100], Step [272/493], Loss: 0.0070\n",
            "Epoch [98/100], Step [273/493], Loss: 0.0059\n",
            "Epoch [98/100], Step [274/493], Loss: 0.0029\n",
            "Epoch [98/100], Step [275/493], Loss: 0.0131\n",
            "Epoch [98/100], Step [276/493], Loss: 0.1181\n",
            "Epoch [98/100], Step [277/493], Loss: 0.0153\n",
            "Epoch [98/100], Step [278/493], Loss: 0.0325\n",
            "Epoch [98/100], Step [279/493], Loss: 0.0087\n",
            "Epoch [98/100], Step [280/493], Loss: 0.0085\n",
            "Epoch [98/100], Step [281/493], Loss: 0.0036\n",
            "Epoch [98/100], Step [282/493], Loss: 0.0043\n",
            "Epoch [98/100], Step [283/493], Loss: 0.0035\n",
            "Epoch [98/100], Step [284/493], Loss: 0.0077\n",
            "Epoch [98/100], Step [285/493], Loss: 0.0042\n",
            "Epoch [98/100], Step [286/493], Loss: 0.0092\n",
            "Epoch [98/100], Step [287/493], Loss: 0.0298\n",
            "Epoch [98/100], Step [288/493], Loss: 0.2699\n",
            "Epoch [98/100], Step [289/493], Loss: 0.0016\n",
            "Epoch [98/100], Step [290/493], Loss: 0.0017\n",
            "Epoch [98/100], Step [291/493], Loss: 0.0033\n",
            "Epoch [98/100], Step [292/493], Loss: 0.0264\n",
            "Epoch [98/100], Step [293/493], Loss: 0.0088\n",
            "Epoch [98/100], Step [294/493], Loss: 0.5350\n",
            "Epoch [98/100], Step [295/493], Loss: 0.0103\n",
            "Epoch [98/100], Step [296/493], Loss: 0.0100\n",
            "Epoch [98/100], Step [297/493], Loss: 0.0014\n",
            "Epoch [98/100], Step [298/493], Loss: 0.0075\n",
            "Epoch [98/100], Step [299/493], Loss: 0.0105\n",
            "Epoch [98/100], Step [300/493], Loss: 0.0382\n",
            "Epoch [98/100], Step [301/493], Loss: 0.0081\n",
            "Epoch [98/100], Step [302/493], Loss: 0.0040\n",
            "Epoch [98/100], Step [303/493], Loss: 0.0015\n",
            "Epoch [98/100], Step [304/493], Loss: 0.0077\n",
            "Epoch [98/100], Step [305/493], Loss: 0.0025\n",
            "Epoch [98/100], Step [306/493], Loss: 0.0058\n",
            "Epoch [98/100], Step [307/493], Loss: 0.0064\n",
            "Epoch [98/100], Step [308/493], Loss: 0.0012\n",
            "Epoch [98/100], Step [309/493], Loss: 0.0393\n",
            "Epoch [98/100], Step [310/493], Loss: 0.0074\n",
            "Epoch [98/100], Step [311/493], Loss: 0.0073\n",
            "Epoch [98/100], Step [312/493], Loss: 0.1286\n",
            "Epoch [98/100], Step [313/493], Loss: 0.0050\n",
            "Epoch [98/100], Step [314/493], Loss: 0.0077\n",
            "Epoch [98/100], Step [315/493], Loss: 0.0116\n",
            "Epoch [98/100], Step [316/493], Loss: 0.0445\n",
            "Epoch [98/100], Step [317/493], Loss: 0.1971\n",
            "Epoch [98/100], Step [318/493], Loss: 0.0077\n",
            "Epoch [98/100], Step [319/493], Loss: 0.0051\n",
            "Epoch [98/100], Step [320/493], Loss: 0.0096\n",
            "Epoch [98/100], Step [321/493], Loss: 0.0063\n",
            "Epoch [98/100], Step [322/493], Loss: 0.0032\n",
            "Epoch [98/100], Step [323/493], Loss: 0.0578\n",
            "Epoch [98/100], Step [324/493], Loss: 0.0369\n",
            "Epoch [98/100], Step [325/493], Loss: 0.0046\n",
            "Epoch [98/100], Step [326/493], Loss: 0.0019\n",
            "Epoch [98/100], Step [327/493], Loss: 0.0070\n",
            "Epoch [98/100], Step [328/493], Loss: 0.0456\n",
            "Epoch [98/100], Step [329/493], Loss: 0.0045\n",
            "Epoch [98/100], Step [330/493], Loss: 0.0686\n",
            "Epoch [98/100], Step [331/493], Loss: 0.0055\n",
            "Epoch [98/100], Step [332/493], Loss: 0.0229\n",
            "Epoch [98/100], Step [333/493], Loss: 0.0166\n",
            "Epoch [98/100], Step [334/493], Loss: 0.0073\n",
            "Epoch [98/100], Step [335/493], Loss: 0.0045\n",
            "Epoch [98/100], Step [336/493], Loss: 0.0316\n",
            "Epoch [98/100], Step [337/493], Loss: 0.0065\n",
            "Epoch [98/100], Step [338/493], Loss: 0.0434\n",
            "Epoch [98/100], Step [339/493], Loss: 0.0062\n",
            "Epoch [98/100], Step [340/493], Loss: 0.0084\n",
            "Epoch [98/100], Step [341/493], Loss: 0.0047\n",
            "Epoch [98/100], Step [342/493], Loss: 0.0010\n",
            "Epoch [98/100], Step [343/493], Loss: 0.0145\n",
            "Epoch [98/100], Step [344/493], Loss: 0.0110\n",
            "Epoch [98/100], Step [345/493], Loss: 0.0033\n",
            "Epoch [98/100], Step [346/493], Loss: 0.0072\n",
            "Epoch [98/100], Step [347/493], Loss: 0.0240\n",
            "Epoch [98/100], Step [348/493], Loss: 0.0027\n",
            "Epoch [98/100], Step [349/493], Loss: 0.0005\n",
            "Epoch [98/100], Step [350/493], Loss: 0.0124\n",
            "Epoch [98/100], Step [351/493], Loss: 0.0097\n",
            "Epoch [98/100], Step [352/493], Loss: 0.0253\n",
            "Epoch [98/100], Step [353/493], Loss: 0.0032\n",
            "Epoch [98/100], Step [354/493], Loss: 0.0020\n",
            "Epoch [98/100], Step [355/493], Loss: 0.0070\n",
            "Epoch [98/100], Step [356/493], Loss: 0.0280\n",
            "Epoch [98/100], Step [357/493], Loss: 0.0149\n",
            "Epoch [98/100], Step [358/493], Loss: 0.0060\n",
            "Epoch [98/100], Step [359/493], Loss: 0.0175\n",
            "Epoch [98/100], Step [360/493], Loss: 0.0264\n",
            "Epoch [98/100], Step [361/493], Loss: 0.0340\n",
            "Epoch [98/100], Step [362/493], Loss: 0.0065\n",
            "Epoch [98/100], Step [363/493], Loss: 0.0059\n",
            "Epoch [98/100], Step [364/493], Loss: 0.0116\n",
            "Epoch [98/100], Step [365/493], Loss: 0.0055\n",
            "Epoch [98/100], Step [366/493], Loss: 0.0014\n",
            "Epoch [98/100], Step [367/493], Loss: 0.0161\n",
            "Epoch [98/100], Step [368/493], Loss: 0.0264\n",
            "Epoch [98/100], Step [369/493], Loss: 0.0227\n",
            "Epoch [98/100], Step [370/493], Loss: 0.0055\n",
            "Epoch [98/100], Step [371/493], Loss: 0.0253\n",
            "Epoch [98/100], Step [372/493], Loss: 0.0168\n",
            "Epoch [98/100], Step [373/493], Loss: 0.0078\n",
            "Epoch [98/100], Step [374/493], Loss: 0.0043\n",
            "Epoch [98/100], Step [375/493], Loss: 0.0072\n",
            "Epoch [98/100], Step [376/493], Loss: 0.0413\n",
            "Epoch [98/100], Step [377/493], Loss: 0.0109\n",
            "Epoch [98/100], Step [378/493], Loss: 0.0412\n",
            "Epoch [98/100], Step [379/493], Loss: 0.0003\n",
            "Epoch [98/100], Step [380/493], Loss: 0.0018\n",
            "Epoch [98/100], Step [381/493], Loss: 0.0054\n",
            "Epoch [98/100], Step [382/493], Loss: 0.0206\n",
            "Epoch [98/100], Step [383/493], Loss: 0.0079\n",
            "Epoch [98/100], Step [384/493], Loss: 0.0068\n",
            "Epoch [98/100], Step [385/493], Loss: 0.0141\n",
            "Epoch [98/100], Step [386/493], Loss: 0.0391\n",
            "Epoch [98/100], Step [387/493], Loss: 0.0132\n",
            "Epoch [98/100], Step [388/493], Loss: 0.0066\n",
            "Epoch [98/100], Step [389/493], Loss: 0.0070\n",
            "Epoch [98/100], Step [390/493], Loss: 0.0059\n",
            "Epoch [98/100], Step [391/493], Loss: 0.0045\n",
            "Epoch [98/100], Step [392/493], Loss: 0.0079\n",
            "Epoch [98/100], Step [393/493], Loss: 0.0086\n",
            "Epoch [98/100], Step [394/493], Loss: 0.0094\n",
            "Epoch [98/100], Step [395/493], Loss: 0.1317\n",
            "Epoch [98/100], Step [396/493], Loss: 0.0040\n",
            "Epoch [98/100], Step [397/493], Loss: 0.0092\n",
            "Epoch [98/100], Step [398/493], Loss: 0.0284\n",
            "Epoch [98/100], Step [399/493], Loss: 0.0067\n",
            "Epoch [98/100], Step [400/493], Loss: 0.0179\n",
            "Epoch [98/100], Step [401/493], Loss: 0.0097\n",
            "Epoch [98/100], Step [402/493], Loss: 0.0230\n",
            "Epoch [98/100], Step [403/493], Loss: 0.0147\n",
            "Epoch [98/100], Step [404/493], Loss: 0.0068\n",
            "Epoch [98/100], Step [405/493], Loss: 0.0023\n",
            "Epoch [98/100], Step [406/493], Loss: 0.0872\n",
            "Epoch [98/100], Step [407/493], Loss: 0.0191\n",
            "Epoch [98/100], Step [408/493], Loss: 0.0072\n",
            "Epoch [98/100], Step [409/493], Loss: 0.0051\n",
            "Epoch [98/100], Step [410/493], Loss: 0.0046\n",
            "Epoch [98/100], Step [411/493], Loss: 0.0011\n",
            "Epoch [98/100], Step [412/493], Loss: 0.0074\n",
            "Epoch [98/100], Step [413/493], Loss: 0.0112\n",
            "Epoch [98/100], Step [414/493], Loss: 0.4792\n",
            "Epoch [98/100], Step [415/493], Loss: 0.2286\n",
            "Epoch [98/100], Step [416/493], Loss: 0.0208\n",
            "Epoch [98/100], Step [417/493], Loss: 0.0258\n",
            "Epoch [98/100], Step [418/493], Loss: 0.0031\n",
            "Epoch [98/100], Step [419/493], Loss: 0.0078\n",
            "Epoch [98/100], Step [420/493], Loss: 0.0014\n",
            "Epoch [98/100], Step [421/493], Loss: 0.0066\n",
            "Epoch [98/100], Step [422/493], Loss: 0.0305\n",
            "Epoch [98/100], Step [423/493], Loss: 0.1114\n",
            "Epoch [98/100], Step [424/493], Loss: 0.0031\n",
            "Epoch [98/100], Step [425/493], Loss: 0.0130\n",
            "Epoch [98/100], Step [426/493], Loss: 0.2135\n",
            "Epoch [98/100], Step [427/493], Loss: 0.0092\n",
            "Epoch [98/100], Step [428/493], Loss: 0.0056\n",
            "Epoch [98/100], Step [429/493], Loss: 0.0255\n",
            "Epoch [98/100], Step [430/493], Loss: 0.0096\n",
            "Epoch [98/100], Step [431/493], Loss: 0.0100\n",
            "Epoch [98/100], Step [432/493], Loss: 0.0074\n",
            "Epoch [98/100], Step [433/493], Loss: 0.0061\n",
            "Epoch [98/100], Step [434/493], Loss: 0.0136\n",
            "Epoch [98/100], Step [435/493], Loss: 0.0075\n",
            "Epoch [98/100], Step [436/493], Loss: 0.1683\n",
            "Epoch [98/100], Step [437/493], Loss: 0.0057\n",
            "Epoch [98/100], Step [438/493], Loss: 0.0097\n",
            "Epoch [98/100], Step [439/493], Loss: 0.0072\n",
            "Epoch [98/100], Step [440/493], Loss: 0.0029\n",
            "Epoch [98/100], Step [441/493], Loss: 0.0472\n",
            "Epoch [98/100], Step [442/493], Loss: 0.0075\n",
            "Epoch [98/100], Step [443/493], Loss: 0.3149\n",
            "Epoch [98/100], Step [444/493], Loss: 0.0058\n",
            "Epoch [98/100], Step [445/493], Loss: 0.0099\n",
            "Epoch [98/100], Step [446/493], Loss: 0.0770\n",
            "Epoch [98/100], Step [447/493], Loss: 0.0023\n",
            "Epoch [98/100], Step [448/493], Loss: 0.0200\n",
            "Epoch [98/100], Step [449/493], Loss: 0.0140\n",
            "Epoch [98/100], Step [450/493], Loss: 0.0030\n",
            "Epoch [98/100], Step [451/493], Loss: 0.0001\n",
            "Epoch [98/100], Step [452/493], Loss: 0.1160\n",
            "Epoch [98/100], Step [453/493], Loss: 0.0022\n",
            "Epoch [98/100], Step [454/493], Loss: 0.0058\n",
            "Epoch [98/100], Step [455/493], Loss: 0.0053\n",
            "Epoch [98/100], Step [456/493], Loss: 0.0144\n",
            "Epoch [98/100], Step [457/493], Loss: 0.0048\n",
            "Epoch [98/100], Step [458/493], Loss: 0.0068\n",
            "Epoch [98/100], Step [459/493], Loss: 0.0097\n",
            "Epoch [98/100], Step [460/493], Loss: 0.0131\n",
            "Epoch [98/100], Step [461/493], Loss: 0.0098\n",
            "Epoch [98/100], Step [462/493], Loss: 0.0062\n",
            "Epoch [98/100], Step [463/493], Loss: 0.0139\n",
            "Epoch [98/100], Step [464/493], Loss: 0.0597\n",
            "Epoch [98/100], Step [465/493], Loss: 0.0104\n",
            "Epoch [98/100], Step [466/493], Loss: 0.0023\n",
            "Epoch [98/100], Step [467/493], Loss: 0.0131\n",
            "Epoch [98/100], Step [468/493], Loss: 0.0123\n",
            "Epoch [98/100], Step [469/493], Loss: 0.0096\n",
            "Epoch [98/100], Step [470/493], Loss: 0.0074\n",
            "Epoch [98/100], Step [471/493], Loss: 0.0138\n",
            "Epoch [98/100], Step [472/493], Loss: 0.0097\n",
            "Epoch [98/100], Step [473/493], Loss: 0.0040\n",
            "Epoch [98/100], Step [474/493], Loss: 0.0108\n",
            "Epoch [98/100], Step [475/493], Loss: 0.0095\n",
            "Epoch [98/100], Step [476/493], Loss: 0.0083\n",
            "Epoch [98/100], Step [477/493], Loss: 0.0128\n",
            "Epoch [98/100], Step [478/493], Loss: 0.1575\n",
            "Epoch [98/100], Step [479/493], Loss: 0.0121\n",
            "Epoch [98/100], Step [480/493], Loss: 0.0029\n",
            "Epoch [98/100], Step [481/493], Loss: 0.0140\n",
            "Epoch [98/100], Step [482/493], Loss: 0.0037\n",
            "Epoch [98/100], Step [483/493], Loss: 0.0100\n",
            "Epoch [98/100], Step [484/493], Loss: 0.0039\n",
            "Epoch [98/100], Step [485/493], Loss: 0.0838\n",
            "Epoch [98/100], Step [486/493], Loss: 0.0052\n",
            "Epoch [98/100], Step [487/493], Loss: 0.0001\n",
            "Epoch [98/100], Step [488/493], Loss: 0.0097\n",
            "Epoch [98/100], Step [489/493], Loss: 0.0087\n",
            "Epoch [98/100], Step [490/493], Loss: 0.0124\n",
            "Epoch [98/100], Step [491/493], Loss: 0.0253\n",
            "Epoch [98/100], Step [492/493], Loss: 0.0060\n",
            "Epoch [98/100], Step [493/493], Loss: 0.0076\n",
            "Epoch [99/100], Step [1/493], Loss: 0.0161\n",
            "Epoch [99/100], Step [2/493], Loss: 0.0027\n",
            "Epoch [99/100], Step [3/493], Loss: 0.0141\n",
            "Epoch [99/100], Step [4/493], Loss: 0.0102\n",
            "Epoch [99/100], Step [5/493], Loss: 0.0369\n",
            "Epoch [99/100], Step [6/493], Loss: 0.0178\n",
            "Epoch [99/100], Step [7/493], Loss: 0.0143\n",
            "Epoch [99/100], Step [8/493], Loss: 0.0128\n",
            "Epoch [99/100], Step [9/493], Loss: 0.0046\n",
            "Epoch [99/100], Step [10/493], Loss: 0.0259\n",
            "Epoch [99/100], Step [11/493], Loss: 0.0095\n",
            "Epoch [99/100], Step [12/493], Loss: 0.0068\n",
            "Epoch [99/100], Step [13/493], Loss: 0.0119\n",
            "Epoch [99/100], Step [14/493], Loss: 0.2135\n",
            "Epoch [99/100], Step [15/493], Loss: 0.0058\n",
            "Epoch [99/100], Step [16/493], Loss: 0.0069\n",
            "Epoch [99/100], Step [17/493], Loss: 0.0032\n",
            "Epoch [99/100], Step [18/493], Loss: 0.0124\n",
            "Epoch [99/100], Step [19/493], Loss: 0.0146\n",
            "Epoch [99/100], Step [20/493], Loss: 0.0039\n",
            "Epoch [99/100], Step [21/493], Loss: 0.0133\n",
            "Epoch [99/100], Step [22/493], Loss: 0.2014\n",
            "Epoch [99/100], Step [23/493], Loss: 0.1286\n",
            "Epoch [99/100], Step [24/493], Loss: 0.0058\n",
            "Epoch [99/100], Step [25/493], Loss: 0.0179\n",
            "Epoch [99/100], Step [26/493], Loss: 0.0010\n",
            "Epoch [99/100], Step [27/493], Loss: 0.0075\n",
            "Epoch [99/100], Step [28/493], Loss: 0.0065\n",
            "Epoch [99/100], Step [29/493], Loss: 0.0055\n",
            "Epoch [99/100], Step [30/493], Loss: 0.0085\n",
            "Epoch [99/100], Step [31/493], Loss: 0.0015\n",
            "Epoch [99/100], Step [32/493], Loss: 0.0170\n",
            "Epoch [99/100], Step [33/493], Loss: 0.0036\n",
            "Epoch [99/100], Step [34/493], Loss: 0.0057\n",
            "Epoch [99/100], Step [35/493], Loss: 0.0099\n",
            "Epoch [99/100], Step [36/493], Loss: 0.0058\n",
            "Epoch [99/100], Step [37/493], Loss: 0.0018\n",
            "Epoch [99/100], Step [38/493], Loss: 0.0061\n",
            "Epoch [99/100], Step [39/493], Loss: 0.0016\n",
            "Epoch [99/100], Step [40/493], Loss: 0.0070\n",
            "Epoch [99/100], Step [41/493], Loss: 0.0121\n",
            "Epoch [99/100], Step [42/493], Loss: 0.0055\n",
            "Epoch [99/100], Step [43/493], Loss: 0.0057\n",
            "Epoch [99/100], Step [44/493], Loss: 0.0023\n",
            "Epoch [99/100], Step [45/493], Loss: 0.1666\n",
            "Epoch [99/100], Step [46/493], Loss: 0.0117\n",
            "Epoch [99/100], Step [47/493], Loss: 0.0072\n",
            "Epoch [99/100], Step [48/493], Loss: 0.0091\n",
            "Epoch [99/100], Step [49/493], Loss: 0.0225\n",
            "Epoch [99/100], Step [50/493], Loss: 0.0046\n",
            "Epoch [99/100], Step [51/493], Loss: 0.0154\n",
            "Epoch [99/100], Step [52/493], Loss: 0.0093\n",
            "Epoch [99/100], Step [53/493], Loss: 0.0065\n",
            "Epoch [99/100], Step [54/493], Loss: 0.0096\n",
            "Epoch [99/100], Step [55/493], Loss: 0.0340\n",
            "Epoch [99/100], Step [56/493], Loss: 0.0037\n",
            "Epoch [99/100], Step [57/493], Loss: 0.0037\n",
            "Epoch [99/100], Step [58/493], Loss: 0.0058\n",
            "Epoch [99/100], Step [59/493], Loss: 0.0179\n",
            "Epoch [99/100], Step [60/493], Loss: 0.0018\n",
            "Epoch [99/100], Step [61/493], Loss: 0.0014\n",
            "Epoch [99/100], Step [62/493], Loss: 0.0154\n",
            "Epoch [99/100], Step [63/493], Loss: 0.0893\n",
            "Epoch [99/100], Step [64/493], Loss: 0.0382\n",
            "Epoch [99/100], Step [65/493], Loss: 0.0020\n",
            "Epoch [99/100], Step [66/493], Loss: 0.0112\n",
            "Epoch [99/100], Step [67/493], Loss: 0.0075\n",
            "Epoch [99/100], Step [68/493], Loss: 0.0010\n",
            "Epoch [99/100], Step [69/493], Loss: 0.0103\n",
            "Epoch [99/100], Step [70/493], Loss: 0.0078\n",
            "Epoch [99/100], Step [71/493], Loss: 0.0080\n",
            "Epoch [99/100], Step [72/493], Loss: 0.0094\n",
            "Epoch [99/100], Step [73/493], Loss: 0.0029\n",
            "Epoch [99/100], Step [74/493], Loss: 0.0126\n",
            "Epoch [99/100], Step [75/493], Loss: 0.0001\n",
            "Epoch [99/100], Step [76/493], Loss: 0.0077\n",
            "Epoch [99/100], Step [77/493], Loss: 0.0133\n",
            "Epoch [99/100], Step [78/493], Loss: 0.0033\n",
            "Epoch [99/100], Step [79/493], Loss: 0.0070\n",
            "Epoch [99/100], Step [80/493], Loss: 0.2245\n",
            "Epoch [99/100], Step [81/493], Loss: 0.0001\n",
            "Epoch [99/100], Step [82/493], Loss: 0.0033\n",
            "Epoch [99/100], Step [83/493], Loss: 0.0045\n",
            "Epoch [99/100], Step [84/493], Loss: 0.0081\n",
            "Epoch [99/100], Step [85/493], Loss: 0.0074\n",
            "Epoch [99/100], Step [86/493], Loss: 0.0075\n",
            "Epoch [99/100], Step [87/493], Loss: 0.0054\n",
            "Epoch [99/100], Step [88/493], Loss: 0.0087\n",
            "Epoch [99/100], Step [89/493], Loss: 0.0060\n",
            "Epoch [99/100], Step [90/493], Loss: 0.0263\n",
            "Epoch [99/100], Step [91/493], Loss: 0.0230\n",
            "Epoch [99/100], Step [92/493], Loss: 0.0099\n",
            "Epoch [99/100], Step [93/493], Loss: 0.0117\n",
            "Epoch [99/100], Step [94/493], Loss: 0.0121\n",
            "Epoch [99/100], Step [95/493], Loss: 0.0050\n",
            "Epoch [99/100], Step [96/493], Loss: 0.0280\n",
            "Epoch [99/100], Step [97/493], Loss: 0.0071\n",
            "Epoch [99/100], Step [98/493], Loss: 0.0122\n",
            "Epoch [99/100], Step [99/493], Loss: 0.0092\n",
            "Epoch [99/100], Step [100/493], Loss: 0.0031\n",
            "Epoch [99/100], Step [101/493], Loss: 0.0045\n",
            "Epoch [99/100], Step [102/493], Loss: 0.0072\n",
            "Epoch [99/100], Step [103/493], Loss: 0.0181\n",
            "Epoch [99/100], Step [104/493], Loss: 0.0952\n",
            "Epoch [99/100], Step [105/493], Loss: 0.0316\n",
            "Epoch [99/100], Step [106/493], Loss: 0.0200\n",
            "Epoch [99/100], Step [107/493], Loss: 0.0023\n",
            "Epoch [99/100], Step [108/493], Loss: 0.0097\n",
            "Epoch [99/100], Step [109/493], Loss: 0.0123\n",
            "Epoch [99/100], Step [110/493], Loss: 0.0172\n",
            "Epoch [99/100], Step [111/493], Loss: 0.0011\n",
            "Epoch [99/100], Step [112/493], Loss: 0.0393\n",
            "Epoch [99/100], Step [113/493], Loss: 0.0103\n",
            "Epoch [99/100], Step [114/493], Loss: 0.0086\n",
            "Epoch [99/100], Step [115/493], Loss: 0.0073\n",
            "Epoch [99/100], Step [116/493], Loss: 0.0019\n",
            "Epoch [99/100], Step [117/493], Loss: 0.0062\n",
            "Epoch [99/100], Step [118/493], Loss: 0.0056\n",
            "Epoch [99/100], Step [119/493], Loss: 0.0086\n",
            "Epoch [99/100], Step [120/493], Loss: 0.0074\n",
            "Epoch [99/100], Step [121/493], Loss: 0.0330\n",
            "Epoch [99/100], Step [122/493], Loss: 0.0070\n",
            "Epoch [99/100], Step [123/493], Loss: 0.0340\n",
            "Epoch [99/100], Step [124/493], Loss: 0.0077\n",
            "Epoch [99/100], Step [125/493], Loss: 0.0033\n",
            "Epoch [99/100], Step [126/493], Loss: 0.0131\n",
            "Epoch [99/100], Step [127/493], Loss: 0.0062\n",
            "Epoch [99/100], Step [128/493], Loss: 0.0100\n",
            "Epoch [99/100], Step [129/493], Loss: 0.2699\n",
            "Epoch [99/100], Step [130/493], Loss: 0.0087\n",
            "Epoch [99/100], Step [131/493], Loss: 0.0015\n",
            "Epoch [99/100], Step [132/493], Loss: 0.0240\n",
            "Epoch [99/100], Step [133/493], Loss: 0.0005\n",
            "Epoch [99/100], Step [134/493], Loss: 0.0344\n",
            "Epoch [99/100], Step [135/493], Loss: 0.0305\n",
            "Epoch [99/100], Step [136/493], Loss: 0.0029\n",
            "Epoch [99/100], Step [137/493], Loss: 0.0180\n",
            "Epoch [99/100], Step [138/493], Loss: 0.0083\n",
            "Epoch [99/100], Step [139/493], Loss: 0.0124\n",
            "Epoch [99/100], Step [140/493], Loss: 0.1790\n",
            "Epoch [99/100], Step [141/493], Loss: 0.0053\n",
            "Epoch [99/100], Step [142/493], Loss: 0.0123\n",
            "Epoch [99/100], Step [143/493], Loss: 0.0094\n",
            "Epoch [99/100], Step [144/493], Loss: 0.0131\n",
            "Epoch [99/100], Step [145/493], Loss: 0.2338\n",
            "Epoch [99/100], Step [146/493], Loss: 0.0078\n",
            "Epoch [99/100], Step [147/493], Loss: 0.1099\n",
            "Epoch [99/100], Step [148/493], Loss: 0.0076\n",
            "Epoch [99/100], Step [149/493], Loss: 0.0003\n",
            "Epoch [99/100], Step [150/493], Loss: 0.0228\n",
            "Epoch [99/100], Step [151/493], Loss: 0.0298\n",
            "Epoch [99/100], Step [152/493], Loss: 0.0001\n",
            "Epoch [99/100], Step [153/493], Loss: 0.0088\n",
            "Epoch [99/100], Step [154/493], Loss: 0.0116\n",
            "Epoch [99/100], Step [155/493], Loss: 0.0031\n",
            "Epoch [99/100], Step [156/493], Loss: 0.0025\n",
            "Epoch [99/100], Step [157/493], Loss: 0.0096\n",
            "Epoch [99/100], Step [158/493], Loss: 0.0412\n",
            "Epoch [99/100], Step [159/493], Loss: 0.0041\n",
            "Epoch [99/100], Step [160/493], Loss: 0.0084\n",
            "Epoch [99/100], Step [161/493], Loss: 0.1317\n",
            "Epoch [99/100], Step [162/493], Loss: 0.0143\n",
            "Epoch [99/100], Step [163/493], Loss: 0.0167\n",
            "Epoch [99/100], Step [164/493], Loss: 0.0066\n",
            "Epoch [99/100], Step [165/493], Loss: 0.0116\n",
            "Epoch [99/100], Step [166/493], Loss: 0.1683\n",
            "Epoch [99/100], Step [167/493], Loss: 0.0046\n",
            "Epoch [99/100], Step [168/493], Loss: 0.0295\n",
            "Epoch [99/100], Step [169/493], Loss: 0.0191\n",
            "Epoch [99/100], Step [170/493], Loss: 0.0047\n",
            "Epoch [99/100], Step [171/493], Loss: 0.0014\n",
            "Epoch [99/100], Step [172/493], Loss: 0.0136\n",
            "Epoch [99/100], Step [173/493], Loss: 0.0166\n",
            "Epoch [99/100], Step [174/493], Loss: 0.0045\n",
            "Epoch [99/100], Step [175/493], Loss: 0.0050\n",
            "Epoch [99/100], Step [176/493], Loss: 0.0139\n",
            "Epoch [99/100], Step [177/493], Loss: 0.0202\n",
            "Epoch [99/100], Step [178/493], Loss: 0.0014\n",
            "Epoch [99/100], Step [179/493], Loss: 0.0045\n",
            "Epoch [99/100], Step [180/493], Loss: 0.0029\n",
            "Epoch [99/100], Step [181/493], Loss: 0.0131\n",
            "Epoch [99/100], Step [182/493], Loss: 0.0132\n",
            "Epoch [99/100], Step [183/493], Loss: 0.0040\n",
            "Epoch [99/100], Step [184/493], Loss: 0.0097\n",
            "Epoch [99/100], Step [185/493], Loss: 0.0058\n",
            "Epoch [99/100], Step [186/493], Loss: 0.0136\n",
            "Epoch [99/100], Step [187/493], Loss: 0.0147\n",
            "Epoch [99/100], Step [188/493], Loss: 0.0163\n",
            "Epoch [99/100], Step [189/493], Loss: 0.0084\n",
            "Epoch [99/100], Step [190/493], Loss: 0.0252\n",
            "Epoch [99/100], Step [191/493], Loss: 0.0096\n",
            "Epoch [99/100], Step [192/493], Loss: 0.0138\n",
            "Epoch [99/100], Step [193/493], Loss: 0.0770\n",
            "Epoch [99/100], Step [194/493], Loss: 0.0209\n",
            "Epoch [99/100], Step [195/493], Loss: 0.5350\n",
            "Epoch [99/100], Step [196/493], Loss: 0.0128\n",
            "Epoch [99/100], Step [197/493], Loss: 0.0044\n",
            "Epoch [99/100], Step [198/493], Loss: 0.0154\n",
            "Epoch [99/100], Step [199/493], Loss: 0.0070\n",
            "Epoch [99/100], Step [200/493], Loss: 0.0094\n",
            "Epoch [99/100], Step [201/493], Loss: 0.0099\n",
            "Epoch [99/100], Step [202/493], Loss: 0.0268\n",
            "Epoch [99/100], Step [203/493], Loss: 0.0413\n",
            "Epoch [99/100], Step [204/493], Loss: 0.0129\n",
            "Epoch [99/100], Step [205/493], Loss: 0.0195\n",
            "Epoch [99/100], Step [206/493], Loss: 0.0264\n",
            "Epoch [99/100], Step [207/493], Loss: 0.0074\n",
            "Epoch [99/100], Step [208/493], Loss: 0.0140\n",
            "Epoch [99/100], Step [209/493], Loss: 0.0114\n",
            "Epoch [99/100], Step [210/493], Loss: 0.0109\n",
            "Epoch [99/100], Step [211/493], Loss: 0.0140\n",
            "Epoch [99/100], Step [212/493], Loss: 0.0253\n",
            "Epoch [99/100], Step [213/493], Loss: 0.0022\n",
            "Epoch [99/100], Step [214/493], Loss: 0.0330\n",
            "Epoch [99/100], Step [215/493], Loss: 0.0838\n",
            "Epoch [99/100], Step [216/493], Loss: 0.0657\n",
            "Epoch [99/100], Step [217/493], Loss: 0.0686\n",
            "Epoch [99/100], Step [218/493], Loss: 0.0093\n",
            "Epoch [99/100], Step [219/493], Loss: 0.0052\n",
            "Epoch [99/100], Step [220/493], Loss: 0.0027\n",
            "Epoch [99/100], Step [221/493], Loss: 0.0144\n",
            "Epoch [99/100], Step [222/493], Loss: 0.0023\n",
            "Epoch [99/100], Step [223/493], Loss: 0.0129\n",
            "Epoch [99/100], Step [224/493], Loss: 0.0141\n",
            "Epoch [99/100], Step [225/493], Loss: 0.0022\n",
            "Epoch [99/100], Step [226/493], Loss: 0.0067\n",
            "Epoch [99/100], Step [227/493], Loss: 0.0268\n",
            "Epoch [99/100], Step [228/493], Loss: 0.0230\n",
            "Epoch [99/100], Step [229/493], Loss: 0.0059\n",
            "Epoch [99/100], Step [230/493], Loss: 0.0685\n",
            "Epoch [99/100], Step [231/493], Loss: 0.0061\n",
            "Epoch [99/100], Step [232/493], Loss: 0.0258\n",
            "Epoch [99/100], Step [233/493], Loss: 0.0035\n",
            "Epoch [99/100], Step [234/493], Loss: 0.0079\n",
            "Epoch [99/100], Step [235/493], Loss: 0.0102\n",
            "Epoch [99/100], Step [236/493], Loss: 0.0054\n",
            "Epoch [99/100], Step [237/493], Loss: 0.0073\n",
            "Epoch [99/100], Step [238/493], Loss: 0.0121\n",
            "Epoch [99/100], Step [239/493], Loss: 0.0300\n",
            "Epoch [99/100], Step [240/493], Loss: 0.0032\n",
            "Epoch [99/100], Step [241/493], Loss: 0.0059\n",
            "Epoch [99/100], Step [242/493], Loss: 0.0077\n",
            "Epoch [99/100], Step [243/493], Loss: 0.0138\n",
            "Epoch [99/100], Step [244/493], Loss: 0.0109\n",
            "Epoch [99/100], Step [245/493], Loss: 0.0227\n",
            "Epoch [99/100], Step [246/493], Loss: 0.0043\n",
            "Epoch [99/100], Step [247/493], Loss: 0.0043\n",
            "Epoch [99/100], Step [248/493], Loss: 0.0066\n",
            "Epoch [99/100], Step [249/493], Loss: 0.0264\n",
            "Epoch [99/100], Step [250/493], Loss: 0.0131\n",
            "Epoch [99/100], Step [251/493], Loss: 0.0055\n",
            "Epoch [99/100], Step [252/493], Loss: 0.0086\n",
            "Epoch [99/100], Step [253/493], Loss: 0.0833\n",
            "Epoch [99/100], Step [254/493], Loss: 0.0041\n",
            "Epoch [99/100], Step [255/493], Loss: 0.0027\n",
            "Epoch [99/100], Step [256/493], Loss: 0.0424\n",
            "Epoch [99/100], Step [257/493], Loss: 0.0118\n",
            "Epoch [99/100], Step [258/493], Loss: 0.0339\n",
            "Epoch [99/100], Step [259/493], Loss: 0.0021\n",
            "Epoch [99/100], Step [260/493], Loss: 0.0058\n",
            "Epoch [99/100], Step [261/493], Loss: 0.0081\n",
            "Epoch [99/100], Step [262/493], Loss: 0.0036\n",
            "Epoch [99/100], Step [263/493], Loss: 0.0051\n",
            "Epoch [99/100], Step [264/493], Loss: 0.0092\n",
            "Epoch [99/100], Step [265/493], Loss: 0.0044\n",
            "Epoch [99/100], Step [266/493], Loss: 0.0131\n",
            "Epoch [99/100], Step [267/493], Loss: 0.0085\n",
            "Epoch [99/100], Step [268/493], Loss: 0.0072\n",
            "Epoch [99/100], Step [269/493], Loss: 0.0097\n",
            "Epoch [99/100], Step [270/493], Loss: 0.0068\n",
            "Epoch [99/100], Step [271/493], Loss: 0.0250\n",
            "Epoch [99/100], Step [272/493], Loss: 0.0069\n",
            "Epoch [99/100], Step [273/493], Loss: 0.0060\n",
            "Epoch [99/100], Step [274/493], Loss: 0.0403\n",
            "Epoch [99/100], Step [275/493], Loss: 0.0277\n",
            "Epoch [99/100], Step [276/493], Loss: 0.0092\n",
            "Epoch [99/100], Step [277/493], Loss: 0.0080\n",
            "Epoch [99/100], Step [278/493], Loss: 0.0063\n",
            "Epoch [99/100], Step [279/493], Loss: 0.0117\n",
            "Epoch [99/100], Step [280/493], Loss: 0.0077\n",
            "Epoch [99/100], Step [281/493], Loss: 0.0072\n",
            "Epoch [99/100], Step [282/493], Loss: 0.0017\n",
            "Epoch [99/100], Step [283/493], Loss: 0.0597\n",
            "Epoch [99/100], Step [284/493], Loss: 0.0089\n",
            "Epoch [99/100], Step [285/493], Loss: 0.0084\n",
            "Epoch [99/100], Step [286/493], Loss: 0.0095\n",
            "Epoch [99/100], Step [287/493], Loss: 0.0168\n",
            "Epoch [99/100], Step [288/493], Loss: 0.0032\n",
            "Epoch [99/100], Step [289/493], Loss: 0.0051\n",
            "Epoch [99/100], Step [290/493], Loss: 0.0068\n",
            "Epoch [99/100], Step [291/493], Loss: 0.0043\n",
            "Epoch [99/100], Step [292/493], Loss: 0.0112\n",
            "Epoch [99/100], Step [293/493], Loss: 0.0472\n",
            "Epoch [99/100], Step [294/493], Loss: 0.0070\n",
            "Epoch [99/100], Step [295/493], Loss: 0.0039\n",
            "Epoch [99/100], Step [296/493], Loss: 0.0081\n",
            "Epoch [99/100], Step [297/493], Loss: 0.0048\n",
            "Epoch [99/100], Step [298/493], Loss: 0.0117\n",
            "Epoch [99/100], Step [299/493], Loss: 0.0018\n",
            "Epoch [99/100], Step [300/493], Loss: 0.0105\n",
            "Epoch [99/100], Step [301/493], Loss: 0.0065\n",
            "Epoch [99/100], Step [302/493], Loss: 0.0253\n",
            "Epoch [99/100], Step [303/493], Loss: 0.0011\n",
            "Epoch [99/100], Step [304/493], Loss: 0.0007\n",
            "Epoch [99/100], Step [305/493], Loss: 0.0022\n",
            "Epoch [99/100], Step [306/493], Loss: 0.0045\n",
            "Epoch [99/100], Step [307/493], Loss: 0.0048\n",
            "Epoch [99/100], Step [308/493], Loss: 0.0002\n",
            "Epoch [99/100], Step [309/493], Loss: 0.0049\n",
            "Epoch [99/100], Step [310/493], Loss: 0.0145\n",
            "Epoch [99/100], Step [311/493], Loss: 0.0360\n",
            "Epoch [99/100], Step [312/493], Loss: 0.1575\n",
            "Epoch [99/100], Step [313/493], Loss: 0.4792\n",
            "Epoch [99/100], Step [314/493], Loss: 0.0098\n",
            "Epoch [99/100], Step [315/493], Loss: 0.0264\n",
            "Epoch [99/100], Step [316/493], Loss: 0.0130\n",
            "Epoch [99/100], Step [317/493], Loss: 0.0032\n",
            "Epoch [99/100], Step [318/493], Loss: 0.0456\n",
            "Epoch [99/100], Step [319/493], Loss: 0.0078\n",
            "Epoch [99/100], Step [320/493], Loss: 0.0026\n",
            "Epoch [99/100], Step [321/493], Loss: 0.0057\n",
            "Epoch [99/100], Step [322/493], Loss: 0.0030\n",
            "Epoch [99/100], Step [323/493], Loss: 0.0066\n",
            "Epoch [99/100], Step [324/493], Loss: 0.0060\n",
            "Epoch [99/100], Step [325/493], Loss: 0.0305\n",
            "Epoch [99/100], Step [326/493], Loss: 0.0081\n",
            "Epoch [99/100], Step [327/493], Loss: 0.0277\n",
            "Epoch [99/100], Step [328/493], Loss: 0.0174\n",
            "Epoch [99/100], Step [329/493], Loss: 0.0100\n",
            "Epoch [99/100], Step [330/493], Loss: 0.0047\n",
            "Epoch [99/100], Step [331/493], Loss: 0.0092\n",
            "Epoch [99/100], Step [332/493], Loss: 0.0062\n",
            "Epoch [99/100], Step [333/493], Loss: 0.0050\n",
            "Epoch [99/100], Step [334/493], Loss: 0.1181\n",
            "Epoch [99/100], Step [335/493], Loss: 0.0029\n",
            "Epoch [99/100], Step [336/493], Loss: 0.1024\n",
            "Epoch [99/100], Step [337/493], Loss: 0.0059\n",
            "Epoch [99/100], Step [338/493], Loss: 0.1087\n",
            "Epoch [99/100], Step [339/493], Loss: 0.0870\n",
            "Epoch [99/100], Step [340/493], Loss: 0.0042\n",
            "Epoch [99/100], Step [341/493], Loss: 0.0067\n",
            "Epoch [99/100], Step [342/493], Loss: 0.0132\n",
            "Epoch [99/100], Step [343/493], Loss: 0.0057\n",
            "Epoch [99/100], Step [344/493], Loss: 0.0012\n",
            "Epoch [99/100], Step [345/493], Loss: 0.0049\n",
            "Epoch [99/100], Step [346/493], Loss: 0.0872\n",
            "Epoch [99/100], Step [347/493], Loss: 0.0019\n",
            "Epoch [99/100], Step [348/493], Loss: 0.0029\n",
            "Epoch [99/100], Step [349/493], Loss: 0.0039\n",
            "Epoch [99/100], Step [350/493], Loss: 0.0051\n",
            "Epoch [99/100], Step [351/493], Loss: 0.0011\n",
            "Epoch [99/100], Step [352/493], Loss: 0.0124\n",
            "Epoch [99/100], Step [353/493], Loss: 0.0029\n",
            "Epoch [99/100], Step [354/493], Loss: 0.0039\n",
            "Epoch [99/100], Step [355/493], Loss: 0.0123\n",
            "Epoch [99/100], Step [356/493], Loss: 0.0030\n",
            "Epoch [99/100], Step [357/493], Loss: 0.0289\n",
            "Epoch [99/100], Step [358/493], Loss: 0.0108\n",
            "Epoch [99/100], Step [359/493], Loss: 0.0034\n",
            "Epoch [99/100], Step [360/493], Loss: 0.0110\n",
            "Epoch [99/100], Step [361/493], Loss: 0.0097\n",
            "Epoch [99/100], Step [362/493], Loss: 0.0016\n",
            "Epoch [99/100], Step [363/493], Loss: 0.0077\n",
            "Epoch [99/100], Step [364/493], Loss: 0.0325\n",
            "Epoch [99/100], Step [365/493], Loss: 0.0059\n",
            "Epoch [99/100], Step [366/493], Loss: 0.0095\n",
            "Epoch [99/100], Step [367/493], Loss: 0.0103\n",
            "Epoch [99/100], Step [368/493], Loss: 0.0141\n",
            "Epoch [99/100], Step [369/493], Loss: 0.0065\n",
            "Epoch [99/100], Step [370/493], Loss: 0.0092\n",
            "Epoch [99/100], Step [371/493], Loss: 0.0079\n",
            "Epoch [99/100], Step [372/493], Loss: 0.0037\n",
            "Epoch [99/100], Step [373/493], Loss: 0.0096\n",
            "Epoch [99/100], Step [374/493], Loss: 0.0244\n",
            "Epoch [99/100], Step [375/493], Loss: 0.0042\n",
            "Epoch [99/100], Step [376/493], Loss: 0.0210\n",
            "Epoch [99/100], Step [377/493], Loss: 0.0077\n",
            "Epoch [99/100], Step [378/493], Loss: 0.0169\n",
            "Epoch [99/100], Step [379/493], Loss: 0.0284\n",
            "Epoch [99/100], Step [380/493], Loss: 0.1114\n",
            "Epoch [99/100], Step [381/493], Loss: 0.0255\n",
            "Epoch [99/100], Step [382/493], Loss: 0.0290\n",
            "Epoch [99/100], Step [383/493], Loss: 0.0229\n",
            "Epoch [99/100], Step [384/493], Loss: 0.0003\n",
            "Epoch [99/100], Step [385/493], Loss: 0.2259\n",
            "Epoch [99/100], Step [386/493], Loss: 0.0147\n",
            "Epoch [99/100], Step [387/493], Loss: 0.0097\n",
            "Epoch [99/100], Step [388/493], Loss: 0.0035\n",
            "Epoch [99/100], Step [389/493], Loss: 0.0063\n",
            "Epoch [99/100], Step [390/493], Loss: 0.0075\n",
            "Epoch [99/100], Step [391/493], Loss: 0.0144\n",
            "Epoch [99/100], Step [392/493], Loss: 0.0175\n",
            "Epoch [99/100], Step [393/493], Loss: 0.0054\n",
            "Epoch [99/100], Step [394/493], Loss: 0.0033\n",
            "Epoch [99/100], Step [395/493], Loss: 0.0067\n",
            "Epoch [99/100], Step [396/493], Loss: 0.0153\n",
            "Epoch [99/100], Step [397/493], Loss: 0.0060\n",
            "Epoch [99/100], Step [398/493], Loss: 0.0445\n",
            "Epoch [99/100], Step [399/493], Loss: 0.0042\n",
            "Epoch [99/100], Step [400/493], Loss: 0.0083\n",
            "Epoch [99/100], Step [401/493], Loss: 0.0308\n",
            "Epoch [99/100], Step [402/493], Loss: 0.0391\n",
            "Epoch [99/100], Step [403/493], Loss: 0.0100\n",
            "Epoch [99/100], Step [404/493], Loss: 0.0029\n",
            "Epoch [99/100], Step [405/493], Loss: 0.1857\n",
            "Epoch [99/100], Step [406/493], Loss: 0.0084\n",
            "Epoch [99/100], Step [407/493], Loss: 0.0073\n",
            "Epoch [99/100], Step [408/493], Loss: 0.0280\n",
            "Epoch [99/100], Step [409/493], Loss: 0.1910\n",
            "Epoch [99/100], Step [410/493], Loss: 0.0033\n",
            "Epoch [99/100], Step [411/493], Loss: 0.0382\n",
            "Epoch [99/100], Step [412/493], Loss: 0.0035\n",
            "Epoch [99/100], Step [413/493], Loss: 0.0040\n",
            "Epoch [99/100], Step [414/493], Loss: 0.0434\n",
            "Epoch [99/100], Step [415/493], Loss: 0.0330\n",
            "Epoch [99/100], Step [416/493], Loss: 0.0081\n",
            "Epoch [99/100], Step [417/493], Loss: 0.0109\n",
            "Epoch [99/100], Step [418/493], Loss: 0.0073\n",
            "Epoch [99/100], Step [419/493], Loss: 0.0071\n",
            "Epoch [99/100], Step [420/493], Loss: 0.0141\n",
            "Epoch [99/100], Step [421/493], Loss: 0.0098\n",
            "Epoch [99/100], Step [422/493], Loss: 0.0042\n",
            "Epoch [99/100], Step [423/493], Loss: 0.0105\n",
            "Epoch [99/100], Step [424/493], Loss: 0.0100\n",
            "Epoch [99/100], Step [425/493], Loss: 0.0059\n",
            "Epoch [99/100], Step [426/493], Loss: 0.0045\n",
            "Epoch [99/100], Step [427/493], Loss: 0.0043\n",
            "Epoch [99/100], Step [428/493], Loss: 0.1160\n",
            "Epoch [99/100], Step [429/493], Loss: 0.0047\n",
            "Epoch [99/100], Step [430/493], Loss: 0.0097\n",
            "Epoch [99/100], Step [431/493], Loss: 0.0023\n",
            "Epoch [99/100], Step [432/493], Loss: 0.0022\n",
            "Epoch [99/100], Step [433/493], Loss: 0.0158\n",
            "Epoch [99/100], Step [434/493], Loss: 0.0092\n",
            "Epoch [99/100], Step [435/493], Loss: 0.0208\n",
            "Epoch [99/100], Step [436/493], Loss: 0.0263\n",
            "Epoch [99/100], Step [437/493], Loss: 0.0059\n",
            "Epoch [99/100], Step [438/493], Loss: 0.0064\n",
            "Epoch [99/100], Step [439/493], Loss: 0.0104\n",
            "Epoch [99/100], Step [440/493], Loss: 0.0578\n",
            "Epoch [99/100], Step [441/493], Loss: 0.0031\n",
            "Epoch [99/100], Step [442/493], Loss: 0.0081\n",
            "Epoch [99/100], Step [443/493], Loss: 0.0040\n",
            "Epoch [99/100], Step [444/493], Loss: 0.0175\n",
            "Epoch [99/100], Step [445/493], Loss: 0.1971\n",
            "Epoch [99/100], Step [446/493], Loss: 0.0054\n",
            "Epoch [99/100], Step [447/493], Loss: 0.0074\n",
            "Epoch [99/100], Step [448/493], Loss: 0.0080\n",
            "Epoch [99/100], Step [449/493], Loss: 0.0078\n",
            "Epoch [99/100], Step [450/493], Loss: 0.0035\n",
            "Epoch [99/100], Step [451/493], Loss: 0.0059\n",
            "Epoch [99/100], Step [452/493], Loss: 0.0067\n",
            "Epoch [99/100], Step [453/493], Loss: 0.0087\n",
            "Epoch [99/100], Step [454/493], Loss: 0.0065\n",
            "Epoch [99/100], Step [455/493], Loss: 0.0033\n",
            "Epoch [99/100], Step [456/493], Loss: 0.0145\n",
            "Epoch [99/100], Step [457/493], Loss: 0.3149\n",
            "Epoch [99/100], Step [458/493], Loss: 0.0117\n",
            "Epoch [99/100], Step [459/493], Loss: 0.0080\n",
            "Epoch [99/100], Step [460/493], Loss: 0.0121\n",
            "Epoch [99/100], Step [461/493], Loss: 0.0040\n",
            "Epoch [99/100], Step [462/493], Loss: 0.0070\n",
            "Epoch [99/100], Step [463/493], Loss: 0.0149\n",
            "Epoch [99/100], Step [464/493], Loss: 0.0024\n",
            "Epoch [99/100], Step [465/493], Loss: 0.0074\n",
            "Epoch [99/100], Step [466/493], Loss: 0.0057\n",
            "Epoch [99/100], Step [467/493], Loss: 0.0253\n",
            "Epoch [99/100], Step [468/493], Loss: 0.0097\n",
            "Epoch [99/100], Step [469/493], Loss: 0.2286\n",
            "Epoch [99/100], Step [470/493], Loss: 0.0083\n",
            "Epoch [99/100], Step [471/493], Loss: 0.0014\n",
            "Epoch [99/100], Step [472/493], Loss: 0.0101\n",
            "Epoch [99/100], Step [473/493], Loss: 0.0054\n",
            "Epoch [99/100], Step [474/493], Loss: 0.0074\n",
            "Epoch [99/100], Step [475/493], Loss: 0.0110\n",
            "Epoch [99/100], Step [476/493], Loss: 0.0072\n",
            "Epoch [99/100], Step [477/493], Loss: 0.0060\n",
            "Epoch [99/100], Step [478/493], Loss: 0.0194\n",
            "Epoch [99/100], Step [479/493], Loss: 0.0074\n",
            "Epoch [99/100], Step [480/493], Loss: 0.0109\n",
            "Epoch [99/100], Step [481/493], Loss: 0.0077\n",
            "Epoch [99/100], Step [482/493], Loss: 0.0128\n",
            "Epoch [99/100], Step [483/493], Loss: 0.0140\n",
            "Epoch [99/100], Step [484/493], Loss: 0.0071\n",
            "Epoch [99/100], Step [485/493], Loss: 0.0055\n",
            "Epoch [99/100], Step [486/493], Loss: 0.0031\n",
            "Epoch [99/100], Step [487/493], Loss: 0.0018\n",
            "Epoch [99/100], Step [488/493], Loss: 0.0156\n",
            "Epoch [99/100], Step [489/493], Loss: 0.0068\n",
            "Epoch [99/100], Step [490/493], Loss: 0.2984\n",
            "Epoch [99/100], Step [491/493], Loss: 0.0005\n",
            "Epoch [99/100], Step [492/493], Loss: 0.0079\n",
            "Epoch [99/100], Step [493/493], Loss: 0.0206\n",
            "Epoch [100/100], Step [1/493], Loss: 0.0024\n",
            "Epoch [100/100], Step [2/493], Loss: 0.0037\n",
            "Epoch [100/100], Step [3/493], Loss: 0.0071\n",
            "Epoch [100/100], Step [4/493], Loss: 0.0032\n",
            "Epoch [100/100], Step [5/493], Loss: 0.0770\n",
            "Epoch [100/100], Step [6/493], Loss: 0.0044\n",
            "Epoch [100/100], Step [7/493], Loss: 0.0169\n",
            "Epoch [100/100], Step [8/493], Loss: 0.0298\n",
            "Epoch [100/100], Step [9/493], Loss: 0.0065\n",
            "Epoch [100/100], Step [10/493], Loss: 0.0145\n",
            "Epoch [100/100], Step [11/493], Loss: 0.0042\n",
            "Epoch [100/100], Step [12/493], Loss: 0.0172\n",
            "Epoch [100/100], Step [13/493], Loss: 0.0305\n",
            "Epoch [100/100], Step [14/493], Loss: 0.0092\n",
            "Epoch [100/100], Step [15/493], Loss: 0.0179\n",
            "Epoch [100/100], Step [16/493], Loss: 0.4792\n",
            "Epoch [100/100], Step [17/493], Loss: 0.0838\n",
            "Epoch [100/100], Step [18/493], Loss: 0.0071\n",
            "Epoch [100/100], Step [19/493], Loss: 0.0093\n",
            "Epoch [100/100], Step [20/493], Loss: 0.0105\n",
            "Epoch [100/100], Step [21/493], Loss: 0.0010\n",
            "Epoch [100/100], Step [22/493], Loss: 0.0023\n",
            "Epoch [100/100], Step [23/493], Loss: 0.0057\n",
            "Epoch [100/100], Step [24/493], Loss: 0.0031\n",
            "Epoch [100/100], Step [25/493], Loss: 0.0043\n",
            "Epoch [100/100], Step [26/493], Loss: 0.0263\n",
            "Epoch [100/100], Step [27/493], Loss: 0.0685\n",
            "Epoch [100/100], Step [28/493], Loss: 0.0076\n",
            "Epoch [100/100], Step [29/493], Loss: 0.0305\n",
            "Epoch [100/100], Step [30/493], Loss: 0.0308\n",
            "Epoch [100/100], Step [31/493], Loss: 0.0032\n",
            "Epoch [100/100], Step [32/493], Loss: 0.0277\n",
            "Epoch [100/100], Step [33/493], Loss: 0.0092\n",
            "Epoch [100/100], Step [34/493], Loss: 0.0049\n",
            "Epoch [100/100], Step [35/493], Loss: 0.0022\n",
            "Epoch [100/100], Step [36/493], Loss: 0.2135\n",
            "Epoch [100/100], Step [37/493], Loss: 0.0058\n",
            "Epoch [100/100], Step [38/493], Loss: 0.0029\n",
            "Epoch [100/100], Step [39/493], Loss: 0.0097\n",
            "Epoch [100/100], Step [40/493], Loss: 0.0042\n",
            "Epoch [100/100], Step [41/493], Loss: 0.0078\n",
            "Epoch [100/100], Step [42/493], Loss: 0.0058\n",
            "Epoch [100/100], Step [43/493], Loss: 0.0034\n",
            "Epoch [100/100], Step [44/493], Loss: 0.1575\n",
            "Epoch [100/100], Step [45/493], Loss: 0.0015\n",
            "Epoch [100/100], Step [46/493], Loss: 0.0123\n",
            "Epoch [100/100], Step [47/493], Loss: 0.0075\n",
            "Epoch [100/100], Step [48/493], Loss: 0.0078\n",
            "Epoch [100/100], Step [49/493], Loss: 0.0339\n",
            "Epoch [100/100], Step [50/493], Loss: 0.0068\n",
            "Epoch [100/100], Step [51/493], Loss: 0.0095\n",
            "Epoch [100/100], Step [52/493], Loss: 0.0074\n",
            "Epoch [100/100], Step [53/493], Loss: 0.0070\n",
            "Epoch [100/100], Step [54/493], Loss: 0.0268\n",
            "Epoch [100/100], Step [55/493], Loss: 0.0131\n",
            "Epoch [100/100], Step [56/493], Loss: 0.0080\n",
            "Epoch [100/100], Step [57/493], Loss: 0.0057\n",
            "Epoch [100/100], Step [58/493], Loss: 0.0086\n",
            "Epoch [100/100], Step [59/493], Loss: 0.0100\n",
            "Epoch [100/100], Step [60/493], Loss: 0.0023\n",
            "Epoch [100/100], Step [61/493], Loss: 0.0041\n",
            "Epoch [100/100], Step [62/493], Loss: 0.0016\n",
            "Epoch [100/100], Step [63/493], Loss: 0.0059\n",
            "Epoch [100/100], Step [64/493], Loss: 0.1317\n",
            "Epoch [100/100], Step [65/493], Loss: 0.0099\n",
            "Epoch [100/100], Step [66/493], Loss: 0.0005\n",
            "Epoch [100/100], Step [67/493], Loss: 0.0063\n",
            "Epoch [100/100], Step [68/493], Loss: 0.0412\n",
            "Epoch [100/100], Step [69/493], Loss: 0.0014\n",
            "Epoch [100/100], Step [70/493], Loss: 0.0140\n",
            "Epoch [100/100], Step [71/493], Loss: 0.0163\n",
            "Epoch [100/100], Step [72/493], Loss: 0.0097\n",
            "Epoch [100/100], Step [73/493], Loss: 0.0253\n",
            "Epoch [100/100], Step [74/493], Loss: 0.0059\n",
            "Epoch [100/100], Step [75/493], Loss: 0.0055\n",
            "Epoch [100/100], Step [76/493], Loss: 0.0033\n",
            "Epoch [100/100], Step [77/493], Loss: 0.0117\n",
            "Epoch [100/100], Step [78/493], Loss: 0.0054\n",
            "Epoch [100/100], Step [79/493], Loss: 0.0081\n",
            "Epoch [100/100], Step [80/493], Loss: 0.0048\n",
            "Epoch [100/100], Step [81/493], Loss: 0.0079\n",
            "Epoch [100/100], Step [82/493], Loss: 0.0070\n",
            "Epoch [100/100], Step [83/493], Loss: 0.0092\n",
            "Epoch [100/100], Step [84/493], Loss: 0.0029\n",
            "Epoch [100/100], Step [85/493], Loss: 0.0029\n",
            "Epoch [100/100], Step [86/493], Loss: 0.0077\n",
            "Epoch [100/100], Step [87/493], Loss: 0.0109\n",
            "Epoch [100/100], Step [88/493], Loss: 0.0043\n",
            "Epoch [100/100], Step [89/493], Loss: 0.0040\n",
            "Epoch [100/100], Step [90/493], Loss: 0.0080\n",
            "Epoch [100/100], Step [91/493], Loss: 0.0200\n",
            "Epoch [100/100], Step [92/493], Loss: 0.1286\n",
            "Epoch [100/100], Step [93/493], Loss: 0.0003\n",
            "Epoch [100/100], Step [94/493], Loss: 0.0015\n",
            "Epoch [100/100], Step [95/493], Loss: 0.0087\n",
            "Epoch [100/100], Step [96/493], Loss: 0.0037\n",
            "Epoch [100/100], Step [97/493], Loss: 0.0191\n",
            "Epoch [100/100], Step [98/493], Loss: 0.0403\n",
            "Epoch [100/100], Step [99/493], Loss: 0.0051\n",
            "Epoch [100/100], Step [100/493], Loss: 0.0072\n",
            "Epoch [100/100], Step [101/493], Loss: 0.0051\n",
            "Epoch [100/100], Step [102/493], Loss: 0.0133\n",
            "Epoch [100/100], Step [103/493], Loss: 0.0382\n",
            "Epoch [100/100], Step [104/493], Loss: 0.0057\n",
            "Epoch [100/100], Step [105/493], Loss: 0.0133\n",
            "Epoch [100/100], Step [106/493], Loss: 0.0050\n",
            "Epoch [100/100], Step [107/493], Loss: 0.0118\n",
            "Epoch [100/100], Step [108/493], Loss: 0.0071\n",
            "Epoch [100/100], Step [109/493], Loss: 0.0117\n",
            "Epoch [100/100], Step [110/493], Loss: 0.0253\n",
            "Epoch [100/100], Step [111/493], Loss: 0.0110\n",
            "Epoch [100/100], Step [112/493], Loss: 0.0040\n",
            "Epoch [100/100], Step [113/493], Loss: 0.0087\n",
            "Epoch [100/100], Step [114/493], Loss: 0.0011\n",
            "Epoch [100/100], Step [115/493], Loss: 0.0295\n",
            "Epoch [100/100], Step [116/493], Loss: 0.0081\n",
            "Epoch [100/100], Step [117/493], Loss: 0.0047\n",
            "Epoch [100/100], Step [118/493], Loss: 0.0456\n",
            "Epoch [100/100], Step [119/493], Loss: 0.0047\n",
            "Epoch [100/100], Step [120/493], Loss: 0.0124\n",
            "Epoch [100/100], Step [121/493], Loss: 0.0081\n",
            "Epoch [100/100], Step [122/493], Loss: 0.0011\n",
            "Epoch [100/100], Step [123/493], Loss: 0.0035\n",
            "Epoch [100/100], Step [124/493], Loss: 0.0020\n",
            "Epoch [100/100], Step [125/493], Loss: 0.0230\n",
            "Epoch [100/100], Step [126/493], Loss: 0.0147\n",
            "Epoch [100/100], Step [127/493], Loss: 0.0225\n",
            "Epoch [100/100], Step [128/493], Loss: 0.0074\n",
            "Epoch [100/100], Step [129/493], Loss: 0.0039\n",
            "Epoch [100/100], Step [130/493], Loss: 0.0167\n",
            "Epoch [100/100], Step [131/493], Loss: 0.0132\n",
            "Epoch [100/100], Step [132/493], Loss: 0.0098\n",
            "Epoch [100/100], Step [133/493], Loss: 0.0101\n",
            "Epoch [100/100], Step [134/493], Loss: 0.0094\n",
            "Epoch [100/100], Step [135/493], Loss: 0.0280\n",
            "Epoch [100/100], Step [136/493], Loss: 0.0072\n",
            "Epoch [100/100], Step [137/493], Loss: 0.0073\n",
            "Epoch [100/100], Step [138/493], Loss: 0.0061\n",
            "Epoch [100/100], Step [139/493], Loss: 0.0069\n",
            "Epoch [100/100], Step [140/493], Loss: 0.1790\n",
            "Epoch [100/100], Step [141/493], Loss: 0.0073\n",
            "Epoch [100/100], Step [142/493], Loss: 0.0144\n",
            "Epoch [100/100], Step [143/493], Loss: 0.0872\n",
            "Epoch [100/100], Step [144/493], Loss: 0.0161\n",
            "Epoch [100/100], Step [145/493], Loss: 0.0074\n",
            "Epoch [100/100], Step [146/493], Loss: 0.0095\n",
            "Epoch [100/100], Step [147/493], Loss: 0.0136\n",
            "Epoch [100/100], Step [148/493], Loss: 0.0089\n",
            "Epoch [100/100], Step [149/493], Loss: 0.0121\n",
            "Epoch [100/100], Step [150/493], Loss: 0.0330\n",
            "Epoch [100/100], Step [151/493], Loss: 0.0047\n",
            "Epoch [100/100], Step [152/493], Loss: 0.0103\n",
            "Epoch [100/100], Step [153/493], Loss: 0.0340\n",
            "Epoch [100/100], Step [154/493], Loss: 0.0026\n",
            "Epoch [100/100], Step [155/493], Loss: 0.0141\n",
            "Epoch [100/100], Step [156/493], Loss: 0.0140\n",
            "Epoch [100/100], Step [157/493], Loss: 0.3149\n",
            "Epoch [100/100], Step [158/493], Loss: 0.0141\n",
            "Epoch [100/100], Step [159/493], Loss: 0.0066\n",
            "Epoch [100/100], Step [160/493], Loss: 0.0369\n",
            "Epoch [100/100], Step [161/493], Loss: 0.0019\n",
            "Epoch [100/100], Step [162/493], Loss: 0.0040\n",
            "Epoch [100/100], Step [163/493], Loss: 0.0043\n",
            "Epoch [100/100], Step [164/493], Loss: 0.0017\n",
            "Epoch [100/100], Step [165/493], Loss: 0.0112\n",
            "Epoch [100/100], Step [166/493], Loss: 0.0085\n",
            "Epoch [100/100], Step [167/493], Loss: 0.0045\n",
            "Epoch [100/100], Step [168/493], Loss: 0.0154\n",
            "Epoch [100/100], Step [169/493], Loss: 0.0064\n",
            "Epoch [100/100], Step [170/493], Loss: 0.0046\n",
            "Epoch [100/100], Step [171/493], Loss: 0.0043\n",
            "Epoch [100/100], Step [172/493], Loss: 0.0097\n",
            "Epoch [100/100], Step [173/493], Loss: 0.0067\n",
            "Epoch [100/100], Step [174/493], Loss: 0.0014\n",
            "Epoch [100/100], Step [175/493], Loss: 0.0240\n",
            "Epoch [100/100], Step [176/493], Loss: 0.0578\n",
            "Epoch [100/100], Step [177/493], Loss: 0.0048\n",
            "Epoch [100/100], Step [178/493], Loss: 0.0131\n",
            "Epoch [100/100], Step [179/493], Loss: 0.0023\n",
            "Epoch [100/100], Step [180/493], Loss: 0.0098\n",
            "Epoch [100/100], Step [181/493], Loss: 0.1666\n",
            "Epoch [100/100], Step [182/493], Loss: 0.0030\n",
            "Epoch [100/100], Step [183/493], Loss: 0.2014\n",
            "Epoch [100/100], Step [184/493], Loss: 0.0145\n",
            "Epoch [100/100], Step [185/493], Loss: 0.0033\n",
            "Epoch [100/100], Step [186/493], Loss: 0.0035\n",
            "Epoch [100/100], Step [187/493], Loss: 0.0061\n",
            "Epoch [100/100], Step [188/493], Loss: 0.0264\n",
            "Epoch [100/100], Step [189/493], Loss: 0.0325\n",
            "Epoch [100/100], Step [190/493], Loss: 0.0030\n",
            "Epoch [100/100], Step [191/493], Loss: 0.0121\n",
            "Epoch [100/100], Step [192/493], Loss: 0.0094\n",
            "Epoch [100/100], Step [193/493], Loss: 0.0039\n",
            "Epoch [100/100], Step [194/493], Loss: 0.1181\n",
            "Epoch [100/100], Step [195/493], Loss: 0.0109\n",
            "Epoch [100/100], Step [196/493], Loss: 0.0033\n",
            "Epoch [100/100], Step [197/493], Loss: 0.0073\n",
            "Epoch [100/100], Step [198/493], Loss: 0.0066\n",
            "Epoch [100/100], Step [199/493], Loss: 0.0112\n",
            "Epoch [100/100], Step [200/493], Loss: 0.0088\n",
            "Epoch [100/100], Step [201/493], Loss: 0.0128\n",
            "Epoch [100/100], Step [202/493], Loss: 0.0086\n",
            "Epoch [100/100], Step [203/493], Loss: 0.0083\n",
            "Epoch [100/100], Step [204/493], Loss: 0.0073\n",
            "Epoch [100/100], Step [205/493], Loss: 0.0062\n",
            "Epoch [100/100], Step [206/493], Loss: 0.0175\n",
            "Epoch [100/100], Step [207/493], Loss: 0.0007\n",
            "Epoch [100/100], Step [208/493], Loss: 0.0058\n",
            "Epoch [100/100], Step [209/493], Loss: 0.0031\n",
            "Epoch [100/100], Step [210/493], Loss: 0.0117\n",
            "Epoch [100/100], Step [211/493], Loss: 0.0074\n",
            "Epoch [100/100], Step [212/493], Loss: 0.0081\n",
            "Epoch [100/100], Step [213/493], Loss: 0.0039\n",
            "Epoch [100/100], Step [214/493], Loss: 0.0181\n",
            "Epoch [100/100], Step [215/493], Loss: 0.0068\n",
            "Epoch [100/100], Step [216/493], Loss: 0.0021\n",
            "Epoch [100/100], Step [217/493], Loss: 0.0072\n",
            "Epoch [100/100], Step [218/493], Loss: 0.0070\n",
            "Epoch [100/100], Step [219/493], Loss: 0.0099\n",
            "Epoch [100/100], Step [220/493], Loss: 0.0128\n",
            "Epoch [100/100], Step [221/493], Loss: 0.0210\n",
            "Epoch [100/100], Step [222/493], Loss: 0.0074\n",
            "Epoch [100/100], Step [223/493], Loss: 0.0077\n",
            "Epoch [100/100], Step [224/493], Loss: 0.0175\n",
            "Epoch [100/100], Step [225/493], Loss: 0.0264\n",
            "Epoch [100/100], Step [226/493], Loss: 0.0360\n",
            "Epoch [100/100], Step [227/493], Loss: 0.0139\n",
            "Epoch [100/100], Step [228/493], Loss: 0.0005\n",
            "Epoch [100/100], Step [229/493], Loss: 0.0259\n",
            "Epoch [100/100], Step [230/493], Loss: 0.0036\n",
            "Epoch [100/100], Step [231/493], Loss: 0.0049\n",
            "Epoch [100/100], Step [232/493], Loss: 0.0870\n",
            "Epoch [100/100], Step [233/493], Loss: 0.0833\n",
            "Epoch [100/100], Step [234/493], Loss: 0.0042\n",
            "Epoch [100/100], Step [235/493], Loss: 0.0077\n",
            "Epoch [100/100], Step [236/493], Loss: 0.0103\n",
            "Epoch [100/100], Step [237/493], Loss: 0.2699\n",
            "Epoch [100/100], Step [238/493], Loss: 0.0065\n",
            "Epoch [100/100], Step [239/493], Loss: 0.0037\n",
            "Epoch [100/100], Step [240/493], Loss: 0.0060\n",
            "Epoch [100/100], Step [241/493], Loss: 0.0035\n",
            "Epoch [100/100], Step [242/493], Loss: 0.0096\n",
            "Epoch [100/100], Step [243/493], Loss: 0.0056\n",
            "Epoch [100/100], Step [244/493], Loss: 0.0080\n",
            "Epoch [100/100], Step [245/493], Loss: 0.0054\n",
            "Epoch [100/100], Step [246/493], Loss: 0.0195\n",
            "Epoch [100/100], Step [247/493], Loss: 0.0109\n",
            "Epoch [100/100], Step [248/493], Loss: 0.0230\n",
            "Epoch [100/100], Step [249/493], Loss: 0.1971\n",
            "Epoch [100/100], Step [250/493], Loss: 0.0277\n",
            "Epoch [100/100], Step [251/493], Loss: 0.0156\n",
            "Epoch [100/100], Step [252/493], Loss: 0.0121\n",
            "Epoch [100/100], Step [253/493], Loss: 0.0100\n",
            "Epoch [100/100], Step [254/493], Loss: 0.0149\n",
            "Epoch [100/100], Step [255/493], Loss: 0.0170\n",
            "Epoch [100/100], Step [256/493], Loss: 0.0140\n",
            "Epoch [100/100], Step [257/493], Loss: 0.0062\n",
            "Epoch [100/100], Step [258/493], Loss: 0.0022\n",
            "Epoch [100/100], Step [259/493], Loss: 0.0091\n",
            "Epoch [100/100], Step [260/493], Loss: 0.0138\n",
            "Epoch [100/100], Step [261/493], Loss: 0.0067\n",
            "Epoch [100/100], Step [262/493], Loss: 0.2245\n",
            "Epoch [100/100], Step [263/493], Loss: 0.0077\n",
            "Epoch [100/100], Step [264/493], Loss: 0.0033\n",
            "Epoch [100/100], Step [265/493], Loss: 0.0250\n",
            "Epoch [100/100], Step [266/493], Loss: 0.0280\n",
            "Epoch [100/100], Step [267/493], Loss: 0.0016\n",
            "Epoch [100/100], Step [268/493], Loss: 0.0179\n",
            "Epoch [100/100], Step [269/493], Loss: 0.0045\n",
            "Epoch [100/100], Step [270/493], Loss: 0.0070\n",
            "Epoch [100/100], Step [271/493], Loss: 0.1160\n",
            "Epoch [100/100], Step [272/493], Loss: 0.1857\n",
            "Epoch [100/100], Step [273/493], Loss: 0.0424\n",
            "Epoch [100/100], Step [274/493], Loss: 0.0100\n",
            "Epoch [100/100], Step [275/493], Loss: 0.0147\n",
            "Epoch [100/100], Step [276/493], Loss: 0.0059\n",
            "Epoch [100/100], Step [277/493], Loss: 0.0018\n",
            "Epoch [100/100], Step [278/493], Loss: 0.0952\n",
            "Epoch [100/100], Step [279/493], Loss: 0.0255\n",
            "Epoch [100/100], Step [280/493], Loss: 0.0027\n",
            "Epoch [100/100], Step [281/493], Loss: 0.0174\n",
            "Epoch [100/100], Step [282/493], Loss: 0.0075\n",
            "Epoch [100/100], Step [283/493], Loss: 0.0035\n",
            "Epoch [100/100], Step [284/493], Loss: 0.2984\n",
            "Epoch [100/100], Step [285/493], Loss: 0.0290\n",
            "Epoch [100/100], Step [286/493], Loss: 0.0001\n",
            "Epoch [100/100], Step [287/493], Loss: 0.0060\n",
            "Epoch [100/100], Step [288/493], Loss: 0.0083\n",
            "Epoch [100/100], Step [289/493], Loss: 0.0096\n",
            "Epoch [100/100], Step [290/493], Loss: 0.0018\n",
            "Epoch [100/100], Step [291/493], Loss: 0.0081\n",
            "Epoch [100/100], Step [292/493], Loss: 0.0117\n",
            "Epoch [100/100], Step [293/493], Loss: 0.0054\n",
            "Epoch [100/100], Step [294/493], Loss: 0.0158\n",
            "Epoch [100/100], Step [295/493], Loss: 0.0340\n",
            "Epoch [100/100], Step [296/493], Loss: 0.0893\n",
            "Epoch [100/100], Step [297/493], Loss: 0.0046\n",
            "Epoch [100/100], Step [298/493], Loss: 0.0018\n",
            "Epoch [100/100], Step [299/493], Loss: 0.0104\n",
            "Epoch [100/100], Step [300/493], Loss: 0.0050\n",
            "Epoch [100/100], Step [301/493], Loss: 0.0060\n",
            "Epoch [100/100], Step [302/493], Loss: 0.0092\n",
            "Epoch [100/100], Step [303/493], Loss: 0.0263\n",
            "Epoch [100/100], Step [304/493], Loss: 0.0108\n",
            "Epoch [100/100], Step [305/493], Loss: 0.0144\n",
            "Epoch [100/100], Step [306/493], Loss: 0.0054\n",
            "Epoch [100/100], Step [307/493], Loss: 0.0001\n",
            "Epoch [100/100], Step [308/493], Loss: 0.0109\n",
            "Epoch [100/100], Step [309/493], Loss: 0.0072\n",
            "Epoch [100/100], Step [310/493], Loss: 0.0121\n",
            "Epoch [100/100], Step [311/493], Loss: 0.0143\n",
            "Epoch [100/100], Step [312/493], Loss: 0.0095\n",
            "Epoch [100/100], Step [313/493], Loss: 0.0102\n",
            "Epoch [100/100], Step [314/493], Loss: 0.0094\n",
            "Epoch [100/100], Step [315/493], Loss: 0.0060\n",
            "Epoch [100/100], Step [316/493], Loss: 0.0081\n",
            "Epoch [100/100], Step [317/493], Loss: 0.0110\n",
            "Epoch [100/100], Step [318/493], Loss: 0.0084\n",
            "Epoch [100/100], Step [319/493], Loss: 0.0025\n",
            "Epoch [100/100], Step [320/493], Loss: 0.0066\n",
            "Epoch [100/100], Step [321/493], Loss: 0.2338\n",
            "Epoch [100/100], Step [322/493], Loss: 0.0330\n",
            "Epoch [100/100], Step [323/493], Loss: 0.0065\n",
            "Epoch [100/100], Step [324/493], Loss: 0.0019\n",
            "Epoch [100/100], Step [325/493], Loss: 0.0114\n",
            "Epoch [100/100], Step [326/493], Loss: 0.0046\n",
            "Epoch [100/100], Step [327/493], Loss: 0.0033\n",
            "Epoch [100/100], Step [328/493], Loss: 0.0686\n",
            "Epoch [100/100], Step [329/493], Loss: 0.0070\n",
            "Epoch [100/100], Step [330/493], Loss: 0.0100\n",
            "Epoch [100/100], Step [331/493], Loss: 0.0055\n",
            "Epoch [100/100], Step [332/493], Loss: 0.0022\n",
            "Epoch [100/100], Step [333/493], Loss: 0.0083\n",
            "Epoch [100/100], Step [334/493], Loss: 0.0052\n",
            "Epoch [100/100], Step [335/493], Loss: 0.0042\n",
            "Epoch [100/100], Step [336/493], Loss: 0.0002\n",
            "Epoch [100/100], Step [337/493], Loss: 0.0058\n",
            "Epoch [100/100], Step [338/493], Loss: 0.0244\n",
            "Epoch [100/100], Step [339/493], Loss: 0.0074\n",
            "Epoch [100/100], Step [340/493], Loss: 0.0202\n",
            "Epoch [100/100], Step [341/493], Loss: 0.0434\n",
            "Epoch [100/100], Step [342/493], Loss: 0.0206\n",
            "Epoch [100/100], Step [343/493], Loss: 0.0058\n",
            "Epoch [100/100], Step [344/493], Loss: 0.0080\n",
            "Epoch [100/100], Step [345/493], Loss: 0.0055\n",
            "Epoch [100/100], Step [346/493], Loss: 0.0168\n",
            "Epoch [100/100], Step [347/493], Loss: 0.0119\n",
            "Epoch [100/100], Step [348/493], Loss: 0.0445\n",
            "Epoch [100/100], Step [349/493], Loss: 0.0124\n",
            "Epoch [100/100], Step [350/493], Loss: 0.0344\n",
            "Epoch [100/100], Step [351/493], Loss: 0.0078\n",
            "Epoch [100/100], Step [352/493], Loss: 0.0031\n",
            "Epoch [100/100], Step [353/493], Loss: 0.0300\n",
            "Epoch [100/100], Step [354/493], Loss: 0.0472\n",
            "Epoch [100/100], Step [355/493], Loss: 0.0070\n",
            "Epoch [100/100], Step [356/493], Loss: 0.0097\n",
            "Epoch [100/100], Step [357/493], Loss: 0.0085\n",
            "Epoch [100/100], Step [358/493], Loss: 0.0068\n",
            "Epoch [100/100], Step [359/493], Loss: 0.0078\n",
            "Epoch [100/100], Step [360/493], Loss: 0.0079\n",
            "Epoch [100/100], Step [361/493], Loss: 0.0058\n",
            "Epoch [100/100], Step [362/493], Loss: 0.0099\n",
            "Epoch [100/100], Step [363/493], Loss: 0.0067\n",
            "Epoch [100/100], Step [364/493], Loss: 0.0063\n",
            "Epoch [100/100], Step [365/493], Loss: 0.0045\n",
            "Epoch [100/100], Step [366/493], Loss: 0.0227\n",
            "Epoch [100/100], Step [367/493], Loss: 0.0068\n",
            "Epoch [100/100], Step [368/493], Loss: 0.0131\n",
            "Epoch [100/100], Step [369/493], Loss: 0.0029\n",
            "Epoch [100/100], Step [370/493], Loss: 0.2286\n",
            "Epoch [100/100], Step [371/493], Loss: 0.0097\n",
            "Epoch [100/100], Step [372/493], Loss: 0.0059\n",
            "Epoch [100/100], Step [373/493], Loss: 0.0023\n",
            "Epoch [100/100], Step [374/493], Loss: 0.1683\n",
            "Epoch [100/100], Step [375/493], Loss: 0.0086\n",
            "Epoch [100/100], Step [376/493], Loss: 0.0044\n",
            "Epoch [100/100], Step [377/493], Loss: 0.0103\n",
            "Epoch [100/100], Step [378/493], Loss: 0.0084\n",
            "Epoch [100/100], Step [379/493], Loss: 0.0014\n",
            "Epoch [100/100], Step [380/493], Loss: 0.0143\n",
            "Epoch [100/100], Step [381/493], Loss: 0.0053\n",
            "Epoch [100/100], Step [382/493], Loss: 0.0022\n",
            "Epoch [100/100], Step [383/493], Loss: 0.0057\n",
            "Epoch [100/100], Step [384/493], Loss: 0.0057\n",
            "Epoch [100/100], Step [385/493], Loss: 0.0077\n",
            "Epoch [100/100], Step [386/493], Loss: 0.0124\n",
            "Epoch [100/100], Step [387/493], Loss: 0.0060\n",
            "Epoch [100/100], Step [388/493], Loss: 0.0036\n",
            "Epoch [100/100], Step [389/493], Loss: 0.0129\n",
            "Epoch [100/100], Step [390/493], Loss: 0.0059\n",
            "Epoch [100/100], Step [391/493], Loss: 0.0012\n",
            "Epoch [100/100], Step [392/493], Loss: 0.0077\n",
            "Epoch [100/100], Step [393/493], Loss: 0.0382\n",
            "Epoch [100/100], Step [394/493], Loss: 0.0050\n",
            "Epoch [100/100], Step [395/493], Loss: 0.0123\n",
            "Epoch [100/100], Step [396/493], Loss: 0.0105\n",
            "Epoch [100/100], Step [397/493], Loss: 0.0136\n",
            "Epoch [100/100], Step [398/493], Loss: 0.0096\n",
            "Epoch [100/100], Step [399/493], Loss: 0.1087\n",
            "Epoch [100/100], Step [400/493], Loss: 0.0657\n",
            "Epoch [100/100], Step [401/493], Loss: 0.0014\n",
            "Epoch [100/100], Step [402/493], Loss: 0.0059\n",
            "Epoch [100/100], Step [403/493], Loss: 0.0123\n",
            "Epoch [100/100], Step [404/493], Loss: 0.0084\n",
            "Epoch [100/100], Step [405/493], Loss: 0.0102\n",
            "Epoch [100/100], Step [406/493], Loss: 0.0075\n",
            "Epoch [100/100], Step [407/493], Loss: 0.0096\n",
            "Epoch [100/100], Step [408/493], Loss: 0.0393\n",
            "Epoch [100/100], Step [409/493], Loss: 0.5350\n",
            "Epoch [100/100], Step [410/493], Loss: 0.0132\n",
            "Epoch [100/100], Step [411/493], Loss: 0.0209\n",
            "Epoch [100/100], Step [412/493], Loss: 0.0010\n",
            "Epoch [100/100], Step [413/493], Loss: 0.0011\n",
            "Epoch [100/100], Step [414/493], Loss: 0.0154\n",
            "Epoch [100/100], Step [415/493], Loss: 0.0284\n",
            "Epoch [100/100], Step [416/493], Loss: 0.0093\n",
            "Epoch [100/100], Step [417/493], Loss: 0.0045\n",
            "Epoch [100/100], Step [418/493], Loss: 0.0067\n",
            "Epoch [100/100], Step [419/493], Loss: 0.0032\n",
            "Epoch [100/100], Step [420/493], Loss: 0.0097\n",
            "Epoch [100/100], Step [421/493], Loss: 0.0074\n",
            "Epoch [100/100], Step [422/493], Loss: 0.0141\n",
            "Epoch [100/100], Step [423/493], Loss: 0.0122\n",
            "Epoch [100/100], Step [424/493], Loss: 0.0079\n",
            "Epoch [100/100], Step [425/493], Loss: 0.0166\n",
            "Epoch [100/100], Step [426/493], Loss: 0.0117\n",
            "Epoch [100/100], Step [427/493], Loss: 0.0194\n",
            "Epoch [100/100], Step [428/493], Loss: 0.0258\n",
            "Epoch [100/100], Step [429/493], Loss: 0.0051\n",
            "Epoch [100/100], Step [430/493], Loss: 0.0065\n",
            "Epoch [100/100], Step [431/493], Loss: 0.0027\n",
            "Epoch [100/100], Step [432/493], Loss: 0.1114\n",
            "Epoch [100/100], Step [433/493], Loss: 0.1910\n",
            "Epoch [100/100], Step [434/493], Loss: 0.0045\n",
            "Epoch [100/100], Step [435/493], Loss: 0.0597\n",
            "Epoch [100/100], Step [436/493], Loss: 0.0146\n",
            "Epoch [100/100], Step [437/493], Loss: 0.0126\n",
            "Epoch [100/100], Step [438/493], Loss: 0.0131\n",
            "Epoch [100/100], Step [439/493], Loss: 0.0065\n",
            "Epoch [100/100], Step [440/493], Loss: 0.0330\n",
            "Epoch [100/100], Step [441/493], Loss: 0.0289\n",
            "Epoch [100/100], Step [442/493], Loss: 0.0116\n",
            "Epoch [100/100], Step [443/493], Loss: 0.0253\n",
            "Epoch [100/100], Step [444/493], Loss: 0.0054\n",
            "Epoch [100/100], Step [445/493], Loss: 0.0092\n",
            "Epoch [100/100], Step [446/493], Loss: 0.0029\n",
            "Epoch [100/100], Step [447/493], Loss: 0.0039\n",
            "Epoch [100/100], Step [448/493], Loss: 0.0130\n",
            "Epoch [100/100], Step [449/493], Loss: 0.0391\n",
            "Epoch [100/100], Step [450/493], Loss: 0.0003\n",
            "Epoch [100/100], Step [451/493], Loss: 0.0027\n",
            "Epoch [100/100], Step [452/493], Loss: 0.0092\n",
            "Epoch [100/100], Step [453/493], Loss: 0.0141\n",
            "Epoch [100/100], Step [454/493], Loss: 0.0040\n",
            "Epoch [100/100], Step [455/493], Loss: 0.0131\n",
            "Epoch [100/100], Step [456/493], Loss: 0.0075\n",
            "Epoch [100/100], Step [457/493], Loss: 0.0084\n",
            "Epoch [100/100], Step [458/493], Loss: 0.0229\n",
            "Epoch [100/100], Step [459/493], Loss: 0.0129\n",
            "Epoch [100/100], Step [460/493], Loss: 0.0001\n",
            "Epoch [100/100], Step [461/493], Loss: 0.0264\n",
            "Epoch [100/100], Step [462/493], Loss: 0.0062\n",
            "Epoch [100/100], Step [463/493], Loss: 0.0069\n",
            "Epoch [100/100], Step [464/493], Loss: 0.0252\n",
            "Epoch [100/100], Step [465/493], Loss: 0.0097\n",
            "Epoch [100/100], Step [466/493], Loss: 0.0041\n",
            "Epoch [100/100], Step [467/493], Loss: 0.0029\n",
            "Epoch [100/100], Step [468/493], Loss: 0.0029\n",
            "Epoch [100/100], Step [469/493], Loss: 0.0128\n",
            "Epoch [100/100], Step [470/493], Loss: 0.0045\n",
            "Epoch [100/100], Step [471/493], Loss: 0.0072\n",
            "Epoch [100/100], Step [472/493], Loss: 0.0138\n",
            "Epoch [100/100], Step [473/493], Loss: 0.1024\n",
            "Epoch [100/100], Step [474/493], Loss: 0.0059\n",
            "Epoch [100/100], Step [475/493], Loss: 0.0154\n",
            "Epoch [100/100], Step [476/493], Loss: 0.0018\n",
            "Epoch [100/100], Step [477/493], Loss: 0.0180\n",
            "Epoch [100/100], Step [478/493], Loss: 0.0032\n",
            "Epoch [100/100], Step [479/493], Loss: 0.0208\n",
            "Epoch [100/100], Step [480/493], Loss: 0.0031\n",
            "Epoch [100/100], Step [481/493], Loss: 0.0268\n",
            "Epoch [100/100], Step [482/493], Loss: 0.2259\n",
            "Epoch [100/100], Step [483/493], Loss: 0.0077\n",
            "Epoch [100/100], Step [484/493], Loss: 0.0055\n",
            "Epoch [100/100], Step [485/493], Loss: 0.0228\n",
            "Epoch [100/100], Step [486/493], Loss: 0.0153\n",
            "Epoch [100/100], Step [487/493], Loss: 0.1099\n",
            "Epoch [100/100], Step [488/493], Loss: 0.0116\n",
            "Epoch [100/100], Step [489/493], Loss: 0.0413\n",
            "Epoch [100/100], Step [490/493], Loss: 0.0033\n",
            "Epoch [100/100], Step [491/493], Loss: 0.0316\n",
            "Epoch [100/100], Step [492/493], Loss: 0.0087\n",
            "Epoch [100/100], Step [493/493], Loss: 0.0178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0P6Emoi-9EC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "783b96ef-b85a-4fa8-8048-3452bb9d4278"
      },
      "source": [
        "model = Net(input_size, hidden_size, num_classes)\n",
        "model.load_state_dict(torch.load('model.ckpt'))\n",
        "model.eval()\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    for (data,labels) in test_loader:\n",
        "        #data=transforms.ToTensor(data)\n",
        "        labels=labels[:,np.newaxis]\n",
        "        outputs = model(data.float())\n",
        "        loss = criterion(outputs,labels.float())\n",
        "\n",
        "My_list = list(range(1, num_epochs+1))\n",
        "plt.plot(My_list,loss1,label = \"Training Loss\")\n",
        "plt.xlabel('Epochs') \n",
        "plt.ylabel('Loss(MMSE)') \n",
        "plt.title('Training Loss') \n",
        "plt.legend() \n",
        "plt.show()\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([10, 1, 4])) that is different to the input size (torch.Size([10, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([4, 1, 4])) that is different to the input size (torch.Size([4, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loLSFHULT49Q",
        "colab_type": "text"
      },
      "source": [
        "### Probability of Outage Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m6svDWo0S7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "outputId": "e31cbaac-c70d-4914-a80f-728e759ef02c"
      },
      "source": [
        "model = Net(input_size, hidden_size, num_classes)\n",
        "model.load_state_dict(torch.load('model.ckpt'))\n",
        "model.eval()\n",
        "trials=1000\n",
        "\n",
        "thresh=1\n",
        "scale=1\n",
        "power_total=[50,90,100,200,300,800,895,1000,1222,1268,1452,1584,1778,2000,2238,2511,2888,3169];\n",
        "            #[800,895,100,122,126,145,158,177,200,223,251,288,316,410];\n",
        "prob_outage=np.zeros([len(power_total),4])\n",
        "for i in range(len(power_total)):\n",
        "    p=np.zeros([1,4]);\n",
        "    no_outages=np.zeros([1,4])\n",
        "    for j in range(trials):\n",
        "        hr = [abs(np.asscalar(1*np.random.normal(0,1,1))),np.asscalar(abs(2*np.random.normal(0,1,1))),np.asscalar(abs(3*np.random.normal(0,1,1))),np.asscalar(abs(4*np.random.normal(0,1,1)))]\n",
        "        hr.sort()\n",
        "        h=[hr[1],hr[3],hr[2],hr[0]]\n",
        "        input=hr+[power_total[i]]\n",
        "        input = torch.from_numpy(np.array(input)).float()\n",
        "        p=model(input)\n",
        "        c1 = m.log2(1+((h[0]**2)*p[0]/((h[0]**2)*p[1]+(h[0]**2)*p[2]+1)))\n",
        "        c2 = m.log2(1+(h[1]**2)*p[1])\n",
        "        c3 = m.log2(1+((h[2]**2)*p[2]/((h[1]**2)*p[1]+1)))\n",
        "        c4 = m.log2(1+((h[3]**2)*p[3]/((h[3]**2)*p[0]+(h[3]**2)*p[1]+(h[3]**2)*p[2]+1)))\n",
        "        \n",
        "        if c1<thresh:\n",
        "          no_outages[0][0]+=1\n",
        "        if c2<thresh:\n",
        "          no_outages[0][1]+=1\n",
        "        if c3<thresh:\n",
        "          no_outages[0][2]+=1\n",
        "        if c4<thresh:\n",
        "          no_outages[0][3]+=1\n",
        "        \n",
        "    for j in range(4):\n",
        "      prob_outage[i][j]=no_outages[0][j]/trials\n",
        "\n",
        "\n",
        "plt.plot(power_total,prob_outage[:,0],label = \"U1\")\n",
        "plt.plot(power_total,prob_outage[:,1],label = \"U2\") \n",
        "plt.plot(power_total,prob_outage[:,2],label = \"U3\")\n",
        "plt.plot(power_total,prob_outage[:,3],label = \"U4\")\n",
        "plt.xlabel('Power_total') \n",
        "plt.ylabel('Prob of outage') \n",
        "plt.title('Probability of outage') \n",
        "plt.legend() \n",
        "plt.show()\n",
        "\n",
        "        \n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.229 0.001 0.938 0.799]\n",
            " [0.152 0.    0.924 0.68 ]\n",
            " [0.125 0.    0.921 0.638]\n",
            " [0.091 0.    0.894 0.489]\n",
            " [0.06  0.    0.884 0.422]\n",
            " [0.028 0.    0.898 0.284]\n",
            " [0.023 0.    0.88  0.288]\n",
            " [0.025 0.    0.877 0.272]\n",
            " [0.016 0.    0.88  0.252]\n",
            " [0.014 0.    0.896 0.246]\n",
            " [0.014 0.    0.888 0.219]\n",
            " [0.02  0.    0.877 0.222]\n",
            " [0.011 0.    0.895 0.207]\n",
            " [0.017 0.    0.879 0.182]\n",
            " [0.012 0.    0.879 0.194]\n",
            " [0.014 0.    0.887 0.169]\n",
            " [0.007 0.    0.884 0.155]\n",
            " [0.01  0.    0.897 0.178]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIl-leZyPoGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}